Title,Year,Link,Abstract
"Performing parallel monte carlo and moment equations methods for itô and stratonovich stochastic differential systems: R package sim.diffproc",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097089494&doi=10.18637%2fjss.v096.i02&partnerID=40&md5=ed003698720282ddbcb2eff7842d9872","We introduce Sim.DiffProc, an R package for symbolic and numerical computations on scalar and multivariate systems of stochastic differential equations (SDEs). It provides users with a wide range of tools to simulate, estimate, analyze, and visualize the dynamics of these systems in both forms, Itô and Stratonovich. One of Sim.DiffProc key features is to implement the Monte Carlo method for the iterative evaluation and approximation of an interesting quantity at a fixed time on SDEs with parallel computing, on multiple processors on a single machine or a cluster of computers, which is an important tool to improve capacity and speed-up calculations. We also provide an easy-to-use interface for symbolic calculation and numerical approximation of the first and central second-order moments of SDEs (i.e., mean, variance and covariance), by solving a system of ordinary differential equations, which yields insights into the dynamics of stochastic systems. The final result object of Monte Carlo and moment equations can be derived and presented in terms of LATEX math expressions and visualized in terms of LATEX tables. Furthermore, we illustrate various features of the package by proposing a general bivariate nonlinear dynamic system of Haken-Zwanzig, driven by additive, linear and nonlinear multiplicative noises. In addition, we consider the particular case of a scalar SDE driven by three independent Wiener processes. The Monte Carlo simulation thereof is obtained through a transformation to a system of three equations. We also study some important applications of SDEs in different fields. © 2020, American Statistical Association. All rights reserved."
"Localcontrol: An R package for comparative safety and effectiveness research",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097082876&doi=10.18637%2fjss.v096.i04&partnerID=40&md5=c5cdfa75c60d270c6442db97dfddf9c8","The LocalControl R package implements novel approaches to address biases and con-founding when comparing treatments or exposures in observational studies of outcomes. While designed and appropriate for use in comparative safety and effectiveness research involving medicine and the life sciences, the package can be used in other situations involving outcomes with multiple confounders. LocalControl is an open-source tool for researchers whose aim is to generate high quality evidence using observational data. The package implements a family of methods for non-parametric bias correction when comparing treatments in observational studies, including survival analysis settings, where competing risks and/or censoring may be present. The approach extends to bias-corrected personalized predictions of treatment outcome differences, and analysis of heterogeneity of treatment effect-sizes across patient subgroups. © 2020, American Statistical Association. All rights reserved."
"Generating optimal designs for discrete choice experiments in R: The idefix package",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097081818&doi=10.18637%2fjss.v096.i03&partnerID=40&md5=b2ed988066e5627f9d9c34a658e0205b","Discrete choice experiments are widely used in a broad area of research fields to capture the preference structure of respondents. The design of such experiments will determine to a large extent the accuracy with which the preference parameters can be estimated. This paper presents a new R package, called idefix, which enables users to generate optimal designs for discrete choice experiments. Besides Bayesian D-efficient designs for the multinomial logit model, the package includes functions to generate Bayesian adaptive designs which can be used to gather data for the mixed logit model. In addition, the package provides the necessary tools to set up actual surveys and collect empirical data. After data collection, idefix can be used to transform the data into the necessary format in order to use existing estimation software in R. © 2020, American Statistical Association. All rights reserved."
"Generalized network autoregressive processes and the gnar package",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097058983&doi=10.18637%2fjss.v096.i05&partnerID=40&md5=307f71e122c5521f982104543e380f12","This article introduces the GNAR package, which fits, predicts, and simulates from a powerful new class of generalized network autoregressive processes. Such processes consist of a multivariate time series along with a real, or inferred, network that provides information about inter-variable relationships. The GNAR model relates values of a time series for a given variable and time to earlier values of the same variable and of neighboring variables, with inclusion controlled by the network structure. The GNAR package is designed to fit this new model, while working with standard ‘ts’ objects and the igraph package for ease of use. © 2020, American Statistical Association. All rights reserved."
"Colorspace: A toolbox for manipulating and assessing colors and palettes",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097057043&doi=10.18637%2fjss.v096.i01&partnerID=40&md5=f4c6fd5363d9adac9cdcb2df998a3b23","The R package colorspace provides a flexible toolbox for selecting individual colors or color palettes, manipulating these colors, and employing them in statistical graphics and data visualizations. In particular, the package provides a broad range of color palettes based on the HCL (hue-chroma-luminance) color space. The three HCL dimensions have been shown to match those of the human visual system very well, thus facilitating intu-itive selection of color palettes through trajectories in this space. Using the HCL color model, general strategies for three types of palettes are implemented: (1) Qualitative for coding categorical information, i.e., where no particular ordering of categories is available. (2) Sequential for coding ordered/numeric information, i.e., going from high to low (or vice versa). (3) Diverging for coding ordered/numeric information around a central neutral value, i.e., where colors diverge from neutral to two extremes. To aid selection and application of these palettes, the package also contains scales for use with ggplot2, shiny and tcltk apps for interactive exploration, visualizations of palette properties, accompany-ing manipulation utilities (like desaturation and lighten/darken), and emulation of color vision deficiencies. The shiny apps are also hosted online at http://hclwizard.org/. © 2020, American Statistical Association. All rights reserved."
"Assoctests: An R package for genetic association studies",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087402051&doi=10.18637%2fjss.v094.i05&partnerID=40&md5=9ac26510bf26244ad032e5d13f11c5e4","The R package AssocTests provides some procedures which are commonly used in genetic association studies. These procedures are population stratification correction through eigenvectors, principal coordinates of clusterings, Tracy-Widom test, distance regression, single-marker test, maximum test based on three Cochran-Armitage trend tests, non-parametric trend test, and non-parametric maximum test. The trait values for these methods should be discrete or continuous. The discrete traits can be coded by 1/0 for cases/controls. The genotype values can be 0, 1, or 2 indicating the number of risk alleles for a biallelic single-nucleotide polymorphism. This article introduces the methods and algorithms implemented in the package. Some examples are provided to illustrate the package’s capability. © 2020, American Statistical Association. All rights reserved."
"Bess: An R package for best subset selection in linear, logistic and cox proportional hazards models",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087380644&doi=10.18637%2fjss.v094.i04&partnerID=40&md5=00ea829238383908d71a0e5cdab93ec5","We introduce a new R package, BeSS, for solving the best subset selection problem in linear, logistic and Cox’s proportional hazard (CoxPH) models. It utilizes a highly efficient active set algorithm based on primal and dual variables, and supports sequential and golden search strategies for best subset selection. We provide a C++ implementation of the algorithm using an Rcpp interface. We demonstrate through numerical experiments based on enormous simulation and real datasets that the new BeSS package has competitive performance compared to other R packages for best subset selection purposes. © 2020, American Statistical Association. All rights reserved."
"Bayesnetbp: An R package for probabilistic reasoning in Bayesian networks",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087363785&doi=10.18637%2fjss.v094.i03&partnerID=40&md5=48adb989b17e5241e8feae7ce8769936","The BayesNetBP package has been developed for probabilistic reasoning and visualization in Bayesian networks with nodes that are purely discrete, continuous or mixed (discrete and continuous). Probabilistic reasoning enables a user to absorb information into a Bayesian network and make queries about how the probabilities within the network change in light of new information. The package was developed in the R programming language and is freely available from the Comprehensive R Archive Network. A shiny app with Cytoscape widgets provides an interactive interface for evidence absorption, queries, and visualizations. © 2020, American Statistical Association. All rights reserved."
"Fastnet: An r package for fast simulation and analysis of large-scale social networks",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097680777&doi=10.18637%2fjss.v096.i07&partnerID=40&md5=96037c9296c7b67d9163c0b2e3489bcb","Traditional tools and software for social network analysis are seldom scalable and/or fast. This paper provides an overview of an R package called fastnet, a tool for scaling and speeding up the simulation and analysis of large-scale social networks. fastnet uses multi-core processing and sub-graph sampling algorithms to achieve the desired scale-up and speed-up. Simple examples, usages, and comparisons of scale-up and speed-up as compared to other R packages, i.e., igraph and statnet, are presented. © 2020, American Statistical Association. All rights reserved."
"Continuous ordinal regression for analysis of visual analogue scales: The r package ordinalcont",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097665151&doi=10.18637%2fjss.v096.i08&partnerID=40&md5=4121f89d43b6749d12ba8ab76a80e846","This paper introduces the R package ordinalCont, which implements an ordinal regression framework for response variables which are recorded on a visual analogue scale (VAS). This scale is used when recording subjects’ perception of an intangible quantity such as pain, anxiety or quality of life, and consists of a mark made on a linear scale. We implement continuous ordinal regression models for VAS as the appropriate method of analysis for such responses, and introduce smoothing terms and random effects in the linear predictor. The model parameters are estimated using constrained optimization of the penalized likelihood and the penalty parameters are automatically selected via max-imization of their marginal likelihood. The estimation algorithm is shown to perform well, in a simulation study. Two examples of application are given: the first involves the analysis of pain outcomes in a clinical trial for laser treatment for chronic neck pain; the second is an analysis of quality of life outcomes in a clinical trial for chemotherapy for the treatment of breast cancer. © 2020, American Statistical Association. All rights reserved."
"Wilcoxon rank-based tests for clustered data with r package clusrank",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097663798&doi=10.18637%2fjss.v096.i06&partnerID=40&md5=6c89cb2f4bf46f74c6f40a22a8a5e505","Wilcoxon rank-based tests are distribution-free alternatives to the popular two-sample and paired t tests. For independent data, they are available in several R packages such as stats and coin. For clustered data, in spite of the recent methodological developments, there did not exist an R package that makes them available at one place. We present a package clusrank where the latest developments are implemented and wrapped under a unified user-friendly interface. With different methods dispatched based on the inputs, this package offers great flexibility in rank-based tests for various clustered data. Exact tests based on permutations are also provided for some methods. Details of the major schools of different methods are briefly reviewed. Usages of the package clusrank are illustrated with simulated data as well as a real dataset from an ophthalmological study. The package also enables convenient comparison between selected methods under settings that have not been studied before and the results are discussed. © 2020, American Statistical Association. All rights reserved."
"Using gnu make to manage the workflow of data analysis projects",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093895699&doi=10.18637%2fjss.v094.c01&partnerID=40&md5=b338d397b172cb1ddd80112a65d78f70","Data analysis projects invariably involve a series of steps such as reading, cleaning, summarizing and plotting data, statistical analysis and reporting. To facilitate reproducible research, rather than employing a relatively ad-hoc point-and-click cut-and-paste approach, we typically break down these tasks into manageable chunks by employing separate files of statistical, programming or text processing syntax for each step including the final report. Real world data analysis often requires an iterative process because many of these steps may need to be repeated any number of times. Manually repeating these steps is problematic in that some necessary steps may be left out or some reported results may not be for the most recent data set or syntax. GNU Make may be used to automate the mundane task of regenerating output given dependencies between syntax and data files. In addition to facilitating the management of and documenting the workflow of a complex data analysis project, such automation can help minimize errors and make the project more reproducible. It is relatively simple to construct Makefiles for small data analysis projects. As projects increase in size, difficulties arise because GNU Make does not have inbuilt rules for statistical and related software. Without such rules, Makefiles can become unwieldy and error-prone. This article addresses these issues by providing GNU Make pattern rules for R, Sweave, rmarkdown, SAS, Stata, Perl and Python to streamline management of data analysis and reporting projects. Rules are used by adding a single line to project Makefiles. Additional flexibility is incorporated for modifying standard program options. An overall strategy is outlined for Makefile construction and illustrated via simple and complex examples. © 2020, American Statistical Association. All rights reserved."
"Streamingbandit: Experimenting with bandit policies",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093878460&doi=10.18637%2fjss.v094.i09&partnerID=40&md5=01e8b5a7d5029f1d5042066df8efd746","A large number of statistical decision problems in the social sciences and beyond can be framed as a (contextual) multi-armed bandit problem. However, it is notoriously hard to develop and evaluate policies that tackle these types of problems, and to use such policies in applied studies. To address this issue, this paper introduces StreamingBandit, a Python web application for developing and testing bandit policies in field studies. StreamingBandit can sequentially select treatments using (online) policies in real time. Once StreamingBandit is implemented in an applied context, different policies can be tested, altered, nested, and compared. StreamingBandit makes it easy to apply a multitude of bandit policies for sequential allocation in field experiments, and allows for the quick development and re-use of novel policies. In this article, we detail the implementation logic of StreamingBandit and provide several examples of its use. © 2020, American Statistical Association. All rights reserved."
"Computationally efficient simulation of queues: The r package queuecomputer",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092689217&doi=10.18637%2fjss.v095.i05&partnerID=40&md5=18e10d195827ca5880f05a3a2cb193d2","Large networks of queueing systems model important real-world systems such as MapReduce clusters, web-servers, hospitals, call centers and airport passenger terminals. To model such systems accurately, we must infer queueing parameters from data. Unfortunately, for many queueing networks there is no clear way to proceed with parameter inference from data. Approximate Bayesian computation could offer a straightforward way to infer parameters for such networks if we could simulate data quickly enough. We present a computationally efficient method for simulating from a very general set of queueing networks with the R package queuecomputer. Remarkable speedups of more than 2 orders of magnitude are observed relative to the popular DES packages simmer and simpy. We replicate output from these packages to validate the package. The package is modular and integrates well with the popular R package dplyr. Complex queueing networks with tandem, parallel and fork/join topologies can easily be built with these two packages together. We show how to use this package with two examples: a call center and an airport terminal. © 2020, American Statistical Association. All rights reserved."
"Mlogit: Random utility models in r",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092535640&doi=10.18637%2fjss.v095.i11&partnerID=40&md5=77d2c339a82d3da959403a12d566cbde","mlogit is a package for R which enables the estimation of random utility models with choice situation and/or alternative specific variables. The main extensions of the basic multinomial model (heteroscedastic, nested and random parameter models) are implemented. © 2020, American Statistical Association. All rights reserved."
"Patch-wise adaptive weights smoothing in r",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092522545&doi=10.18637%2fjss.v095.i06&partnerID=40&md5=86e012b199d6c27a66226a37c15f65da","Image reconstruction from noisy data has a long history of methodological development and is based on a variety of ideas. In this paper we introduce a new method called patch-wise adaptive smoothing, that extends the propagation-separation approach by using comparisons of local patches of image intensities to define local adaptive weighting schemes for an improved balance of reduced variability and bias in the reconstruction result. We present the implementation of the new method in an R package aws and demonstrate its properties on a number of examples in comparison with other state-of-the art image reconstruction methods. © 2020, American Statistical Association. All rights reserved."
"Anova_robust: A sas macro for parametric tests of mean differences in one-factor anova models",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092514520&doi=10.18637%2fjss.v095.c02&partnerID=40&md5=a1ad56dbf5a36b0e5a934277e74d3e23","Testing the equality of several independent group means is a common statistical practice in the social sciences. The traditional analysis of variance (ANOVA) is one of the most popular methods. However, the ANOVA F test is sensitive to violations of the homogeneity of variance assumption. Many alternative tests have been developed in response to this problem of the F test. These tests include some modifications of the ANOVA F test and others based on the structured means modeling technique. This paper provides a SAS macro for testing the equality of group means using thirteen methods including the regular ANOVA F test. In addition, this paper summarizes the results of a simulation study that compares the performance of these tests in terms of their Type I error rate under different conditions, especially under violations of the homogeneity of variance assumption. © 2020, American Statistical Association. All rights reserved."
"Acebayes: An r package for bayesian optimal design of experiments via approximate coordinate exchange",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092514370&doi=10.18637%2fjss.v095.i13&partnerID=40&md5=e811cbf4f9a6fc7c850d4241d0860804","We describe the R package acebayes and demonstrate its use to find Bayesian optimal experimental designs. A decision-theoretic approach is adopted, with the optimal design maximizing an expected utility. Finding Bayesian optimal designs for realistic problems is challenging, as the expected utility is typically intractable and the design space may be high-dimensional. The package implements the approximate coordinate exchange algorithm to optimize (an approximation to) the expected utility via a sequence of conditional one-dimensional optimization steps. At each step, a Gaussian process regression model is used to approximate, and subsequently optimize, the expected utility as the function of a single design coordinate (the value taken by one controllable variable for one run of the ex-periment). In addition to functions for bespoke design problems with user-defined utility functions, acebayes provides functions tailored to finding designs for common generalized linear and nonlinear models. The package provides a step-change in the complexity of problems that can be addressed, enabling designs to be found for much larger numbers of variables and runs than previously possible. We provide tutorials on the application of the methodology for four illustrative examples of varying complexity where designs are found for the goals of parameter estimation, model selection and prediction. These examples demonstrate previously unseen functionality of acebayes. © 2020, American Statistical Association. All rights reserved."
"Zigzag expanded navigation plots in r: The r package zenplots",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092489240&doi=10.18637%2fjss.v095.i04&partnerID=40&md5=b4c8fc441ca778b536ea80c4c1358ba1","We describe the features and implementation of the R package zenplots (zigzag expanded navigation plots) for displaying high-dimensional data according to the recently proposed zenplots. By default, zenplots lay out alternating one-and two-dimensional plots in a zigzag-like pattern where adjacent axes share the same variate. Zenplots are especially useful when subsets of pairs can be identified as of particular interest by some measure, or as not meaningfully comparable, or when pairs of variates can be ordered in terms of potential interest to view, or the number of pairs is too large for more traditional layouts such as a scatterplot matrix. They also allow an essentially arbitrary layout of plots. A high-dimensional space can be explored in a zenplot (zenplot()) by navigating through lower dimensional subspaces along a zenpath (zenpath()) which orders the dimensions (i.e., variates) visited according to some measure of interestingness; see Hofert and Oldford (2018) for an application to S&P 500 constituent data. The R package zenplots provides compact displays for high-dimensional data via the notion of zenplots, grouping of variates, and customizable displays of zigzag layouts. It accommodates different graphical systems including the base graphics package of R Core Team (2020b), the package grid of R Core Team (2020a) (and hence packages like ggplot2 of Wickham et al. 2020), and the interactive graphical package loon of Waddell and Oldford (2020). zenplots handles groups of variates, partial and fully missing data, and more. One important feature is that zenplot() and its auxiliary functions in zenplots distinguish layout from plotting which allows one to freely choose and create one-and two-dimensional plot functions; predefined functions are exported for all graphical systems. All R plots in this paper are reproducible with the vignette ""selected_features"" (available in zenplots ≥ 0.0-2). © 2020, American Statistical Association. All rights reserved."
"Survhe: Survival analysis for health economic evaluation and cost-effectiveness modeling",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092489047&doi=10.18637%2fjss.v095.i14&partnerID=40&md5=4cbdfa0126b1f0153bd90ae791a2faa6","Survival analysis features heavily as an important part of health economic evaluation, an increasingly important component of medical research. In this setting, it is important to estimate the mean time to the survival endpoint using limited information (typically from randomized trials) and thus it is useful to consider parametric survival models. In this paper, we review the features of the R package survHE, specifically designed to wrap several tools to perform survival analysis for economic evaluation. In particular, survHE embeds both a standard, frequentist analysis (through the R package flexsurv) and a Bayesian approach, based on Hamiltonian Monte Carlo (via the R package rstan) or integrated nested Laplace approximation (with the R package INLA). Using this composite approach, we obtain maximum flexibility and are able to pre-compile a wide range of parametric models, with a view of simplifying the modelers’ work and allowing them to move away from non-optimal work flows, including spreadsheets (e.g., Microsoft Excel). © 2020, American Statistical Association. All rights reserved."
"Computing the kolmogorov-smirnov distribution when the underlying cdf is purely discrete, mixed, or continuous",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092488264&doi=10.18637%2fjss.v095.i10&partnerID=40&md5=215cf72943f5d6deb8e1fce88db8cf44","The distribution of the Kolmogorov-Smirnov (KS) test statistic has been widely studied under the assumption that the underlying theoretical cumulative distribution function (CDF), F (x), is continuous. However, there are many real-life applications in which fitting discrete or mixed distributions is required. Nevertheless, due to inherent difficulties, the distribution of the KS statistic when F (x) has jump discontinuities has been studied to a much lesser extent and no exact and efficient computational methods have been proposed in the literature. In this paper, we provide a fast and accurate method to compute the (complementary) CDF of the KS statistic when F (x) is discontinuous, and thus obtain exact p values of the KS test. Our approach is to express the complementary CDF through the rectangle probability for uniform order statistics, and to compute it using fast Fourier transform (FFT). Secondly, we provide a C++ and an R implementation of the proposed method, which fills the existing gap in statistical software. We give also a useful extension of the Schmid’s asymptotic formula for the distribution of the KS statistic, relaxing his requirement for F (x) to be increasing between jumps and thus allowing for any general mixed or purely discrete F (x). The numerical performance of the proposed FFT-based method, implemented both in C++ and in the R package KSgeneral, available from https://CRAN.R-project.org/package=KSgeneral, is illustrated when F (x) is mixed, purely discrete, and continuous. The performance of the general asymptotic formula is also studied. © 2020, American Statistical Association. All rights reserved."
"Phonetic spelling algorithm implementations for r",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092485112&doi=10.18637%2fjss.v095.i08&partnerID=40&md5=79029fd570bb1f12f9067bf909e1f4ea","The phonics package provides several functions for indexing words by their English language pronunciation. Over nearly one hundred years, many different algorithms have been developed to support word and name indexing. From Soundex, developed in the early 20th century and predating the digital computer, through to modern digital phonetic algorithms like Phonex, the phonics package provides support for more than a dozen methods. Together, these provide phonetic algorithms appropriate for use in name indexing and name matching across a variety of English language use cases. © 2020, American Statistical Association. All rights reserved."
"Estimating animal abundance with n-mixture models using the r-inla package for r",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092484842&doi=10.18637%2fjss.v095.i02&partnerID=40&md5=9da7fc13db03cc332e255a3684c07bb5","Successful management of wildlife populations requires accurate estimates of abundance. Abundance estimates can be confounded by imperfect detection during wildlife surveys. N-mixture models enable quantification of detection probability and, under appropriate conditions, produce abundance estimates that are less biased. Here, we demonstrate how to use the R-INLA package for R to analyze N-mixture models, and compare performance of R-INLA to two other common approaches: JAGS (via the runjags package for R), which uses Markov chain Monte Carlo and allows Bayesian inference, and the unmarked package for R, which uses maximum likelihood and allows frequentist inference. We show that R-INLA is an attractive option for analyzing N-mixture models when (i) fast computing times are necessary (R-INLA is 10 times faster than unmarked and 500 times faster than JAGS), (ii) familiar model syntax and data format (relative to other R packages) is desired, (iii) survey-level covariates of detection are not essential, and (iv) Bayesian inference is preferred. © 2020, American Statistical Association. All rights reserved."
"Various versatile variances: An object-oriented implementation of clustered covariances in r",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092479678&doi=10.18637%2fjss.v095.i01&partnerID=40&md5=8a24ea5958d75a528bfca283280a4dbd","Clustered covariances or clustered standard errors are very widely used to account for correlated or clustered data, especially in economics, political sciences, and other social sciences. They are employed to adjust the inference following estimation of a standard least-squares regression or generalized linear model estimated by maximum likelihood. Although many publications just refer to “the” clustered standard errors, there is a surprisingly wide variety of clustered covariances, particularly due to different flavors of bias corrections. Furthermore, while the linear regression model is certainly the most important application case, the same strategies can be employed in more general models (e.g., for zero-inflated, censored, or limited responses). In R, functions for covariances in clustered or panel models have been somewhat scattered or available only for certain modeling functions, notably the (generalized) linear regression model. In contrast, an object-oriented approach to “robust” covariance matrix estimation – applicable beyond lm() and glm() – is available in the sandwich package but has been limited to the case of cross-section or time series data. Starting with sandwich 2.4.0, this shortcoming has been corrected: Based on methods for two generic functions (estfun() and bread()), clustered and panel covariances are provided in vcovCL(), vcovPL(), and vcovPC(). Moreover, clustered bootstrap covariances are provided in vcovBS(), using model update() on bootstrap samples. These are directly applicable to models from packages including MASS, pscl, countreg, and betareg, among many others. Some empirical illustrations are provided as well as an assessment of the methods’ performance in a simulation study. © 2020, American Statistical Association. All rights reserved."
"Multibugs: A parallel implementation of the bugs modeling framework for faster bayesian inference",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092472296&doi=10.18637%2fjss.v095.i07&partnerID=40&md5=ce584a4c3a413b86b8aefa964e1354bf","MultiBUGS is a new version of the general-purpose Bayesian modeling software BUGS that implements a generic algorithm for parallelizing Markov chain Monte Carlo (MCMC) algorithms to speed up posterior inference of Bayesian models. The algorithm parallelizes evaluation of the product-form likelihoods formed when a parameter has many children in the directed acyclic graph (DAG) representation; and parallelizes sampling of conditionally-independent sets of parameters. A heuristic algorithm is used to decide which approach to use for each parameter and to apportion computation across computational cores. This enables MultiBUGS to automatically parallelize the broad range of statistical models that can be fitted using BUGS-language software, making the dramatic speed-ups of modern multi-core computing accessible to applied statisticians, without requiring any experience of parallel programming. We demonstrate the use of Multi-BUGS on simulated data designed to mimic a hierarchical e-health linked-data study of methadone prescriptions including 425,112 observations and 20,426 random effects. Posterior inference for the e-health model takes several hours in existing software, but MultiBUGS can perform inference in only 28 minutes using 48 computational cores. © 2020, American Statistical Association. All rights reserved."
"The r package hmi: A convenient tool for hierarchical multiple imputation and beyond",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092451503&doi=10.18637%2fjss.v095.i09&partnerID=40&md5=80d35e1fc33ce55928ff5f22eed7eaaa","Applications of multiple imputation have long outgrown the traditional context of dealing with item nonresponse in cross-sectional data sets. Nowadays multiple imputation is also applied to impute missing values in hierarchical data sets, address confidentiality concerns, combine data from different sources, or correct measurement errors in surveys. However, software developments did not keep up with these recent extensions. Most imputation software can only deal with item nonresponse in cross-sectional settings and extensions for hierarchical data – if available at all – are typically limited in scope. Furthermore, to our knowledge no software is currently available for dealing with measurement error using multiple imputation approaches. The R package hmi tries to close some of these gaps. It offers multiple imputation routines in hierarchical settings for many variable types (for example, nominal, ordinal, or continuous variables). It also provides imputation routines for interval data and handles a common measurement error problem in survey data: biased inferences due to implicit rounding of the reported values. The user-friendly setup which only requires the data and optionally the specification of the analysis model of interest makes the package especially attractive for users less familiar with the peculiarities of multiple imputation. The compatibility with the popular mice package (Van Buuren and Groothuis-Oudshoorn 2011) ensures that the rich set of analysis and diagnostic tools and post-imputation functions available in mice can be used easily, once the data have been imputed. © 2020, American Statistical Association. All rights reserved."
"Pseudo-ranks: How to calculate them efficiently in r",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092444043&doi=10.18637%2fjss.v095.c01&partnerID=40&md5=d325aad30a7ee94a14c805db5204ae75","Many popular nonparametric inferential methods are based on ranks. Among the most commonly used and most famous tests are for example the Wilcoxon-Mann-Whitney test for two independent samples, and the Kruskal-Wallis test for multiple independent groups. However, recently, it has become clear that the use of ranks may lead to paradoxical results in case of more than two groups. Luckily, these problems can be avoided simply by using pseudo-ranks instead of ranks. These pseudo-ranks, however, suffer from being (a) at first less intuitive and not as straightforward in their interpretation, (b) computationally much more expensive to calculate. The computational cost has been prohibitive, for example, for large-scale simulative evaluations or application of resampling-based pseudorank procedures. In this paper, we provide different algorithms to calculate pseudo-ranks efficiently in order to solve problem (b) and thus render it possible to overcome the current limitations of procedures based on pseudo-ranks. © 2020, American Statistical Association. All rights reserved."
"Preference: an r package for two-stage clinical trial design accounting for patient preference",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090687702&doi=10.18637%2fjss.v094.c02&partnerID=40&md5=e3c3cca51a1a0ba5cd229b2f08cc4181","The consideration of a patient’s treatment preference may be essential in determining how a patient will respond to a particular treatment. While traditional clinical trials are unable to capture these effects, the two-stage randomized preference design provides an important tool for researchers seeking to understand the role of patient preferences. In addition to the treatment effect, these designs seek to estimate the role of preferences through testing of selection and preference effects. The R package preference facilitates the use of two-stage clinical trials by providing the necessary tools to design and analyze these studies. To aid in the design, functions are provided to estimate the required sample size and to estimate the study power when a sample size is fixed. In addition, analysis functions are provided to determine the significance of each effect using either raw data or summary statistics. The package is able to incorporate either an unstratified or stratified preference design. The functionality of the package is demonstrated using data from a study evaluating two management methods in women found to have an atypical Pap smear. © 2020, American Statistical Association. All rights reserved."
"Presiduals: An R package for residual analysis using probability-scale residuals",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090661961&doi=10.18637%2fjss.v094.i12&partnerID=40&md5=9e58e0967053d2f55b17ef468616b980","We present the R package PResiduals for residual analysis using the probability-scale residual. This residual is well defined for a wide variety of outcome types and models, including some settings where other popular residuals are not applicable. It can be used for model diagnostics, tests of conditional associations, and covariate-adjustment for Spearman’s rank correlation. These tests and measures of conditional association are applicable to any orderable variable. They use order information but do not require assigning scores to ordered categorical variables or transforming continuous variables, and therefore, can achieve a good balance between robustness and efficiency. We illustrate the usage of the PResiduals package with a publicly available dataset. © 2020, American Statistical Association. All rights reserved."
"BASS: An R package for fitting and performing sensitivity analysis of bayesian adaptive spline surfaces",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090654257&doi=10.18637%2fjss.v094.i08&partnerID=40&md5=f9b871b980d461e0db8855e0a09e8034","We present the R package BASS as a tool for nonparametric regression. The primary focus of the package is fitting fully Bayesian adaptive spline surface (BASS) models and performing global sensitivity analyses of these models. The BASS framework is similar to that of Bayesian multivariate adaptive regression splines (BMARS) from Denison, Mallick, and Smith (1998), but with many added features. The software is built to efficiently handle significant amounts of data with many continuous or categorical predictors and with functional response. Under our Bayesian framework, most priors are automatic but these can be modified by the user to focus on parsimony and the avoidance of overfitting. If directed to do so, the software uses parallel tempering to improve the reversible jump Markov chain Monte Carlo (RJMCMC) methods used to perform inference. We discuss the implementation of these features and present the performance of BASS in a number of analyses of simulated and real data. © 2020, American Statistical Association. All rights reserved."
"Boin: An r package for designing single-agent and drug-combination dose-finding trials using bayesian optimal interval designs",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090632654&doi=10.18637%2fjss.v094.i13&partnerID=40&md5=9c2e6aca4df26b491072091ff097ed16","This article describes the R package BOIN, which implements a recently developed methodology for designing single-agent and drug-combination dose-finding clinical trials using Bayesian optimal interval designs (Liu and Yuan 2015; Yuan, Hess, Hilsenbeck, and Gilbert 2016). The BOIN designs are novel “model-assisted” phase I trial designs that can be implemented simply and transparently, similar to the 3 + 3 design, but yield excellent performance comparable to those of more complicated, model-based designs. The BOIN package provides tools for designing, conducting, and analyzing single-agent and drug-combination dose-finding trials. © 2020, American Statistical Association. All rights reserved."
"Boosting functional regression models with fdboost",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090623953&doi=10.18637%2fjss.v094.i10&partnerID=40&md5=5711fc6538b43dd2c4bd7b8649ce5305","The R add-on package FDboost is a flexible toolbox for the estimation of functional regression models by model-based boosting. It provides the possibility to fit regression models for scalar and functional response with effects of scalar as well as functional covariates, i.e., scalar-on-function, function-on-scalar and function-on-function regression models. In addition to mean regression, quantile regression models as well as generalized additive models for location scale and shape can be fitted with FDboost. Furthermore, boosting can be used in high-dimensional data settings with more covariates than observations. We provide a hands-on tutorial on model fitting and tuning, including the visualization of results. The methods for scalar-on-function regression are illustrated with spectrometric data of fossil fuels and those for functional response regression with a data set including bioelectrical signals for emotional episodes. © 2020, American Statistical Association. All rights reserved."
"Deconvolver: A g-modeling program for deconvolution and empirical bayes estimation",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090544456&doi=10.18637%2fjss.v094.i11&partnerID=40&md5=1bfc3f1343e6fd5ab4e5e492da44206d","Empirical Bayes inference assumes an unknown prior density g(θ) has yielded (un-observables) Θ1, Θ2, …, ΘN, and each Θi produces an independent observation Xi from pi(Xi |Θi). The marginal density fi(Xi) is a convolution of the prior g and pi . The Bayes deconvolution problem is one of recovering g from the data. Although estimation of g – so called g-modeling – is difficult, the results are more encouraging if the prior g is restricted to lie within a parametric family of distributions. We present a deconvolution approach where g is restricted to be in a parametric exponential family, along with an R package deconvolveR designed for the purpose. © 2020, American Statistical Association. All rights reserved."
"CVXR: An R package for disciplined convex optimization",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090542901&doi=10.18637%2fjss.v094.i14&partnerID=40&md5=9a876a1281f66437cbce6ac8f495e567","CVXR is an R package that provides an object-oriented modeling language for convex optimization, similar to CVX, CVXPY, YALMIP, and Convex.jl. It allows the user to formulate convex optimization problems in a natural mathematical syntax rather than the restrictive form required by most solvers. The user specifies an objective and set of constraints by combining constants, variables, and parameters using a library of functions with known mathematical properties. CVXR then applies signed disciplined convex programming (DCP) to verify the problem’s convexity. Once verified, the problem is converted into standard conic form using graph implementations and passed to a cone solver such as ECOS or SCS. We demonstrate CVXR’s modeling framework with several applications. © 2020, American Statistical Association. All rights reserved."
"Roi: An extensible r optimization infrastructure",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090539318&doi=10.18637%2fjss.v094.i15&partnerID=40&md5=1b8eae8329e92504ec607806c9df83b4","Optimization plays an important role in many methods routinely used in statistics, machine learning and data science. Often, implementations of these methods rely on highly specialized optimization algorithms, designed to be only applicable within a spe-cific application. However, in many instances recent advances, in particular in the field of convex optimization, make it possible to conveniently and straightforwardly use modern solvers instead with the advantage of enabling broader usage scenarios and thus promoting reusability. This paper introduces the R optimization infrastructure ROI which provides an extensible infrastructure to model linear, quadratic, conic and general nonlinear optimization problems in a consistent way. Furthermore, the infrastructure administers many different solvers, reformulations, problem collections and functions to read and write optimization problems in various formats. © 2020, American Statistical Association. All rights reserved."
"Credsubs: Multiplicity-adjusted subset identification",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090518091&doi=10.18637%2fjss.v094.i07&partnerID=40&md5=d59bbd9e10e6def4fd96af3f7eadb262","Subset identification methods are used to select the subset of a covariate space over which the conditional distribution of a response has certain properties – for example, identifying types of patients whose conditional treatment effect is positive. An often critical requirement of subset identification methods is multiplicity control, by which the family-wise Type I error rate is controlled, rather than the Type I error rate of each covariate-determined hypothesis separately. The credible subset (or credible subgroup) method provides a multiplicity-controlled estimate of the target subset in the form of two bounding subsets: one which entirely contains the target subset, and one which is entirely contained by it. We introduce a new R package, credsubs, which constructs credible subset estimates using a sample from the joint posterior distribution of any regression model, a description of the covariate space, and a function mapping the parameters and covariates to the subset criterion. We demonstrate parametric and nonparametric applications of the package to a clinical trial dataset and a neuroimaging dataset, respectively. © 2020, American Statistical Association. All rights reserved."
"Hybridmodels: An r package for the stochastic simulation of disease spreading in dynamic networks",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087363134&doi=10.18637%2fjss.v094.i06&partnerID=40&md5=ca13bf5041344c2450a4ffa306cd2c85","Disease spreading simulations are traditionally performed using coupled differential equations. However, in the setting of metapopulations, most of the solutions provided by this method do not account for the dynamic topography of subpopulations. Conversely, the alternative approach of individual-based modeling (IBM) may add computational cost and complexity. Hybrid models allow for the study of disease spreading because they combine both aforementioned approaches by separating them across different scales: a local scale that addresses subpopulation dynamics using coupled differential equations and a global scale that addresses the contact between these subpopulations using IBM. We present a simple way of simulating the spread of disease in dynamic networks using the high-level statistical computational language R and the hybridModels package. We built four examples using disease spread models at the local scale in several different networks: an animal movement network; a three-node network, whose model solution using a stochastic simulation algorithm is compared with the ordinary differential equations approach; the commuting of individuals between patches, which we compare with the permanent migration of individuals; and the commuting of individuals within the metropolitan area of São Paulo. © 2020, American Statistical Association. All rights reserved."
"Covatest: An R package for selecting a class of space-time covariance functions",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087349990&doi=10.18637%2fjss.v094.i01&partnerID=40&md5=cd7528fe49604836c8d5bc8d53a4cfdc","Although a very rich list of classes of space-time covariance functions exists, specific tools for selecting the appropriate class for a given data set are needed. Thus, the main topic of this paper is to present the new R package, covatest, which can be used for testing some characteristics of a covariance function, such as symmetry, separability and type of non-separability, as well as for testing the adequacy of some classes of space-time covariance models. These last aspects can be relevant for choosing a suitable class of covariance models. The proposed results have been applied to an environmental case study. © 2020, American Statistical Association. All rights reserved."
"R package obsmd for follow-up designs in an objective bayesian framework",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087347740&doi=10.18637%2fjss.v094.i02&partnerID=40&md5=000dfa1c248c1fa244d5b86e4fc7e68a","Fractional factorial experiments often produce ambiguous results due to confounding among the factors; as a consequence more than one model is consistent with the data. Thus, the practical problem is how to choose additional runs in order to discriminate among the rival models and to identify the active factors. The R package OBsMD solves this problem by implementing the objective Bayesian methodology proposed by Consonni and Deldossi (2016). The main feature of this approach is that the follow-up designs are obtained through the use of just two functions, OBsProb() and OMD() without requiring any prior specifications, being fully automatic. Thus OBsMD provides a simple tool for conducting a design of experiments to solve real world problems. © 2020, American Statistical Association. All rights reserved."
"Webchem: An R package to retrieve chemical information from the web",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085902470&doi=10.18637%2fjss.v093.i13&partnerID=40&md5=0d83ad53260ae792ef94d9cb05249d15","A wide range of chemical information is freely available online, including identifiers, experimental and predicted chemical properties. However, these data are scattered over various data sources and not easily accessible to researchers. Manual searching and downloading of such data is time-consuming and error-prone. We developed the open-source R package webchem that allows users to automatically query chemical data from currently 14 web sources. These cover a broad spectrum of information. The data are automatically imported into an R object and can directly be used in subsequent analyses. webchem enables easy, structured and reproducible data retrieval and usage from publicly available web sources. In addition, it facilitates data cleaning, identification and reporting of substances. Consequently, it reduces the time researchers need to spend on chemical data compilation. © 2020, American Statistical Association. All rights reserved."
"Idem: An R package for inferences in clinical trials with death and missingness",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085883420&doi=10.18637%2fjss.v093.i12&partnerID=40&md5=3cb99670a4e142e2eb417644157db0a4","In randomized controlled trials of seriously ill patients, death is common and often defined as the primary endpoint. Increasingly, non-mortality outcomes such as functional outcomes are co-primary or secondary endpoints. Functional outcomes are not defined for patients who die, referred to as “truncation due to death”, and among survivors, functional outcomes are often unobserved due to missed clinic visits or loss to follow-up. It is well known that if the functional outcomes “truncated due to death” or missing are handled inappropriately, treatment effect estimation can be biased. In this paper, we describe the package idem that implements a procedure for comparing treatments that is based on a composite endpoint of mortality and the functional outcome among survivors. Among survivors, the procedure incorporates a missing data imputation procedure with a sensitivity analysis strategy. A web-based graphical user interface is provided in the idem package to facilitate users conducting the proposed analysis in an interactive and user-friendly manner. We demonstrate idem using data from a recent trial of sedation interruption among mechanically ventilated patients. © 2020, American Statistical Association. All rights reserved."
"Network coincidence analysis: The netcoin R package",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085882605&doi=10.18637%2fjss.v093.i11&partnerID=40&md5=839b8ec70384df1c454cea239662dbbe","The aim of the R package netCoin is to explore data structures using a number of statistical techniques that share the handling of interdependent variables. The main objective of this analysis is to detect events, characters, objects, attributes or characteristics that tend to appear together within a given set of scenarios. Its most notable feature is the combination of traditional multivariate statistical analysis and network analysis supported by topological graph theory. In addition, netCoin produces HTML graphs using the D3.js visualization library to support the interactive exploration of networked data. Among its many applications, netCoin can be used for the analysis of multiple responses in questionnaires to explore relevant associations, for the development of textual networks, for the study of ecological communities, for audience analysis, for mining large databases or for basket market analysis. © 2020, American Statistical Association. All rights reserved."
"A computer algebra system for R: Macaulay2 and the m2r package",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085872863&doi=10.18637%2fjss.v093.i09&partnerID=40&md5=8a2ab25f32d9fbabfca9b50aefe57605","Algebraic methods have a long history in statistics. Apart from the ubiquitous applications of linear algebra, the most visible manifestations of modern algebra in statistics are found in the young field of algebraic statistics, which brings tools from commutative algebra and algebraic geometry to bear on statistical problems. Now over two decades old, algebraic statistics has applications in a wide range of theoretical and applied statistical domains. Nevertheless, algebraic statistical methods are still not mainstream, mostly due to a lack of easy off-the-shelf implementations. In this article we debut m2r, an R package that connects R to Macaulay2 through a persistent back-end socket connection running locally or on a cloud server. Topics range from basic use of m2r to applications and design philosophy. © 2020, American Statistical Association. All rights reserved."
"Gdina: An R package for cognitive diagnosis modeling",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085769787&doi=10.18637%2fjss.v093.i14&partnerID=40&md5=fc1e2330ee9278342bd7c3d4e1999538","Cognitive diagnosis models (CDMs) have attracted increasing attention in educational measurement because of their potential to provide diagnostic feedback about students’ strengths and weaknesses. This article introduces the feature-rich R package GDINA for conducting a variety of CDM analyses. Built upon a general model framework, a number of CDMs can be calibrated using the GDINA package. Functions are also available for evaluating model-data fit, detecting differential item functioning, validating the item and attribute association, and examining classification accuracy. A grapical user interface is also provided for researchers who are less familar with R. This paper contains both technical details about model estimation and illustrations about how to use the package for data analysis. The GDINA package is also used to replicate published results, showing that it could provide comparable model parameter estimation. © 2020, American Statistical Association. All rights reserved."
"Analysis of archaeological phases using the r package archaeophases",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085759868&doi=10.18637%2fjss.v093.c01&partnerID=40&md5=80c9160553a70c161ec5bd6e152f6366","We propose new statistical tools to analyze and to estimate the characteristics of time periods based on the posterior distribution of an associated collection of dates. These tools are implemented in the R package ArchaeoPhases. The required inputs are simu-lated samples from the posterior distribution of a collection of dates. Such Markov chain Monte Carlo samples are provided, for instance, by BCal, ChronoModel, or OxCal, freely available software applications built for the chronological modeling of estimated dates. We give a practical introduction to the package ArchaeoPhases using published data and comment on the statistical results. © 2020, American Statistical Association. All rights reserved."
"Hierarchical archimedean copulas for matlab and octave: The hacopula toolbox",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085741796&doi=10.18637%2fjss.v093.i10&partnerID=40&md5=ba548e0540cbfd9119682e3d97af28df","To extend the current implementation of copulas in MATLAB to non-elliptical distributions in arbitrary dimensions enabling for asymmetries in the tails, the toolbox HACopula provides functionality for modeling with hierarchical (or nested) Archimedean copulas. This includes their representation as MATLAB objects, evaluation, sampling, estimation and goodness-of-fit testing, as well as tools for their visual representation or computation of corresponding matrices of Kendall’s tau and tail dependence coefficients. These are first presented in a quick-and-simple manner and then elaborated in more detail to show the full capability of HACopula. As an example, sampling, estimation and goodness-of-fit of a 100-dimensional hierarchical Archimedean copula is presented, including a speed up of its computationally most demanding part. The toolbox is also compatible with Octave, where no support for copulas in more than two dimensions is currently provided. © 2020, American Statistical Association. All rights reserved."
"Mvord: An R package for fitting multivariate ordinal regression models",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085051358&doi=10.18637%2fjss.v093.i04&partnerID=40&md5=6204d6f690b938f982e310ffba7a70c9","The R package mvord implements composite likelihood estimation in the class of multivariate ordinal regression models with a multivariate probit and a multivariate logit link. A flexible modeling framework for multiple ordinal measurements on the same subject is set up, which takes into consideration the dependence among the multiple observations by employing different error structures. Heterogeneity in the error structure across the subjects can be accounted for by the package, which allows for covariate dependent error structures. In addition, different regression coefficients and threshold parameters for each response are supported. If a reduction of the parameter space is desired, constraints on the threshold as well as on the regression coefficients can be specified by the user. The proposed multivariate framework is illustrated by means of a credit risk application. © 2020, American Statistical Association. All rights reserved."
"Working with user agent strings in stata: The parseuas command",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084992340&doi=10.18637%2fjss.v092.c01&partnerID=40&md5=a6c7fdb22a7bdd191e5afb98567d8e68","With the rising popularity of web surveys and the increasing use of paradata by survey methodologists, assessing information stored in user agent strings becomes inevitable. These data contain meaningful information about the browser, operating system, and device that a survey respondent uses. This article provides an overview of user agent strings, their specific structure and history, how they can be obtained when conducting a web survey, as well as what kind of information can be extracted from the strings. Further, the user written command parseuas is introduced as an efficient means to gather detailed information from user agent strings. The application of parseuas is illustrated by an example that draws on a pooled data set consisting of 29 web surveys. © 2020, American Statistical Association. All rights reserved."
"Flexible imputation of missing data 2nd edition",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084846489&doi=10.18637%2fjss.v093.b01&partnerID=40&md5=24d42bd9152316dfc7a80bfa2e51b5c8",[No abstract available]
"MGM: Estimating time-varying mixed graphical models in high-dimensional data",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084415721&doi=10.18637%2fjss.v093.i08&partnerID=40&md5=58dcb42b676652aa05da733140f8756d","We present the R package mgm for the estimation of k-order mixed graphical models (MGMs) and mixed vector autoregressive (mVAR) models in high-dimensional data. These are a useful extensions of graphical models for only one variable type, since data sets consisting of mixed types of variables (continuous, count, categorical) are ubiquitous. In addition, we allow to relax the stationarity assumption of both models by introducing time-varying versions of MGMs and mVAR models based on a kernel weighting approach. Time-varying models offer a rich description of temporally evolving systems and allow to identify external influences on the model structure such as the impact of interventions. We provide the background of all implemented methods and provide fully reproducible examples that illustrate how to use the package. © 2020, American Statistical Association. All rights reserved."
"Lmsubsets: Exact variable-subset selection in linear regression for R",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084227429&doi=10.18637%2fjss.v093.i03&partnerID=40&md5=1ff4b0bb41451a2401423e6a8c9d9a8f","An R package for computing the all-subsets regression problem is presented. The proposed algorithms are based on computational strategies recently developed. A novel algorithm for the best-subset regression problem selects subset models based on a pre-determined criterion. The package user can choose from exact and from approximation algorithms. The core of the package is written in C++ and provides an efficient implementation of all the underlying numerical computations. A case study and benchmark results illustrate the usage and the computational efficiency of the package. © 2020, American Statistical Association. All rights reserved."
"Object-oriented software for functional data",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084125520&doi=10.18637%2fjss.v093.i05&partnerID=40&md5=ab79fe07f6e8531f521f2ae3f9f620e1","This paper introduces the funData R package as an object-oriented implementation of functional data. It implements a unified framework for dense univariate and multivariate functional data on one-and higher dimensional domains as well as for irregular functional data. The aim of this package is to provide a user-friendly, self-contained core toolbox for functional data, including important functionalities for creating, accessing and modifying functional data objects, that can serve as a basis for other packages. The package further contains a full simulation toolbox, which is a useful feature when implementing and testing new methodological developments. Based on the theory of object-oriented data analysis, it is shown why it is natural to implement functional data in an object-oriented manner. The classes and methods provided by funData are illustrated in many examples using two freely available datasets. The MFPCA package, which implements multivariate functional principal component analysis, is presented as an example for an advanced methodological package that uses the funData package as a basis, including a case study with real data. Both packages are publicly available on GitHub and the Comprehensive R Archive Network. © 2020, American Statistical Association. All rights reserved."
"Bayesian random-effects meta-analysis using the bayesmeta r package",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084121680&doi=10.18637%2fjss.v093.i06&partnerID=40&md5=28bcafb3dc05b40f0d684a1d70530a17","The random-effects or normal-normal hierarchical model is commonly utilized in a wide range of meta-analysis applications. A Bayesian approach to inference is very at-tractive in this context, especially when a meta-analysis is based only on few studies. The bayesmeta R package provides readily accessible tools to perform Bayesian meta-analyses and generate plots and summaries, without having to worry about computational details. It allows for flexible prior specification and instant access to the resulting posterior distri-butions, including prediction and shrinkage estimation, and facilitating for example quick sensitivity checks. The present paper introduces the underlying theory and showcases its usage. © 2020, American Statistical Association. All rights reserved."
"Lslx: Semi-confirmatory structural equation modeling via penalized likelihood",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084119102&doi=10.18637%2fjss.v093.i07&partnerID=40&md5=23932c6f44d3a07d6f39a3e99c11514b","Sparse estimation via penalized likelihood (PL) is now a popular approach to learn the associations among a large set of variables. This paper describes an R package called lslx that implements PL methods for semi-confirmatory structural equation modeling (SEM). In this semi-confirmatory approach, each model parameter can be specified as free/fixed for theory testing, or penalized for exploration. By incorporating either a L1 or minimax concave penalty, the sparsity pattern of the parameter matrix can be efficiently explored. Package lslx minimizes the PL criterion through a quasi-Newton method. The algorithm conducts line search and checks the first-order condition in each iteration to ensure the optimality of the obtained solution. A numerical comparison between competing packages shows that lslx can reliably find PL estimates with the least time. The current package also supports other advanced functionalities, including a two-stage method with auxiliary variables for missing data handling and a reparameterized multi-group SEM to explore population heterogeneity. © 2020, American Statistical Association. All rights reserved."
"Semi-parametric joint modeling of survival and longitudinal data: The R package JSM",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083988688&doi=10.18637%2fjss.v093.i02&partnerID=40&md5=cb4143e3426f9c5f954b435b7ac27968","This paper is devoted to the R package JSM which performs joint statistical modeling of survival and longitudinal data. In biomedical studies it has been increasingly common to collect both baseline and longitudinal covariates along with a possibly censored survival time. Instead of analyzing the survival and longitudinal outcomes separately, joint modeling approaches have attracted substantive attention in the recent literature and have been shown to correct biases from separate modeling approaches and enhance information. Most existing approaches adopt a linear mixed effects model for the longitudinal component and the Cox proportional hazards model for the survival component. We extend the Cox model to a more general class of transformation models for the survival process, where the baseline hazard function is completely unspecified leading to semiparametric survival models. We also offer a non-parametric multiplicative random effects model for the longitudinal process in JSM in addition to the linear mixed effects model. In this paper, we present the joint modeling framework that is implemented in JSM, as well as the standard error estimation methods, and illustrate the package with two real data examples: A liver cirrhosis data and a Mayo Clinic primary biliary cirrhosis data. © 2020, American Statistical Association. All rights reserved."
"Manifoldoptim: An R interface to the ROPTLIB library for Riemannian manifold optimization",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083955793&doi=10.18637%2fjss.v093.i01&partnerID=40&md5=f3afd1fb7df62114bb67ffd6ceb4f48f","Manifold optimization appears in a wide variety of computational problems in the applied sciences. In recent statistical methodologies such as sufficient dimension reduction and regression envelopes, estimation relies on the optimization of likelihood functions over spaces of matrices such as the Stiefel or Grassmann manifolds. Recently, Huang, Absil, Gallivan, and Hand (2016) have introduced the library ROPTLIB, which provides a framework and state of the art algorithms to optimize real-valued objective functions over commonly used matrix-valued Riemannian manifolds. This article presents ManifoldOptim, an R package that wraps the C++ library ROPTLIB. ManifoldOptim enables users to access functionality in ROPTLIB through R so that optimization problems can easily be constructed, solved, and integrated into larger R codes. Computationally intensive problems can be programmed with Rcpp and RcppArmadillo, and otherwise accessed through R. We illustrate the practical use of ManifoldOptim through several motivating examples involving dimension reduction and envelope methods in regression. © 2020, American Statistical Association. All rights reserved."
"Fitting prediction rule ensembles with R package pre",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083834316&doi=10.18637%2fjss.v092.i12&partnerID=40&md5=654558002ee47a15248ac2fa5890c62e","Prediction rule ensembles (PREs) are sparse collections of rules, offering highly inter-pretable regression and classification models. This paper shows how they can be fitted using function pre from R package pre, which derives PREs largely through the method-ology of Friedman and Popescu (2008). The implementation and functionality of pre is described and illustrated through application on a dataset on the prediction of depres-sion. Furthermore, accuracy and sparsity of pre is compared with that of single trees, random forests, lasso regression and the original RuleFit implementation of Friedman and Popescu (2008) in four benchmark datasets. Results indicate that pre derives ensembles with predictive accuracy similar to that of random forests, while using a smaller number of variables for prediction. Furthermore, pre provided better accuracy and sparsity than the original RuleFit implementation. © 2020, American Statistical Association. All rights reserved."
"A data envelopment analysis toolbox for matlab",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081895169&doi=10.18637%2fjss.v095.i03&partnerID=40&md5=e9fbd49fc29522eefba33c76362f7b60","The Data Envelopment Analysis Toolbox is a new package for MATLAB that includes functions to calculate the main data envelopment analysis models. The package includes code for the standard radial input, output and additive measures, allowing for constant and variable returns to scale, as well as recent developments related to the directional distance function, and including both desirable and undesirable outputs when measuring efficiency and productivity; i.e., Malmquist and Malmquist-Luenberger indices. Bootstrapping to perform statistical analysis is also included. This paper describes the methodology and implementation of the functions, and reports numerical results using a reliable productivity database on US agriculture to illustrate their use. © 2020, American Statistical Association. All rights reserved."
"Gdpc: An R package for generalized dynamic principal components",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081637631&doi=10.18637%2fjss.v092.c02&partnerID=40&md5=a370df3a598fa1a07c85f304b927627a","Gdpc is an R package for the computation of the generalized dynamic principal components proposed in Peña and Yohai (2016). In this paper, we briefly introduce the problem of dynamical principal components, propose a solution based on a reconstruction criteria and present an automatic procedure to compute the optimal reconstruction. This solution can be applied to the non-stationary case, where the components need not be a linear combination of the observations, as is the case in the proposal of Brillinger (1981). This article discusses some new features that are included in the package and that were not considered in Peña and Yohai (2016). The most important one is an automatic procedure for the identification of both the number of lags to be used in the generalized dynamic principal components as well as the number of components required for a given reconstruction accuracy. These tools make it easy to use the proposed procedure in large data sets. The procedure can also be used when the number of series is larger than the number of observations. We describe an iterative algorithm and present an example of the use of the package with real data. © 2020, American Statistical Association. All rights reserved."
"The moeadr package: A component-based framework for multiobjective evolutionary algorithms based on decomposition",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081605956&doi=10.18637%2fjss.v092.i06&partnerID=40&md5=3749d2417872caaa182fa0a5deac54c7","Multiobjective evolutionary algorithms based on decomposition (MOEA/D) represent a widely used class of population-based metaheuristics for the solution of multicriteria optimization problems. We introduce the MOEADr package, which offers many of these variants as instantiations of a component-oriented framework. This approach contributes for easier reproducibility of existing MOEA/D variants from the literature, as well as for faster development and testing of new composite algorithms. The package offers an standardized, modular implementation of MOEA/D based on this framework, which was designed aiming at providing researchers and practitioners with a standard way to discuss and express MOEA/D variants. In this paper we introduce the design principles behind the MOEADr package, as well as its current components. Three case studies are provided to illustrate the main aspects of the package. © 2020, American Statistical Association. All rights reserved."
"Computing the oja median in R: The package ojaNP",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081549388&doi=10.18637%2fjss.v092.i08&partnerID=40&md5=f460ee2c8b9be475b12cbf9205739bb9","The Oja median is one of several extensions of the univariate median to the multivariate case. It has many desirable properties, but is computationally demanding. In this paper, we first review the properties of the Oja median and compare it to other multivariate medians. Then, we discuss four algorithms to compute the Oja median, which are implemented in our R package OjaNP. Besides these algorithms, the package contains also functions to compute Oja signs, Oja signed ranks, Oja ranks, and the related scatter concepts. To illustrate their use, the corresponding multivariate one-and C-sample location tests are implemented. © 2020, American Statistical Association. All rights reserved."
"Spbayessurv: Fitting bayesian spatial survival models using R",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081038462&doi=10.18637%2fjss.v092.i09&partnerID=40&md5=c28c8b28a22483c68ce685c2a49aa546","Spatial survival analysis has received a great deal of attention over the last 20 years due to the important role that geographical information can play in predicting survival. This paper provides an introduction to a set of programs for implementing some Bayesian spatial survival models in R using the package spBayesSurv. The function survregbayes includes the three most commonly-used semiparametric models: proportional hazards, proportional odds, and accelerated failure time. All manner of censored survival times are simultaneously accommodated including uncensored, interval censored, current-status, left and right censored, and mixtures of these. Left-truncated data are also accommodated. Time-dependent covariates are allowed under the piecewise constant assumption. Both georeferenced and areally observed spatial locations are handled via frailties. Model fit is assessed with conditional Cox-Snell residual plots, and model choice is carried out via the log pseudo marginal likelihood, the deviance information criterion and the Watanabe-Akaike information criterion. The accelerated failure time frailty model with a covariate-dependent baseline is included in the function frailtyGAFT. In addition, the package also provides two marginal survival models: proportional hazards and linear dependent Dirichlet process mixtures, where the spatial dependence is modeled via spatial copulas. Note that the package can also handle non-spatial data using non-spatial versions of the aforementioned models. © 2020, American Statistical Association. All rights reserved."
"Algebraic analysis of multiple social networks with multiplex",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081012411&doi=10.18637%2fjss.v092.i11&partnerID=40&md5=6b3edb5d439450c5e841d224c1b0302d","Multiplex is a computer program that provides algebraic tools for the analysis of multiple network structures within the R environment. Apart from the possibility to create and manipulate multivariate data representing multiplex, signed, and two-mode networks, this package offers a collection of functions that deal with algebraic systems – such as the partially ordered semigroup, and balance or cluster semirings – their decomposition, and the enumeration of bundle patterns occurring at different levels of the network. Moreover, through Galois derivations between families of the pairs of subsets in different domains it is possible to analyze affiliation networks with an algebraic approach. Visualization of multigraphs, different forms of bipartite graphs, inclusion lattices, Cayley graphs is supported as well with related packages. © 2020, American Statistical Association. All rights reserved."
"Bridgesampling: An R package for estimating normalizing constants",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081008310&doi=10.18637%2fjss.v092.i10&partnerID=40&md5=66ae861a66dff61ac5f3233a3980645c","Statistical procedures such as Bayes factor model selection and Bayesian model averaging require the computation of normalizing constants (e.g., marginal likelihoods). These normalizing constants are notoriously difficult to obtain, as they usually involve high-dimensional integrals that cannot be solved analytically. Here we introduce an R package that uses bridge sampling (Meng and Wong 1996; Meng and Schilling 2002) to estimate normalizing constants in a generic and easy-to-use fashion. For models implemented in Stan, the estimation procedure is automatic. We illustrate the functionality of the package with three examples. © 2020, American Statistical Association. All rights reserved."
"Most likely transformations: The mlt package",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079885615&doi=10.18637%2fjss.v092.i01&partnerID=40&md5=5f79959403fefa73aca48ba5cb440628","The mlt package implements maximum likelihood estimation in the class of conditional transformation models. Based on a suitable explicit parameterization of the unconditional or conditional transformation function using infrastructure from package basefun, we show how one can define, estimate, and compare a cascade of increasingly complex transformation models in the maximum likelihood framework. Models for the unconditional or conditional distribution function of any univariate response variable are set-up and estimated in the same computational framework simply by choosing an appropriate transformation function and parameterization thereof. As it is computationally cheap to evaluate the distribution function, models can be estimated by maximization of the exact likelihood, especially in the presence of random censoring or truncation. The relatively dense high-level implementation in the R system for statistical computing allows generalization of many established implementations of linear transformation models, such as the Cox model or other parametric models for the analysis of survival or ordered categorical data, to the more complex situations illustrated in this paper. © 2020, American Statistical Association. All rights reserved."
"Integration of R and Scala using rscala",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079882549&doi=10.18637%2fjss.v092.i04&partnerID=40&md5=d1665e579e1ecd21075a93231042f902","The rscala software is a simple, two-way bridge between R and Scala that allows users to leverage the unique strengths of both languages in a single project. Scala classes can be instantiated from R and Scala methods can be called. Arbitrary Scala code can be executed on-the-fly from within R and callbacks to R are supported. R packages can be developed based on Scala. Conversely, rscala also enables R code to be embedded within a Scala application. The rscala package is available from the Comprehensive R Archive Network (CRAN) and has no dependencies beyond base R and the Scala standard library. © 2020, American Statistical Association. All rights reserved."
"PAFit: An R package for the non-parametric estimation of preferential attachment and node fitness in temporal complex networks",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079858736&doi=10.18637%2fjss.v092.i03&partnerID=40&md5=e1f7068ecccb3297fc1eb8b06911da93","Many real-world systems are profitably described as complex networks that grow over time. Preferential attachment and node fitness are two simple growth mechanisms that not only explain certain structural properties commonly observed in real-world systems, but are also tied to a number of applications in modeling and inference. While there are statistical packages for estimating various parametric forms of the preferential attachment function, there is no such package implementing non-parametric estimation procedures. The non-parametric approach to the estimation of the preferential attachment function allows for comparatively finer-grained investigations of the “rich-get-richer” phenomenon that could lead to novel insights in the search to explain certain nonstandard structural properties observed in real-world networks. This paper introduces the R package PAFit, which implements non-parametric procedures for estimating the preferential attachment function and node fitnesses in a growing network, as well as a number of functions for generating complex networks from these two mechanisms. The main computational part of the package is implemented in C++ with OpenMP to ensure scalability to large-scale networks. In this paper, we first introduce the main functionalities of PAFit through simulated examples, and then use the package to analyze a collaboration network between scientists in the field of complex networks. The results indicate the joint presence of “rich-get-richer” and “fit-get-richer” phenomena in the collaboration network. The estimated attachment function is observed to be near-linear, which we interpret as meaning that the chance an author gets a new collaborator is proportional to their current number of collaborators. Furthermore, the estimated author fitnesses reveal a host of familiar faces from the complex networks community among the field’s topmost fittest network scientists. © 2020, American Statistical Association. All rights reserved."
"The calculus of m-estimation in R with geex",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079855185&doi=10.18637%2fjss.v092.i02&partnerID=40&md5=1e71f6db520ee1cb6d341d6857f1315f","M-estimation, or estimating equation, methods are widely applicable for point estimation and asymptotic inference. In this paper, we present an R package that can find roots and compute the empirical sandwich variance estimator for any set of user-specified, unbiased estimating equations. Examples from the M-estimation primer by Stefanski and Boos (2002) demonstrate use of the software. The package also includes a framework for finite sample, heteroscedastic, and autocorrelation variance corrections, and a website with an extensive collection of tutorials. © 2020, American Statistical Association. All rights reserved."
"SQUAREM: An R package for off-the-shelf acceleration of EM, MM and other EM-like monotone algorithms",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079840287&doi=10.18637%2fjss.v092.i07&partnerID=40&md5=b00560de2e20b980eb334b6f460c6a85","We discuss the R package SQUAREM for accelerating iterative algorithms which exhibit slow, monotone convergence. These include the well-known expectation-maximization algorithm, majorize-minimize (MM), and other EM-like algorithms such as expectation conditional maximization, and generalized EM algorithms. We demonstrate the simplicity, generality, and power of SQUAREM through a wide array of applications of EM/MM problems, including binary Poisson mixture, factor analysis, interval censoring, genetics admixture, and logistic regression maximum likelihood estimation (an MM problem). We show that SQUAREM is easy to apply, and can accelerate any fixed-point, smooth, contraction mapping with linear convergence rate. The squared iterative scheme (SQUAREM) algorithm provides significant speed-up of EM-like algorithms. The margin of the advantage for SQUAREM is especially huge for high-dimensional problems or when the EM step is relatively time-consuming to evaluate. SQUAREM can be used off-the-shelf since there is no need for the user to tweak any control parameters to optimize performance. Given its remarkable ease of use, SQUAREM may be considered as a default accelerator for slowly converging EM-like algorithms. All the comparisons of CPU computing time in the paper are made on a quad-core 2.3 GHz Intel Core i7 Mac computer. R package SQUAREM is available from the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=SQUAREM/. © 2020, American Statistical Association. All rights reserved."
"BPEC: An R package for Bayesian phylogeographic and ecological clustering",2020,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064846073&doi=10.18637%2fjss.v092.i05&partnerID=40&md5=12b3e302a07caf07c9a628a1371cafcd","BPEC is an R package for Bayesian phylogeographic and ecological clustering which allows geographical, environmental and phenotypic measurements to be combined with deoxyribonucleic acid (DNA) sequences in order to reveal geographic structuring of DNA sequence clusters consistent with migration events. DNA sequences are modelled using a collapsed version of a simplified coalescent model projected onto haplotype trees, which subsequently give rise to constrained clusterings as migrations occur. Within each cluster, a multivariate Gaussian distribution of the covariates (geographical, environmental, phenotypic) is used. Inference follows tailored reversible jump Markov chain Monte Carlo sampling so that the number of clusters (i.e., migrations) does not need to be pre-specified. A number of output plots and visualizations are provided which reflect the posterior distribution of the parameters of interest. BPEC also includes functions that create output files which can be loaded into Google Earth. The package commands are illustrated through an example dataset of the polytypic Near Eastern brown frog Rana macrocnemis analyzed using BPEC. © 2020, American Statistical Association. All rights reserved."
"DClusterm: Model-based detection of disease clusters",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073756187&doi=10.18637%2fjss.v090.i14&partnerID=40&md5=83334799f1f096d31a613ae32621109d","The detection of regions with unusually high risk plays an important role in disease mapping and the analysis of public health data. In particular, the detection of groups of areas (i.e., clusters) where the risk is significantly high is often conducted by public health authorities. Many methods have been proposed for the detection of these disease clusters, most of them based on moving windows, such as Kulldorff’s spatial scan statistic. Here we describe a model-based approach for the detection of disease clusters implemented in the DClusterm package. Our model-based approach is based on representing a large number of possible clusters by dummy variables and then fitting many generalized linear models to the data where these covariates are included one at a time. Cluster detection is done by performing a variable or model selection among all fitted models using different criteria. Because of our model-based approach, cluster detection can be performed using different types of likelihoods and latent effects. We cover the detection of spatial and spatiotemporal clusters, as well as how to account for covariates, zero-inflated datasets and overdispersion in the data. © 2019, American Statistical Association. All rights reserved."
"Flexible regression models for count data based on renewal processes: The countr package",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073737564&doi=10.18637%2fjss.v090.i13&partnerID=40&md5=07a42a530298997e6ce18ff78b7626d5","A new alternative to the standard Poisson regression model for count data is suggested. This new family of models is based on discrete distributions derived from renewal processes, i.e., distributions of the number of events by some time t. Unlike the Poisson model, these models have, in general, time-dependent hazard functions. Any survival distribution can be used to describe the inter-arrival times between events, which gives a rich class of count processes with great flexibility for modelling both underdispersed and overdispersed data. The R package Countr provides a function, renewalCount(), for fitting renewal count regression models and methods for working with the fitted models. The interface is designed to mimic the glm() interface and standard methods for model exploration, diagnosis and prediction are implemented. Package Countr implements stateof- the-art recently developed methods for fast computation of the count probabilities. The package functionalities are illustrated using several datasets. © 2019, American Statistical Association. All rights reserved."
"Nprobust: Nonparametric Kernel-based estimation and robust bias-corrected inference",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076183463&doi=10.18637%2fjss.v091.i08&partnerID=40&md5=548ce7752ed3dbc93272adc79588d9bc","Nonparametric kernel density and local polynomial regression estimators are very popular in statistics, economics, and many other disciplines. They are routinely employed in applied work, either as part of the main empirical analysis or as a preliminary ingredient entering some other estimation or inference procedure. This article describes the main methodological and numerical features of the software package nprobust, which offers an array of estimation and inference procedures for nonparametric kernel-based density and local polynomial regression methods, implemented in both the R and Stata statistical platforms. The package includes not only classical bandwidth selection, estimation, and inference methods (Wand and Jones 1995; Fan and Gijbels 1996), but also other recent developments in the statistics and econometrics literatures such as robust bias-corrected inference and coverage error optimal bandwidth selection (Calonico, Cattaneo, and Farrell 2018, 2019a). Furthermore, this article also proposes a simple way of estimating optimal bandwidths in practice that always delivers the optimal mean square error convergence rate regardless of the specific evaluation point, that is, no matter whether it is implemented at a boundary or interior point. Numerical performance is illustrated using an empirical application and simulated data, where a detailed numerical comparison with other R packages is given. © 2019, American Statistical Association. All rights reserved."
"Learning large-scale bayesian networks with the sparsebn package",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075460866&doi=10.18637%2fjss.v091.i11&partnerID=40&md5=7f50af8c69ba9a39bc98d405881bec16","Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets often have upwards of thousands – sometimes tens or hundreds of thousands – of variables and far fewer samples. To meet this challenge, we have developed a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing software packages for this task, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. Additionally, the sparsebn package is fully compatible with existing software packages for network analysis. © 2019, American Statistical Association. All rights reserved."
"Siminf: An R package for data-driven stochastic disease spread simulations",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075059116&doi=10.18637%2fjss.v091.i12&partnerID=40&md5=facd3cfcf9871c351a96b43c3fda1fd6","We present the R package SimInf which provides an efficient and very flexible framework to conduct data-driven epidemiological modeling in realistic large scale disease spread simulations. The framework integrates infection dynamics in subpopulations as continuous-time Markov chains using the Gillespie stochastic simulation algorithm and incorporates available data such as births, deaths and movements as scheduled events at predefined time-points. Using C code for the numerical solvers and divide work over multiple processors ensures high performance when simulating a sample outcome. One of our design goals was to make SimInf extendable and enable usage of the numerical solvers from other R extension packages in order to facilitate complex epidemiological research. In this paper, we provide a technical description of the framework and demonstrate its use on some basic examples. We also discuss how to specify and extend the framework with user-defined models. © 2019, American Statistical Association. All rights reserved."
"Evaluating probabilistic forecasts with scoringRules",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075045933&doi=10.18637%2fjss.v090.i12&partnerID=40&md5=29a73882442a73238a65ec4c262e2f33","Probabilistic forecasts in the form of probability distributions over future events have become popular in several fields including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature. © 2019, American Statistical Association. All rights reserved."
"Sgmcmc: An r package for stochastic gradient markov chain monte carlo",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074523210&doi=10.18637%2fjss.v091.i03&partnerID=40&md5=72b48026c0ae30bf53ba1a5ca08841b6","This paper introduces the R package sgmcmc; which can be used for Bayesian inference on problems with large data sets using stochastic gradient Markov chain Monte Carlo (SGMCMC). Traditional Markov chain Monte Carlo (MCMC) methods, such as Metropolis-Hastings, are known to run prohibitively slowly as the data set size increases. SGMCMC solves this issue by only using a subset of data at each iteration. SGMCMC requires calculating gradients of the log-likelihood and log-priors, which can be time consuming and error prone to perform by hand. The sgmcmc package calculates these gradients itself using automatic differentiation, making the implementation of these methods much easier. To do this, the package uses the software library TensorFlow, which has a variety of statistical distributions and mathematical operations as standard, meaning a wide class of models can be built using this framework. SGMCMC has become widely adopted in the machine learning literature, but less so in the statistics community. We believe this may be partly due to lack of software; this package aims to bridge this gap. © 2019, American Statistical Association. All rights reserved."
"Dbscan: Fast density-based clustering with R",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074514721&doi=10.18637%2fjss.v091.i01&partnerID=40&md5=4953ed227b0d06cb20fa54ced80b74a2","This article describes the implementation and use of the R package dbscan, which provides complete and fast implementations of the popular density-based clustering algorithm DBSCAN and the augmented ordering algorithm OPTICS. Package dbscan uses advanced open-source spatial indexing data structures implemented in C++ to speed up computation. An important advantage of this implementation is that it is up-to-date with several improvements that have been added since the original algorithms were publications (e.g., artifact corrections and dendrogram extraction methods for OPTICS). We provide a consistent presentation of the DBSCAN and OPTICS algorithms, and compare dbscan’s implementation with other popular libraries such as the R package fpc, ELKI, WEKA, PyClustering, SciKit-Learn, and SPMF in terms of available features and using an experimental comparison. © 2019, American Statistical Association. All rights reserved."
"Depth and depth-based classification with R package ddalpha",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074499062&doi=10.18637%2fjss.v091.i05&partnerID=40&md5=972a262cb4c8f79193d6ad88e3321d30","Following the seminal idea of Tukey (1975), data depth is a function that measures how close an arbitrary point of the space is located to an implicitly defined center of a data cloud. Having undergone theoretical and computational developments, it is now employed in numerous applications with classification being the most popular one. The R package ddalpha is a software directed to fuse experience of the applicant with recent achievements in the area of data depth and depth-based classification. ddalpha provides an implementation for exact and approximate computation of most reasonable and widely applied notions of data depth. These can be further used in the depth-based multivariate and functional classifiers implemented in the package, where the DDα-procedure is in the main focus. The package is expandable with user-defined custom depth methods and separators. The implemented functions for depth visualization and the built-in benchmark procedures may also serve to provide insights into the geometry of the data and the quality of pattern recognition. © 2019, American Statistical Association. All rights reserved."
"Psychological test toolbox: A new tool to compute factor analysis controlling response bias",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074494081&doi=10.18637%2fjss.v091.i06&partnerID=40&md5=103d3f9f90e5811fd2d61304b4918b69","The effects of response bias in psychological tests have been investigated for years, the two most common being social desirability (SD) and acquiescence (AC). However, the traditional methods for controlling or eliminating the impact of those biases in partici-pants’ scores have several limitations. Some factor analysis-based methods can overcome some of these limitations, such as the procedure proposed by Ferrando, Lorenzo-Seva, and Chico (2009). Nevertheless, this method involves programming skills that are not common among applied researchers or clinicians. Consequently, we have developed a stand-alone, user-friendly application that provides an easy way of using the aforementioned method to perform a factor analysis which controls for the effect of AC and SD. The program has been developed in the MATLAB environment and its distribution is entirely free. © 2019, American Statistical Association. All rights reserved."
"Fuzzy forests: Extending random forest feature selection for correlated, high-dimensional data",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074475858&doi=10.18637%2fjss.v091.i09&partnerID=40&md5=e82cecd6f7f8664d57acbaed91694098","In this paper we introduce fuzzy forests, a novel machine learning algorithm for ranking the importance of features in high-dimensional classification and regression problems. Fuzzy forests is specifically designed to provide relatively unbiased rankings of variable importance in the presence of highly correlated features, especially when the number of features, p, is much larger than the sample size, n (p ≫ n). We introduce our implementation of fuzzy forests in the R package, fuzzyforest. Fuzzy forests works by taking advantage of the network structure between features. First, the features are partitioned into separate modules such that the correlation within modules is high and the correlation between modules is low. The package fuzzyforest allows for easy use of the package WGCNA (weighted gene coexpression network analysis, alternatively known as weighted correlation network analysis) to form modules of features such that the modules are roughly uncorrelated. Then recursive feature elimination random forests (RFE-RFs) are used on each module, separately. From the surviving features, a final group is selected and ranked using one last round of RFE-RFs. This procedure results in a ranked variable importance list whose size is pre-specified by the user. The selected features can then be used to construct a predictive model. © 2019, American Statistical Association. All rights reserved."
"Stm: An R package for structural topic models",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074464991&doi=10.18637%2fjss.v091.i02&partnerID=40&md5=676cbe85f75da6a787ce061401bca7d2","This paper demonstrates how to use the R package stm for structural topic modeling. The structural topic model allows researchers to flexibly estimate a topic model that includes document-level metadata. Estimation is accomplished through a fast variational approximation. The stm package provides many useful features, including rich ways to explore topics, estimate uncertainty, and visualize quantities of interest. © 2019, American Statistical Association. All rights reserved."
"Beyond tandem analysis: Joint dimension reduction and clustering in R",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074461361&doi=10.18637%2fjss.v091.i10&partnerID=40&md5=f90f5c351867c033b557847d186bc1c2","We present the R package clustrd which implements a class of methods that combine dimension reduction and clustering of continuous or categorical data. In particular, for continuous data, the package contains implementations of factorial K-means and reduced K-means; both methods combine principal component analysis with K-means clustering. For categorical data, the package provides MCA K-means, i-FCB and cluster correspondence analysis, which combine multiple correspondence analysis with K-means. Two examples on real data sets are provided to illustrate the usage of the main functions. © 2019, American Statistical Association. All rights reserved."
"Markov-switching GARCH models in R: The MSGARCH package",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074438601&doi=10.18637%2fjss.v091.i04&partnerID=40&md5=fd19853cdc3b667acb7315bc8acc30a6","We describe the package MSGARCH, which implements Markov-switching GARCH (generalized autoregressive conditional heteroscedasticity) models in R with efficient C++ object-oriented programming. Markov-switching GARCH models have become popular methods to account for regime changes in the conditional variance dynamics of time series. The package MSGARCH allows the user to perform simulations as well as maximum likelihood and Bayesian Markov chain Monte Carlo estimations of a very large class of Markov-switching GARCH-type models. The package also provides methods to make single-step and multi-step ahead forecasts of the complete conditional density of the variable of interest. Risk management tools to estimate conditional volatility, value-at-risk, and expected-shortfall are also available. We illustrate the broad functionality of the MSGARCH package using exchange rate and stock market return data. © 2019, American Statistical Association. All rights reserved."
"Multivariate locally stationary wavelet analysis with the mvLSW R package",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073375786&doi=10.18637%2fjss.v090.i11&partnerID=40&md5=f2f0e9888ccc5fdd3e5a0e59e509b04a","This paper describes the R package mvLSW. The package contains a suite of tools for the analysis of multivariate locally stationary wavelet (LSW) time series. Key elements include: (i) the simulation of multivariate LSW time series for a given multivariate evolutionary wavelet spectrum (EWS); (ii) estimation of the time-dependent multivariate EWS for a given time series; (iii) estimation of the time-dependent coherence and partial coherence between time series channels; and, (iv) estimation of approximate confidence intervals for multivariate EWS estimates. A demonstration of the package is presented via both a simulated example and a case study with EuStockMarkets from the datasets package. © 2019, American Statistical Association. All rights reserved."
"Bayesian, and non-bayesian, cause-specific competing-risk analysis for parametric and nonparametric survival functions: The R Package CFC",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073350830&doi=10.18637%2fjss.v089.i09&partnerID=40&md5=b8f5031245eaff7d9b79a1008949e050","The R package CFC performs cause-specific, competing-risk survival analysis by computing cumulative incidence functions from unadjusted, cause-specific survival functions. A high-level API in CFC enables end-to-end survival and competing-risk analysis, using a single-line function call, based on the parametric survival regression models in the survival package. A low-level API allows users to achieve more flexibility by supplying their custom survival functions, perhaps in a Bayesian setting. Utility methods for summarizing and plotting the output allow population-average cumulative incidence functions to be calculated, visualized and compared to unadjusted survival curves. Numerical and computational optimization strategies are employed for efficient and reliable computation of the coupled integrals involved. To address potential integrable singularities caused by infinite cause-specific hazards, particularly near time-from-index of zero, integrals are transformed to remove their dependency on hazard functions, making them solely functions of causespecific, unadjusted survival functions. This implicit variable transformation also provides for easier extensibility of CFC to handle custom survival models since it only requires the users to implement a maximum of one function per cause. The transformed integrals are numerically calculated using a generalization of Simpson’s rule to handle the implicit change of variable from time to survival, while a generalized trapezoidal rule is used as reference for error calculation. An OpenMP-parallelized, efficient C++ implementation – using packages Rcpp and RcppArmadillo – makes the application of CFC in Bayesian settings practical, where a potentially large number of samples represent the posterior distribution of cause-specific survival functions. © 2019 Journal of Statistical Software. All rights reserved."
"Model-Based dose escalation Designs in R with crmPack",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073343768&doi=10.18637%2fjss.v089.i10&partnerID=40&md5=4bbd6165f6af6348cd0ad9aeec266e0e","Model-based dose escalation designs have gained increasing interest due to the need for more efficient and informative Phase I trials. The wide-spread implementation of such designs has been hindered by the need for either licensing specialized commercial software or programming the design and simulations from scratch for each project. The R package crmPack provides a simple and unified object-oriented framework for model-based dose escalation designs. This enables the standard use of such designs, while being able to flexibly adapt and extend them. The framework comprises classes and methods for the data structure including the dose grid, statistical models including prior specification, rules for maximum increments, next best dose, and adaptive stopping and cohort sizes. In addition to multiple modified classic continual reassessment method and escalation with overdose control designs with possibly advanced prior specifications (e.g., minimal informative and mixture priors), crmPack currently features dual-endpoint (safety and biomarker) designs and two-part designs. Optional assignment of a small number of patients in each cohort to placebo instead of treatment enables the use in trials outside oncology. © 2019 Journal of Statistical Software. All rights reserved."
"Efficient code for second order analysis of events on a linear network",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073276331&doi=10.18637%2fjss.v090.i01&partnerID=40&md5=b0fd2551ac094766ff52e5efe89f7666","We describe efficient algorithms and open-source code for the second-order statistical analysis of point events on a linear network. Typical summary statistics are adaptations of Ripley’s K-function and the pair correlation function to the case of a linear network, with distance measured by the shortest path in the network. Simple implementations consume substantial time and memory. For an efficient implementation, the data structure representing the network must be economical in its use of memory, but must also enable rapid searches to be made. We have developed such an efficient implementation in C with an R interface written as an extension to the R package spatstat. The algorithms handle realistic large networks, as we demonstrate using a database of all road accidents recorded in Western Australia. © 2019, American Statistical Association. All rights reserved."
"Cgam: An R package for the constrained generalized additive model",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072077231&doi=10.18637%2fjss.v089.i05&partnerID=40&md5=38494fb958a56d6bbf20a2b5cb538497","The cgam package contains routines to fit the generalized additive model where the components may be modeled with shape and smoothness assumptions. The main routine is cgam and nineteen symbolic routines are provided to indicate the relationship between the response and each predictor, which satisfies constraints such as monotonicity, convexity, their combinations, tree, and umbrella orderings. The user may specify constrained splines to fit the individual components for continuous predictors, and various types of orderings for the ordinal predictors. In addition, the user may specify parametri-cally modeled covariates. Two-way interactions between continuous variables, where the relationship with the response is constrained to be monotone, are modeled with “warped-plane splines.” The set over which the likelihood is maximized is a polyhedral convex cone, and a least-squares solution is obtained by projecting the data vector onto the cone. For generalized models, the fit is obtained through iteratively re-weighted cone projections. The cone information criterion (CIC) is provided and may be used to compare fits for combinations of variables and shapes. The graphical routine plotpersp will plot an estimated mean surface for a selected pair of predictors, given an object fitted with cgam. This package is available from the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=cgam. © 2019, American Statistical Association. All rights reserved."
"Randomized matrix decompositions using R",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071912124&doi=10.18637%2fjss.v089.i11&partnerID=40&md5=7d53681aa2c2e98dbcb2c2dc8dbdcfba","Matrix decompositions are fundamental tools in the area of applied mathematics, statistical computing, and machine learning. In particular, low-rank matrix decompositions are vital, and widely used for data analysis, dimensionality reduction, and data compression. Massive datasets, however, pose a computational challenge for traditional algorithms, placing significant constraints on both memory and processing power. Recently, the powerful concept of randomness has been introduced as a strategy to ease the computational load. The essential idea of probabilistic algorithms is to employ some amount of randomness in order to derive a smaller matrix from a high-dimensional data matrix. The smaller matrix is then used to compute the desired low-rank approximation. Such algorithms are shown to be computationally efficient for approximating matrices with low-rank structure. We present the R package rsvd, and provide a tutorial introduction to randomized matrix decompositions. Specifically, randomized routines for the singular value decomposition, (robust) principal component analysis, interpolative decomposition, and CUR decomposition are discussed. Several examples demonstrate the routines, and show the computational advantage over other methods implemented in R. © 2019 Journal of Statistical Software. All rights reserved."
"Polychrome: Creating and assessing qualitative palettes with many colors",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071040899&doi=10.18637%2fjss.v090.c01&partnerID=40&md5=d3351fa7fefdbeda2a6461f0f06d2bd8","Although R includes numerous tools for creating color palettes to display continuous data, facilities for displaying categorical data primarily use the RColorBrewer package, which is, by default, limited to 12 colors. The colorspace package can produce more colors, but it is not immediately clear how to use it to produce colors that can be reliably distingushed in different kinds of plots. However, applications to genomics would be enhanced by the ability to display at least the 24 human chromosomes in distinct colors, as is common in technologies like spectral karyotyping. In this article, we describe the Polychrome package, which can be used to construct palettes with at least 24 colors that can be distinguished by most people with normal color vision. Polychrome includes a variety of visualization methods allowing users to evaluate the proposed palettes. In addition, we review the history of attempts to construct qualitative color palettes with many colors. © 2019, American Statistical Association. All rights reserved."
"FrailtyEM: An R package for estimating semiparametric shared frailty models",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071037084&doi=10.18637%2fjss.v090.i07&partnerID=40&md5=fc3db160bfb1f9ab2c4c6a3c0d2ff58d","When analyzing correlated time to event data, shared frailty (random effect) models are particularly attractive. However, the estimation of such models has proved challenging. In semiparametric models, this is further complicated by the presence of the nonparametric baseline hazard. Although recent years have seen an increased availability of software for fitting frailty models, most software packages focus either on a small number of distributions of the random effect, or support only on a few data scenarios. frailtyEM is an R package that provides maximum likelihood estimation of semiparametric shared frailty models using the expectation-maximization algorithm. The implementation is consistent across several scenarios, including possibly left truncated clustered failures and recurrent events in both calendar time and gap time formulation. A large number of frailty distributions belonging to the power variance function family are supported. Several methods facilitate access to predicted survival and cumulative hazard curves, both for an individual and on a population level. An extensive number of summary measures and statistical tests are also provided. © 2019, American Statistical Association. All rights reserved."
"Mean and variance modeling of under-dispersed and over-dispersed grouped binary data",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071034873&doi=10.18637%2fjss.v090.i08&partnerID=40&md5=50a2cee5e30be879c88e57dbc45c7e78","This article describes the R package BinaryEPPM and its use in determining maximum likelihood estimates of the parameters of extended Poisson process models for grouped binary data. These provide a Poisson process family of flexible models that can handle unlimited under-dispersion but limited over-dispersion in such data, with the binomial distribution being a special case. Within BinaryEPPM, models with the mean and variance related to covariates are constructed to match a generalized linear model formulation. Combining such under-dispersed models with standard over-dispersed models such as the beta binomial distribution provides a very general form of residual distribution for modeling grouped binary data. Use of the package is illustrated by application to several data-sets. © 2019, American Statistical Association. All rights reserved."
"Datamaid: Your assistant for documenting supervised data quality screening in R",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071018432&doi=10.18637%2fjss.v090.i06&partnerID=40&md5=6dff367ce104f246bc58bb62a2b809bb","Data cleaning and validation are important steps in any data analysis, as the validity of the conclusions from the analysis hinges on the quality of the input data. Mistakes in the data can arise for any number of reasons, including erroneous codings, malfunctioning measurement equipment, and inconsistent data generation manuals. Ideally, a human investigator should go through each variable in the dataset and look for potential errors – both in input values and codings – but that process can be very time-consuming, expensive and error-prone in itself. We describe an R package, dataMaid, which implements an extensive and customiz-able suite of quality assessment aids that can be applied to a dataset in order to identify potential problems in its variables. The results are presented in an auto-generated, nontechnical, stand-alone overview document intended to be perused by an investigator with an understanding of the variables in the data, but not necessarily knowledge of R. Thereby, dataMaid aids the dialogue between data analysts and field experts, while also providing easy documentation of reproducible data quality screening. Moreover, the dataMaid solution changes the data screening process from the usual ad hoc approach to a systematic, well-documented endeavor. dataMaid also provides a suite of more typical R tools for interactive data quality assessment and screening, where the data inspections are executed directly in the R console. © 2019, American Statistical Association. All rights reserved."
"Corr2d: Implementation of two-dimensional correlation analysis in R",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071014309&doi=10.18637%2fjss.v090.i03&partnerID=40&md5=0b11d9b342d88799261b8fbd8a9e3b1a","In the package corr2D two-dimensional correlation analysis is implemented in R. This paper describes how two-dimensional correlation analysis is done in the package and how the mathematical equations are translated into R code. The paper features a simple tutorial with executable code for beginners, insight into the calculations done before the correlation analysis, a detailed look at the parallelization of the fast Fourier transformation based correlation analysis and a speed test of the calculation. The package corr2D offers the possibility to preprocess, correlate and postprocess spectroscopic data using exclusively the R language. Thus, corr2D is a welcome addition to the toolbox of spectro-scopists and makes two-dimensional correlation analysis more accessible and transparent. © 2019, American Statistical Association. All rights reserved."
"Slfm: An R package to evaluate coherent patterns in microarray data via factor analysis",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071011370&doi=10.18637%2fjss.v090.i09&partnerID=40&md5=155aa9bd7cb4064c7de336bf89886625","The development of simulation-based methods, such as Markov chain Monte Carlo (MCMC), has contributed to an increased interest in the Bayesian framework as an alternative to deal with factor models. Many studies have used Bayesian factor analysis to explore gene expression data. We are particularly interested in the application of a sparse latent factor model (SLFM) based on sparsity priors (mixtures) to assess the significance of factors. The SLFM measures how strong the observed coherent expression pattern is in the data, which is an important source of information to evaluate gene activity. In the literature, this type of model has shown better results than other approaches intended for identification of patterns and metagene groups related to the underlying biology. However, a full Bayesian factor model relying on MCMC algorithms has an expensive computational cost, which makes it unattractive for general users. In this paper, we present the package slfm which uses C++ implementation via Rcpp to improve the computational performance of the SLFM within the widely used statistical tool R. We investigate real and simulated microarray data related to breast cancer. © 2019, American Statistical Association. All rights reserved."
"Multiplecar: A graphical user interface matlab toolbox to compute multiple correspondence analysis",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071010164&doi=10.18637%2fjss.v090.i04&partnerID=40&md5=27c3bbd6dc8d9470bb69027534bc6bb3","In this paper we present the toolbox MultipleCar, which is a general program for computing multiple correspondence analysis and which was designed using a graphical user interface. The procedures implemented in MultipleCar are the usual ones that are already available in other applications, plus some additional procedures. MultipleCar makes it possible to compute (1) joint correspondence analysis, and (2) orthogonal and oblique rotation of coordinates. Although MultipleCar was developed in MATLAB, we compiled it as a standalone application for Windows operative systems based on graphical user interfaces. The users can decide whether to use the advanced MATLAB version of MultipleCar, or the standalone version (which does not require any programming skills). © 2019, American Statistical Association. All rights reserved."
"BsamGP: An R package for bayesian spectral analysis models using Gaussian process priors",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070951986&doi=10.18637%2fjss.v090.i10&partnerID=40&md5=e1d211a8794786f3c854d3c0765999e6","The Bayesian spectral analysis model (BSAM) is a powerful tool to deal with semiparametric methods in regression and density estimation based on the spectral representation of Gaussian process priors. The bsamGP package for R provides a comprehensive set of programs for the implementation of fully Bayesian semiparametric methods based on BSAM. Currently, bsamGP includes semiparametric additive models for regression, generalized models and density estimation. In particular, bsamGP deals with constrained regression models with monotone, convex/concave, S-shaped and U-shaped functions by modeling derivatives of regression functions as squared Gaussian processes. bsamGP also contains Bayesian model selection procedures for testing the adequacy of a parametric model relative to a non-specific semiparametric alternative and the existence of the shape restriction. To maximize computational efficiency, we carry out posterior sampling algorithms of all models using compiled Fortran code. The package is illustrated through Bayesian semiparametric analyses of synthetic data and benchmark data. © 2019, American Statistical Association. All rights reserved."
"Simmer: Discrete-event simulation for R",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070951597&doi=10.18637%2fjss.v090.i02&partnerID=40&md5=11ed9808530cbea17b19a2dee9ad6df1","The simmer package brings discrete-event simulation to R. It is designed as a generic yet powerful process-oriented framework. The architecture encloses a robust and fast simulation core written in C++ with automatic monitoring capabilities. It provides a rich and flexible R API (application programming interface) that revolves around the concept of trajectory, a common path in the simulation model for entities of the same type. © 2019, American Statistical Association. All rights reserved."
"Weighted distance-based models for ranking data using the R package rankdist",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070519392&doi=10.18637%2fjss.v090.i05&partnerID=40&md5=1ef7faa8e9bc6ff361c41501ac33584d","Rankdist is a recently developed R package which implements various distance-based ranking models. These models capture the occurring probability of rankings based on the distances between them. The package provides a framework for fitting and evaluating finite mixture of distance-based models. This paper also presents a new probability model for ranking data based on a new notion of weighted Kendall distance. The new model is flexible and more interpretable than the existing models. We show that the new model has an analytic form of the probability mass function and the maximum likelihood estimates of the model parameters can be obtained efficiently even for ranking involving a large number of objects. © 2019, American Statistical Association. All rights reserved."
"Wavelet-based and fourier-based multivariate whittle estimation: Multiwave",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069744400&doi=10.18637%2fjss.v089.i06&partnerID=40&md5=ca70cf2076eb14138fe24ea1585f2b93","Multivariate time series with long-dependence are observed in many applications such as finance, geophysics or neuroscience. Many packages provide estimation tools for univariate settings but few are addressing the problem of long-dependence estimation for multivariate settings. The package multiwave is providing efficient estimation procedures for multivariate time series. Two semi-parametric estimation methods of the long-memory exponents and long-run covariance matrix of time series are implemented. The first one is the Fourier-based estimation proposed by Shimotsu (2007) and the second one is a wavelet-based estimation described in Achard and Gannaz (2016). The objective of this paper is to provide an overview of the R package multiwave with its practical application perspectives. © 2019, American Statistical Association. All rights reserved."
"Gpareto: An r package for gaussian-process-based multi-objective optimization and analysis",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069646634&doi=10.18637%2fjss.v089.i08&partnerID=40&md5=8b5badd70a53d30f67fee603ed2e2c13","The GPareto package for R provides multi-objective optimization algorithms for expensive black-box functions and an ensemble of dedicated uncertainty quantification methods. Popular methods such as efficient global optimization in the mono-objective case rely on Gaussian processes or kriging to build surrogate models. Driven by the prediction uncertainty given by these models, several infill criteria have also been proposed in a multi-objective setup to select new points sequentially and efficiently cope with severely limited evaluation budgets. They are implemented in the package, in addition with Pareto front estimation and uncertainty quantification visualization in the design and objective spaces. Finally, it attempts to fill the gap between expert use of the corresponding methods and user-friendliness, where many efforts have been put on providing graphical post-processing, standard tuning and interactivity. © 2019, American Statistical Association. All rights reserved."
"Ggenealogy: An R package for visualizing genealogical data",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069530728&doi=10.18637%2fjss.v089.i13&partnerID=40&md5=217b3fcf82f0c01bbed8377a56c79cda","This paper introduces ggenealogy (Rutter, Vanderplas, and Cook 2019), a developing R software package that provides tools for searching through genealogical data, generating basic statistics on their graphical structures using parent and child connections, parsing and performing calculations on branches of interest, and displaying the results. It is possible to draw the genealogy in relation to variables related to the nodes, and to determine and display the shortest path distances between the nodes. Production of pairwise distance matrices and genealogical diagrams constrained on generation are also available in the visualization toolkit. The tools are being tested on a dataset with milestone cultivars of soybean varieties (Hymowitz, Newell, and Carmer 1977) as well as on a web-based database of the academic genealogy of mathematicians (North Dakota State University and American Mathematical Society 2010). The latest stable package version is available in source and binary form on the Comprehensive R Archive Network (CRAN). © 2019, American Statistical Association. All rights reserved."
"Oasisr: An R package to bring some order to the world of segregation measurement",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069476455&doi=10.18637%2fjss.v089.i07&partnerID=40&md5=a1fdf45660877ee62e5aa615724bec99","Interest in social segregation measurement has increased strongly over the years and the number of segregation indices proposed in the literature have become more complex. However there are only a few software applications that can be employed to analyze social segregation, and these are usually available as a plug-in/package in geographic information system (GIS) software or as limited stand-alone application. Thus, the development of a package which exploits the power and versatility of the R environment for statistical computing and graphics would be desirable. Also, analysis of the segregation indices shows that there are ambiguities and errors in the literature, and consequently in the available software applications. This is an even more important reason why we need to develop a new tool to bring some order to the world of segregation measurement. This paper contributes also by proposing an automatic statistical testing methodology for these indices, using several resampling techniques: randomization tests, bootstrap and jackknife. © 2019, American Statistical Association. All rights reserved."
"The R package emdi for estimating and mapping regionally disaggregated indicators",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068735853&doi=10.18637%2fjss.v091.i07&partnerID=40&md5=cf5e4af82454dee6ecdec7d55fa960fb","The R package emdi enables the estimation of regionally disaggregated indicators using small area estimation methods and includes tools for processing, assessing, and presenting the results. The mean of the target variable, the quantiles of its distribution, the head-count ratio, the poverty gap, the Gini coefficient, the quintile share ratio, and customized indicators are estimated using direct and model-based estimation with the empirical best predictor (Molina and Rao 2010). The user is assisted by automatic estimation of data-driven transformation parameters. Parametric and semi-parametric, wild bootstrap for mean squared error estimation are implemented with the latter offering protection against possible misspecification of the error distribution. Tools for (a) customized parallel computing, (b) model diagnostic analyses, (c) creating high quality maps and (d) exporting the results to Excel and OpenDocument Spreadsheets are included. The functionality of the package is illustrated with example data sets for estimating the Gini coefficient and median income for districts in Austria. © 2019, American Statistical Association. All rights reserved."
"BDgraph: An R package for Bayesian structure learning in graphical models",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068378595&doi=10.18637%2fjss.v089.i03&partnerID=40&md5=ff58e0a8b9d40d34bc5b62dfb2719b03","Graphical models provide powerful tools to uncover complicated patterns in multivariate data and are commonly used in Bayesian statistics and machine learning. In this paper, we introduce the R package BDgraph which performs Bayesian structure learning for general undirected graphical models (decomposable and non-decomposable) with continuous, discrete, and mixed variables. The package efficiently implements recent improvements in the Bayesian literature, including that of Mohammadi and Wit (2015) and Dobra and Mohammadi (2018). To speed up computations, the computationally intensive tasks have been implemented in C++ and interfaced with R, and the package has parallel computing capabilities. In addition, the package contains several functions for simulation and visualization, as well as several multivariate datasets taken from the literature and used to describe the package capabilities. The paper includes a brief overview of the statistical methods which have been implemented in the package. The main part of the paper explains how to use the package. Furthermore, we illustrate the package’s functionality in both real and artificial examples. © 2019, American Statistical Association. All rights reserved."
"Directional statistics and filtering using libdirectional",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068375176&doi=10.18637%2fjss.v089.i04&partnerID=40&md5=5c20e3305fe5d2bc0b7960206e4d091d","In this paper, we present libDirectional, a MATLAB library for directional statistics and directional estimation. It supports a variety of commonly used distributions on the unit circle, such as the von Mises, wrapped normal, and wrapped Cauchy distributions. Furthermore, various distributions on higher-dimensional manifolds such as the unit hypersphere and the hypertorus are available. Based on these distributions, several recursive filtering algorithms in libDirectional allow estimation on these manifolds. The functionality is implemented in a clear, well-documented, and object-oriented structure that is both easy to use and easy to extend. © 2019, American Statistical Association. All rights reserved."
"BFDA: A MATLAB toolbox for bayesian functional data analysis",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068362229&doi=10.18637%2fjss.v089.i02&partnerID=40&md5=d6cebb61e2f1832aef23741886dc046b","We provide a MATLAB toolbox, BFDA, that implements a Bayesian hierarchical model to smooth multiple functional data samples with the assumptions of the same underlying Gaussian process distribution, a Gaussian process prior for the mean function, and an Inverse-Wishart process prior for the covariance function. This model-based approach can borrow strength from all functional data samples to increase the smoothing accuracy, as well as simultaneously estimate the mean-covariance functions. An option of approximating the Bayesian inference process using cubic B-spline basis functions is integrated in BFDA, which allows for efficiently dealing with high-dimensional functional data. Examples of using BFDA in various scenarios and conducting follow-up functional regression are provided. The advantages of BFDA include: (1) simultaneously smooths multiple functional data samples and estimates the mean-covariance functions in a nonparametric way; (2) flexibly deals with sparse and high-dimensional functional data with stationary and nonstationary covariance functions, and without the requirement of common observation grids; (3) provides accurately smoothed functional data for follow-up analysis. © 2019, American Statistical Association. All rights reserved."
"Distance sampling in R",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068356686&doi=10.18637%2fjss.v089.i01&partnerID=40&md5=646d9eaa8b59c9673781de932fb06d56","Estimating the abundance and spatial distribution of animal and plant populations is essential for conservation and management. We introduce the R package Distance that implements distance sampling methods to estimate abundance. We describe how users can obtain estimates of abundance (and density) using the package as well as documenting the links it provides with other more specialized R packages. We also demonstrate how Distance provides a migration pathway from previous software, thereby allowing us to deliver cutting-edge methods to the users more quickly. © 2019, American Statistical Association. All rights reserved."
"BTLLasso: A common framework and software package for the inclusion and selection of covariates in Bradley-Terry models",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068016351&doi=10.18637%2fjss.v088.i09&partnerID=40&md5=5de33031b97966dcfd34b32d064bf62e","In paired comparison models, the inclusion of covariates is a tool to account for the heterogeneity of preferences and to investigate which characteristics determine the preferences. Although methods for the selection of variables have been proposed no coherent framework that combines all possible types of covariates is available. There are three different types of covariates that can occur in paired comparisons, the covariates can either vary over the subjects, the objects or both the subjects and the objects of the paired comparisons. This paper gives an overview over all possible types of covariates in paired comparisons and introduces a general framework to include covariate effects into Bradley-Terry models. For each type of covariate, appropriate penalty terms that allow for sparser models and, therefore, easier interpretation are proposed. The whole framework is implemented in the R package BTLLasso. The main functionality and the visualization tools of the package are introduced and illustrated by real data sets. © 2019, American Statistical Association. All rights reserved."
"Plssem: A stata package for structural equation modeling with partial least squares",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068011682&doi=10.18637%2fjss.v088.i08&partnerID=40&md5=4855ee2e21774ea54650e52a66bf5853","We provide a package called plssem that fits partial least squares structural equation models, which is often considered an alternative to the commonly known covariance-based structural equation modeling. plssem is developed in line with the algorithm provided by Wold (1975) and Lohmöller (1989). To demonstrate its features, we present an empirical application on the relationship between perception of self-attractiveness and two specific types of motivations for working out using a real-life data set. In the paper we also show that, in line with other software performing structural equation modeling, plssem can be used for putting in relation single-item observed variables too and not only for latent variable modeling. © 2019, American Statistical Association. All rights reserved."
"Getting started with particle metropolis-hastings for inference in nonlinear dynamical models",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067969565&doi=10.18637%2fjss.v088.c02&partnerID=40&md5=3851ee3f34259d8946ca44b1ae6c7829","This tutorial provides a gentle introduction to the particle Metropolis-Hastings (PMH) algorithm for parameter inference in nonlinear state-space models together with a software implementation in the statistical programming language R. We employ a step-by-step approach to develop an implementation of the PMH algorithm (and the particle filter within) together with the reader. This final implementation is also available as the package pmhtutorial from the Comprehensive R Archive Network (CRAN) repository. Throughout the tutorial, we provide some intuition as to how the algorithm operates and discuss some solutions to problems that might occur in practice. To illustrate the use of PMH, we consider parameter inference in a linear Gaussian state-space model with synthetic data and a nonlinear stochastic volatility model with real-world data. © 2019, American Statistical Association. All right reserved."
"Coclust: A python package for co-clustering",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067969031&doi=10.18637%2fjss.v088.i07&partnerID=40&md5=57c0c65b0eff84edb2041140f4114e67","Co-clustering (also known as biclustering), is an important extension of cluster analysis since it allows to simultaneously group objects and features in a matrix, resulting in row and column clusters that are both more accurate and easier to interpret. This paper presents the theory underlying several effective diagonal and non-diagonal co-clustering algorithms, and describes CoClust, a package which provides implementations for these algorithms. The quality of the results produced by the implemented algorithms is demonstrated through extensive tests performed on datasets of various size and balance. CoClust has been designed to complete and easily interface with popular Python machine learning libraries such as scikit-learn. © 2019, American Statistical Association. All rights reserved."
"Hyperspectral data analysis in R: The hsdar package",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067402066&doi=10.18637%2fjss.v089.i12&partnerID=40&md5=b91286ec3d3bffcfb6acc98809ab7b77","Hyperspectral remote sensing is a promising tool for a variety of applications including ecology, geology, analytical chemistry and medical research. This article presents the new hsdar package for R statistical software, which performs a variety of analysis steps taken during a typical hyperspectral remote sensing approach. The package introduces a new class for efficiently storing large hyperspectral data sets such as hyperspectral cubes within R. The package includes several important hyperspectral analysis tools such as continuum removal, normalized ratio indices and integrates two widely used radiation transfer models. In addition, the package provides methods to directly use the functionality of the caret package for machine learning tasks. Two case studies demonstrate the package’s range of functionality: First, plant leaf chlorophyll content is estimated and second, cancer in the human larynx is detected from hyperspectral data. © 2019, American Statistical Association. All rights reserved."
"Dynamic modeling, parameter estimation, and uncertainty analysis in R",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066873978&doi=10.18637%2fjss.v088.i10&partnerID=40&md5=3a9bb7d1e52479bf9fe4acdbdb3b8412","In a wide variety of research fields, dynamic modeling is employed as an instrument to learn and understand complex systems. The differential equations involved in this process are usually non-linear and depend on many parameters whose values determine the characteristics of the emergent system. The inverse problem, i.e., the inference or estimation of parameter values from observed data, is of interest from two points of view. First, the existence point of view, dealing with the question whether the system is able to reproduce the observed dynamics for any parameter values. Second, the identifiability point of view, investigating invariance of the prediction under change of parameter values, as well as the quantification of parameter uncertainty. In this paper, we present the R package dMod providing a framework for dealing with the inverse problem in dynamic systems modeled by ordinary differential equations. The uniqueness of the approach taken by dMod is to provide and propagate accurate derivatives computed from symbolic expressions wherever possible. This derivative information highly supports the convergence of optimization routines and enhances their numerical stability, a requirement for the applicability of sophisticated uncertainty analysis methods. Computational efficiency is achieved by automatic generation and execution of C code. The framework is object-oriented (S3) and provides a variety of functions to set up ordinary differential equation models, observation functions and parameter transformations for multi-conditional parameter estimation. The key elements of the framework and the methodology implemented in dMod are highlighted by an application on a three-compartment transporter model. © 2019, American Statistical Association. All rights reserved."
"dtwSat: Time-weighted dynamic time warping for satellite image time series analysis in R",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061403230&doi=10.18637%2fjss.v088.i05&partnerID=40&md5=f89426b24cf2706b986da3ece471dcde","The opening of large archives of satellite data such as LANDSAT, MODIS and the SENTINELs has given researchers unprecedented access to data, allowing them to better quantify and understand local and global land change. The need to analyze such large data sets has led to the development of automated and semi-automated methods for satellite image time series analysis. However, few of the proposed methods for remote sensing time series analysis are available as open source software. In this paper we present the R package dtwSat. This package provides an implementation of the time-weighted dynamic time warping method for land cover mapping using sequence of multi-band satellite images. Methods based on dynamic time warping are flexible to handle irregular sampling and out-of-phase time series, and they have achieved significant results in time series analysis. Package dtwSat is available from the Comprehensive R Archive Network (CRAN) and contributes to making methods for satellite time series analysis available to a larger audience. The package supports the full cycle of land cover classification using image time series, ranging from selecting temporal patterns to visualizing and assessing the results. © 2019, American Statistical Association. All rights reserved."
"ETAS: An R package for fitting the space-time ETAS model to earthquake data",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061400712&doi=10.18637%2fjss.v088.c01&partnerID=40&md5=4c636468a6906820d61284401f0fffc3","The epidemic-type aftershock sequence (ETAS) model is the most widely used statistical model to describe earthquake catalogs. ETAS is an R package for fitting the space-time ETAS model to an earthquake catalog using the stochastic declustering approach introduced by Zhuang, Ogata, and Vere-Jones (2002). The package provides two classes and several functions to facilitate data preparation, model fitting and some simple diagnostic checks. The present paper is a description of the package and illustrates modeling earthquake data using the space-time ETAS model. © 2019, American Statistical Association. All rights reserved."
"Cdfquantreg: An R package for CDF-quantile regression",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061397766&doi=10.18637%2fjss.v088.i01&partnerID=40&md5=f869c35f3d27b4a8f8c5775aff17a525","The CDF-quantile family of two-parameter distributions with support (0, 1) described in Smithson and Merkle (2014) and recently elaborated by Smithson and Shou (2017), considerably expands the variety of distributions available for modeling random variables on the unit interval. This family is especially useful for modeling quantiles, and also sometimes out-performs the other distributions. The distributions are very tractable, with a location and dispersion parameter, explicit probability distribution functions, cumulative distribution functions, and quantiles. They enable a wide variety of quantile regression models with predictors for the location and dispersion parameters, and simple interpretations of those parameters. The R package cdfquantreg (Shou and Smithson 2019) (at least R 3.2.0) presented in this paper includes 36 distributions from the CDF-quantile family. Separate submodels may be specified for the location and for the dispersion parameters, with different or overlapping sets of predictors in each. The package offers maximum likelihood, Bayesian MCMC, and bootstrap estimation methods. Model diagnostics, including the gradient, three types of residuals, and the dfbeta influence measures, are available for evaluating models. The package also provides pseudo-random generators for all of its distributions. Many of its functions and their usage have forms familiar to R users, and the documentation is extensive. We also present a SAS macro for general linear models using the CDF-quantile family that includes many of the same capabilities as the cdfquantreg package. The paper provides examples of applications to real data-sets. © 2019, American Statistical Association. All rights reserved."
"Generalized autoregressive score models in R: The GAS package",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061395598&doi=10.18637%2fjss.v088.i06&partnerID=40&md5=2c0b07c80c949042b39b94f61b2444d7","This paper presents the R package GAS for the analysis of time series under the generalized autoregressive score (GAS) framework of Creal, Koopman, and Lucas (2013) and Harvey (2013). The distinctive feature of the GAS approach is the use of the score function as the driver of time-variation in the parameters of non-linear models. The GAS package provides functions to simulate univariate and multivariate GAS processes, to estimate the GAS parameters and to make time series forecasts. We illustrate the use of the GAS package with a detailed case study on estimating the time-varying conditional densities of financial asset returns. © 2019, American Statistical Association. All rights reserved."
"Mixture hidden Markov models for sequence data: The seqhmm package in R",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061377276&doi=10.18637%2fjss.v088.i03&partnerID=40&md5=34cbd43c3a8d7e8752fbed2de2b01fd8","Sequence analysis is being more and more widely used for the analysis of social sequences and other multivariate categorical time series data. However, it is often complex to describe, visualize, and compare large sequence data, especially when there are multiple parallel sequences per subject. Hidden (latent) Markov models (HMMs) are able to detect underlying latent structures and they can be used in various longitudinal settings: to account for measurement error, to detect unobservable states, or to compress information across several types of observations. Extending to mixture hidden Markov models (MHMMs) allows clustering data into homogeneous subsets, with or without external covariates. The seqHMM package in R is designed for the efficient modeling of sequences and other categorical time series data containing one or multiple subjects with one or multiple interdependent sequences using HMMs and MHMMs. Also other restricted variants of the MHMM can be fitted, e.g., latent class models, Markov models, mixture Markov models, or even ordinary multinomial regression models with suitable parameterization of the HMM. Good graphical presentations of data and models are useful during the whole analysis process from the first glimpse at the data to model fitting and presentation of results. The package provides easy options for plotting parallel sequence data, and proposes visualizing HMMs as directed graphs. © 2019, American Statistical Association. All rights reserved."
"JASP: Graphical statistical software for common statistical designs",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061360981&doi=10.18637%2fjss.v088.i02&partnerID=40&md5=ad4aeee802b60e999c1d5929d3033853","This paper introduces JASP, a free graphical software package for basic statistical procedures such as t tests, ANOVAs, linear regression models, and analyses of contingency tables. JASP is open-source and differentiates itself from existing open-source solutions in two ways. First, JASP provides several innovations in user interface design; specifically, results are provided immediately as the user makes changes to options, output is attractive, minimalist, and designed around the principle of progressive disclosure, and analyses can be peer reviewed without requiring a “syntax”. Second, JASP provides some of the recent developments in Bayesian hypothesis testing and Bayesian parameter estimation. The ease with which these relatively complex Bayesian techniques are available in JASP encourages their broader adoption and furthers a more inclusive statistical reporting practice. The JASP analyses are implemented in R and a series of R packages. © 2019, American Statistical Association. All rights reserved."
"The R package MAMS for designing multi-arm multi-stage clinical trials",2019,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061360133&doi=10.18637%2fjss.v088.i04&partnerID=40&md5=2c31eb55cdbc817201da010ba07299ad","In the early stages of drug development there is often uncertainty about the most promising among a set of different treatments, different doses of the same treatment, or combinations of treatments. Multi-arm multi-stage (MAMS) clinical studies provide an efficient solution to determine which intervention is most promising. In this paper we discuss the R package MAMS that allows designing such studies within the group-sequential framework. The package implements MAMS studies with normal, binary, ordinal, or time-to-event endpoints in which either the single best treatment or all promising treatments are continued at the interim analyses. Additionally unexpected design modifications can be accounted for via the use of the conditional error approach. We provide illustrative examples of the use of the package based on real trial designs. © 2019, American Statistical Association. All rights reserved."
"Ggtern: Ternary diagrams using ggplot2",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060255878&doi=10.18637%2fjss.v087.c03&partnerID=40&md5=46accc5fec085f9161010fd0f16e0b40","This paper presents the ggtern package for R, which has been developed for the rendering of ternary diagrams. Based on the well-established ggplot2 package (Wickham 2009), the present package adopts the familiar and convenient programming syntax of its parent. We demonstrate that ggplot2 can be used as the basis for producing specialized plotting packages and, in the present case, a package has been developed specifically for the production of high quality ternary diagrams. In order to produce ggtern, it was necessary to overcome a number of design issues, such as finding a means to modify existing geometries designed for a 2D Cartesian coordinate system and permitting them to function in an environment that requires an additional spatial aesthetic mapping. In the present paper, we provide examples of this package in its most basic form followed by a demonstration of its ease of use, particularly if one is familiar with, and has a predilection towards using ggplot2 on a regular basis. © 2018, American Statistical Association. All rights reserved."
"Seasonal adjustment by X-13ARIMA-SEATS in R",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060248940&doi=10.18637%2fjss.v087.i11&partnerID=40&md5=803fa8863c7656a1675880ddce9c7924","Seasonal is a powerful interface between R and X-13ARIMA-SEATS, the seasonal adjustment software developed by the United States Census Bureau. It offers access to almost all features of X-13, including seasonal adjustment via the X-11 and SEATS approaches, automatic ARIMA model search, outlier detection, and support for user-defined holiday variables such as the Chinese New Year or Indian Diwali. The required X-13 binaries are provided by the x13binary package, which facilitates a fully-automatic installation on most platforms. A demo website (at http://www.seasonal.website/) supports interactive modeling of custom data. A graphical user interface in the seasonalview package offers the same functionality locally. X-13 can handle monthly, quarterly or bi-annual time series. © 2018, American Statistical Association. All rights reserved."
"Extremefit: A package for extreme quantiles",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060231493&doi=10.18637%2fjss.v087.i12&partnerID=40&md5=6903fd5d1983788f8c70786b259f35a2","Extremefit is a package to estimate the extreme quantiles and probabilities of rare events. The idea of our approach is to adjust the tail of the distribution function over a threshold with a Pareto distribution. We propose a pointwise data driven procedure to choose the threshold. To illustrate the method, we use simulated data sets and three real-world data sets included in the package. © 2018, American Statistical Association. All rights reserved."
"GcKrig: An R package for the analysis of geostatistical count data using gaussian copulas",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060021714&doi=10.18637%2fjss.v087.i13&partnerID=40&md5=f7817944fb0b0ab0133362bc8e0e8a72","This work describes the R package gcKrig for the analysis of geostatistical count data using Gaussian copulas. The package performs likelihood-based inference and spatial prediction using Gaussian copula models with discrete marginals. Two different classes of methods are implemented to evaluate/approximate the likelihood and the predictive distribution. The package implements the computationally intensive tasks in C++ using an R/C++ interface, and has parallel computing capabilities to predict the response at multiple locations simultaneously. In addition, gcKrig also provides functions to simulate and visualize geostatistical count data, and to compute the correlation function of the counts. It is designed to allow a flexible specification of both the marginals and the spatial correlation function. The principal features of the package are illustrated by three data examples from ecology, agronomy and petrology, and a comparison between gcKrig and two other R packages. © 2018, American Statistical Association. All rights reserved."
"Visualizing fit and lack of fit in complex regression models with predictor effect plots and partial residuals",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060214296&doi=10.18637%2fjss.v087.i09&partnerID=40&md5=f025578802c1b45ae3a2d79186510df5","Predictor effect displays, introduced in this article, visualize the response surface of complex regression models by averaging and conditioning, producing a sequence of 2D line graphs, one graph or set of graphs for each predictor in the regression problem. Partial residual plots visualize lack of fit, traditionally in relatively simple additive regression models. We combine partial residuals with effect displays to visualize both fit and lack of fit simultaneously in complex regression models, plotting residuals from a model around 2D slices of the fitted response surface. Employing fundamental results on partial residual plots along with examples for both real and contrived data, we discuss and illustrate both the strengths and limitations of the resulting graphs. The methods described in this paper are implemented in the effects package for R. © 2018, American Statistical Association. All rights reserved."
"rTensor: An R package for multidimensional array (tensor) unfolding, multiplication, and decomposition",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060198620&doi=10.18637%2fjss.v087.i10&partnerID=40&md5=823ad039a635308096c17ccbbc4164ef","rTensor is an R package designed to provide a common set of operations and decompositions for multidimensional arrays (tensors). We provide an S4 class that wraps around the base ‘array’ class and overloads familiar operations to users of ‘array’, and we provide additional functionality for tensor operations that are becoming more relevant in recent literature. We also provide a general unfolding operation, for which the k-mode unfolding and the matrix vectorization are special cases of. Finally, package rTensor implements common tensor decompositions such as canonical polyadic decomposition, Tucker decomposition, multilinear principal component analysis, t-singular value decomposition, as well as related matrix-based algorithms such as generalized low rank approximation of matrices and popular value decomposition. © 2018, American Statistical Association. All rights reserved."
"Rqc: A bioconductor package for quality control of high-throughput sequencing data",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060180931&doi=10.18637%2fjss.v087.c02&partnerID=40&md5=63f1a1b8ed36cfdc855594c3cd78e7ce","As sequencing costs drop with the constant improvements in the field, next-generation sequencing becomes one of the most used technologies in biological research. Sequencing technology allows the detailed characterization of events at the molecular level, including gene expression, genomic sequence and structural variants. Such experiments result in billions of sequenced nucleotides and each one of them is associated to a quality score. Several software tools allow the quality assessment of whole experiments. However, users need to switch between software environments to perform all steps of data analysis, adding an extra layer of complexity to the data analysis workflow. We developed Rqc, a Bioconductor package designed to assist the analyst during assessment of high-throughput sequencing data quality. The package uses parallel computing strategies to optimize large data sets processing, regardless of the sequencing platform. We created new data quality visualization strategies by using established analytical procedures. That improves the ability of identifying patterns that may affect downstream procedures, including undesired sources technical variability. The software provides a framework for writing customized reports that integrates seamlessly to the R/Bioconductor environment, including publication-ready images. The package also offers an interactive tool to generate quality reports dynamically. Rqc is implemented in R and it is freely available through the Bioconductor project (https://bioconductor.org/packages/Rqc/) for Windows, Linux and Mac OS X operating systems. © 2018, American Statistical Association. All rights reserved."
"MerDeriv: Derivative computations for linear mixed effects models with application to robust standard errors",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060146748&doi=10.18637%2fjss.v087.c01&partnerID=40&md5=8d0d224de0a69bb3808c5454b3580160","While likelihood-based derivatives and related facilities are available in R for many types of statistical models, the facilities are notably lacking for models estimated via lme4. This is because the necessary statistical output, including the Hessian, Fisher information and casewise contributions to the model gradient, is not immediately available from lme4 and is not trivial to obtain. In this article, we describe merDeriv, an R package which supplies new functions to obtain analytic output from Gaussian mixed models. We discuss the theoretical results implemented in the code, focusing on calculation of robust standard errors via package sandwich. We also use the sleepstudy data to illustrate the package and to compare it to a benchmark from package lavaan. © 2018, American Statistical Association. All rights reserved."
"Latent class probabilistic latent feature analysis of three-way three-mode binary data",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060254150&doi=10.18637%2fjss.v087.i01&partnerID=40&md5=63aab74c0cd8c1365b0f3e0d84e55365","The analysis of binary three-way data (i.e., persons who indicate which attributes apply to each of a set of objects) may be of interest in several substantive domains as sensory profiling, marketing research or personality assessment. Latent class probabilistic latent feature models (LCPLFMs) may be used to explain binary object-attribute associations on the basis of a small number of binary latent variables (called latent features). As LCPLFMs aim to model object-attribute associations using a small number of latent features they may be more suited to analyze data with many objects/attributes than standard multilevel latent class models which do not include such a dimension reduction. In this paper we describe new functions of the plfm package for analyzing binary three-way data with LCPLFMs. The new functions provide a flexible modeling approach as they allow to (1) specify different assumptions for modeling statistical dependencies between object-attribute pairs, (2) use different assumptions for modeling parameter heterogeneity across persons, (3) conduct a confirmatory analysis by constraining specific parameters to pre-specified values, (4) inspect results with print, summary and plot methods. As an illustration, the models are applied to analyze data on the perception of midsize cars, and to study the situational determinants of anger-related behavior. © 2018, American Statistical Association. All rights reserved."
"Semiparametric regression analysis via Infer.NET",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060232328&doi=10.18637%2fjss.v087.i02&partnerID=40&md5=baccda141b3821234cf76e08529cff45","We provide several examples of Bayesian semiparametric regression analysis via the Infer.NET package for approximate deterministic inference in Bayesian models. The examples are chosen to encompass a wide range of semiparametric regression situations. Infer.NET is shown to produce accurate inference in comparison with Markov chain Monte Carlo via the BUGS package, but to be considerably faster. Potentially, this contribution represents the start of a new era for semiparametric regression, where large and complex analyses are performed via fast Bayesian inference methodology and software, mainly being developed within Machine Learning. © 2018, American Statistical Association. All rights reserved."
"Flexible self-organizing maps in kohonen 3.0",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060222319&doi=10.18637%2fjss.v087.i07&partnerID=40&md5=2fea944d541f3d1da79a83437d714295","Self-organizing maps (SOMs) are popular tools for grouping and visualizing data in many areas of science. This paper describes recent changes in package kohonen, implementing several different forms of SOMs. These changes are primarily focused on making the package more useable for large data sets. Memory consumption has decreased dramatically, amongst others, by replacing the old interface to the underlying compiled code by a new one relying on Rcpp. The batch SOM algorithm for training has been added in both sequential and parallel forms. A final important extension of the package’s repertoire is the possibility to define and use data-dependent distance functions, extremely useful in cases where standard distances like the Euclidean distance are not appropriate. Several examples of possible applications are presented. © 2018, American Statistical Association. All rights reserved."
"Nonparametric relative survival analysis with the R package relsurv",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060205686&doi=10.18637%2fjss.v087.i08&partnerID=40&md5=212862d1f666927aea6c36dd286a5d05","Relative survival methods are crucial with data in which the cause of death information is either not given or inaccurate, but cause-specific information is nevertheless required. This methodology is standard in cancer registry data analysis and can also be found in other areas. The idea of relative survival is to join the observed data with the general mortality population data and thus extract the information on the disease-specific hazard. While this idea is clear and easy to understand, the practical implementation of the estimators is rather complex since the population hazard for each individual depends on demographic variables and changes in time. A considerable advance in the methodology of this field has been observed in the past decade and while some methods represent only a modification of existing estimators, others require newly programmed functions. The package relsurv covers all the steps of the analysis, from importing the general population tables to estimating and plotting the results. The syntax mimics closely that of the classical survival packages like survival and cmprsk, thus enabling the users to directly use its functions without any further familiarization. In this paper we focus on the nonparametric relative survival analysis, and in particular, on the two key estimators for net survival and crude probability of death. Both estimators were first presented in our package and are still missing in many other software packages, a fact which greatly hampers their frequency of use. The paper offers guidelines for the actual use of the software by means of a detailed nonparametric analysis of the data describing the survival of patients with colon cancer. The data have been provided by the Cancer Registry of Slovenia. © 2018, American Statistical Association. All rights reserved."
"Database-inspired optimizations for statistical analysis",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060187240&doi=10.18637%2fjss.v087.i04&partnerID=40&md5=977055e291f620ea819230aef9bb46d2","Computing complex statistics on large amounts of data is no longer a corner case, but a daily challenge. However, current tools such as GNU R were not built to efficiently handle large data sets. We propose to vastly improve the execution of R scripts by interpreting them as a declaration of intent rather than an imperative order set in stone. This allows us to apply optimization techniques from the columnar data management research field. We have implemented several of these optimizers in Renjin, an open-source execution environment for R scripts targeted at the Java virtual machine. The demonstration of our approach using a series of micro-benchmarks and experiments on complex survey analysis show orders-of-magnitude improvements in analysis cost. © 2018, American Statistical Association. All rights reserved."
"Simulated data for linear regression with structured and sparse penalties: Introducing pylearn-simulate",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060177715&doi=10.18637%2fjss.v087.i03&partnerID=40&md5=d02da83d0c6f3b0dda864fcc28baff72","A currently very active field of research is how to incorporate structure and prior knowledge in machine learning methods. It has lead to numerous developments in the field of non-smooth convex minimization. With recently developed methods it is possible to perform an analysis in which the computed model can be linked to a given structure of the data and simultaneously do variable selection to find a few important features in the data. However, there is still no way to unambiguously simulate data to test proposed algorithms, since the exact solutions to such problems are unknown. The main aim of this paper is to present a theoretical framework for generating simulated data. These simulated data are appropriate when comparing optimization algorithms in the context of linear regression problems with sparse and structured penalties. Additionally, this approach allows the user to control the signal-to-noise ratio, the correlation structure of the data and the optimization problem to which they are the solution. The traditional approach is to simulate random data without taking into account the actual model that will be fit to the data. But when using such an approach it is not possible to know the exact solution of the underlying optimization problem. With our contribution, it is possible to know the exact theoretical solution of a penalized linear regression problem, and it is thus possible to compare algorithms without the need to use, e.g., cross-validation. We also present our implementation, the Python package pylearn-simulate, available at https://github.com/neurospin/pylearn-simulate and released under the BSD 3-clause license. We describe the package and give examples at the end of the paper. © 2018, American Statistical Association. All rights reserved."
"MTPmle: A SAS macro and Stata programs for marginalized inference in semi-continuous data",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060139572&doi=10.18637%2fjss.v087.i06&partnerID=40&md5=bec6c09d6101ffcf065213fc1e9156ee","We develop a SAS macro and equivalent Stata programs that provide marginalized inference for semi-continuous data using a maximum likelihood approach. These software extensions are based on recently developed methods for marginalized two-part (MTP) models. Both the SAS and Stata extensions can fit simple MTP models for cross-sectional semi-continuous data. In addition, the SAS macro can fit random intercept models for longitudinal or clustered data, whereas the Stata programs can fit MTP models that account for subject level heteroscedasticity and for a complex survey design. Differences and similarities between the two software extensions are highlighted to provide a comparative picture of the available options for estimation, inclusion of random effects, convergence diagnosis, and graphical display. We provide detailed programming syntax, simulated and real data examples to facilitate the implementation of the MTP models for both SAS and Stata software users. © 2018, American Statistical Association. All rights reserved."
"SSpace: A toolbox for state space modeling",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051662136&doi=10.18637%2fjss.v087.i05&partnerID=40&md5=b69bcbf67dddceddd97e082aa9938051","SSpace is a MATLAB toolbox for state space modeling. State space modeling is in itself a powerful and flexible framework for dynamic system modeling, and SSpace is conceived in a way that tries to maximize this flexibility. One of the most salient features is that users implement their models by coding a MATLAB function. In this way, users have complete flexibility when specifying the systems, have absolute control on parame-terizations, constraints among parameters, etc. Besides, the toolbox allows for some ways to implement either non-standard models or standard models with non-standard extensions, like heteroskedasticity, time-varying parameters, arbitrary nonlinear relations with inputs, transfer functions without the need of using explicitly the state space form, etc. The toolbox may be used on the basis of scratch state space systems, but is supplied with a number of templates for standard widespread models. A full help system and documentation are provided. The way the toolbox is built allows for extensions in many ways. In order to fuel such extensions and discussions an online forum has been launched. © 2018, American Statistical Association. All rights reserved."
"DNest4: Diffusive nested sampling in C++ and python",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053285199&doi=10.18637%2fjss.v086.i07&partnerID=40&md5=de3e5c5bcdaea6952d4f31aeb5c81e2a","In probabilistic (Bayesian) inferences, we typically want to compute properties of the posterior distribution, describing knowledge of unknown quantities in the context of a particular dataset and the assumed prior information. The marginal likelihood, also known as the “evidence”, is a key quantity in Bayesian model selection. The diffusive nested sampling algorithm, a variant of nested sampling, is a powerful tool for generating posterior samples and estimating marginal likelihoods. It is effective at solving complex problems including many where the posterior distribution is multimodal or has strong dependencies between variables. DNest4 is an open source (MIT licensed), multi-threaded implementation of this algorithm in C++11, along with associated utilities including: (i) ‘RJObject’, a class template for finite mixture models; and (ii) a Python package allowing basic use without C++ coding. In this paper we demonstrate DNest4 usage through examples including simple Bayesian data analysis, finite mixture models, and approximate Bayesian computation. © 2018, American Statistical Association. All rights reserved."
"General semiparametric shared frailty model: Estimation and simulation with frailtysurv",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053273994&doi=10.18637%2fjss.v086.i04&partnerID=40&md5=8fa0d722e60be6ec83bd5dea0a80e70a","The R package frailtySurv for simulating and fitting semi-parametric shared frailty models is introduced. Package frailtySurv implements semi-parametric consistent estimators for a variety of frailty distributions, including gamma, log-normal, inverse Gaussian and power variance function, and provides consistent estimators of the standard errors of the parameters’ estimators. The parameters’ estimators are asymptotically normally distributed, and therefore statistical inference based on the results of this package, such as hypothesis testing and confidence intervals, can be performed using the normal distribution. Extensive simulations demonstrate the flexibility and correct implementation of the estimator. Two case studies performed with publicly available datasets demonstrate applicability of the package. In the Diabetic Retinopathy Study, the onset of blindness is clustered by patient, and in a large hard drive failure dataset, failure times are thought to be clustered by the hard drive manufacturer and model. © 2018, American Statistical Association. All rights reserved."
"Rectangular statistical cartograms in R: The recmap package",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053267987&doi=10.18637%2fjss.v086.c01&partnerID=40&md5=f0cdc166fe52e999ce626e6798556b66","Cartogram drawing is a technique for showing geography-related statistical information, such as demographic and epidemiological data. The idea is to distort a map by resizing its regions according to a statistical parameter by keeping the map recognizable. This article describes an R package implementing an algorithm called RecMap which approximates every map region by a rectangle where the area corresponds to the given statistical value (maintain zero cartographic error). The package implements the computationally intensive tasks in C++. This paper’s contribution is that it demonstrates on real and synthetic maps how package recmap can be used, how it is implemented and how it is used with other statistical packages. © 2018, American Statistical Association. All rights reserved."
"Near-far matching in R: The nearfar package",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053263357&doi=10.18637%2fjss.v086.c05&partnerID=40&md5=694d29b14c58bd8c947a5e331d3e76b9","Estimating the causal treatment effect of an intervention using observational data is difficult due to unmeasured confounders. Many analysts use instrumental variables (IVs) to introduce a randomizing element to observational data analysis, potentially reducing bias created by unobserved confounders. Several persistent problems in the field have served as limitations to IV analyses, particularly the prevalence of “weak” IVs, or instrumental variables that do not effectively randomize individuals to the intervention or control group (leading to biased and unstable treatment effect estimates), as well as IV-based estimates being highly model dependent, requiring parametric adjustment for measured confounders, and often having high mean squared errors in the estimated causal effects. To overcome these problems, the study design method of “near-far matching” has been devised, which “filters” data from a cohort by simultaneously matching individuals within the cohort to be “near” (similar) on measured confounders and “far” (different) on levels of an IV. To facilitate the application of near-far matching to analytical problems, we introduce the R package nearfar and illustrate its application to both a classical example and a simulated dataset. We illustrate how the package can be used to “strengthen” a weak IV by adjusting the “near-ness” and “far-ness” of a match, reduce model dependency, enable nonparametric adjustment for measured confounders, and lower mean squared error in estimated causal effects. We additionally illustrate how to utilize the nearfar package when analyzing either continuous or binary treatments, how to prioritize variables in the match, and how to calculate F statistics of IV strength with or without adjustment for measured confounders. © 2018, American Statistical Association. All rights reserved."
"Supervised multiblock analysis in R with the ade4 package",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053255627&doi=10.18637%2fjss.v086.i01&partnerID=40&md5=4761cb2121e748d4d2073fd901548119","This paper presents two novel statistical analyses of multiblock data using the R language. It is designed for data organized in (K +1) blocks (i.e., tables) consisting of a block of response variables to be explained by a large number of explanatory variables which are divided into K meaningful blocks. All the variables – explanatory and dependent – are measured on the same individuals. Two multiblock methods both useful in practice are included, namely multiblock partial least squares regression and multiblock principal component analysis with instrumental variables. The proposed new methods are included within the ade4 package widely used thanks to its great variety of multivariate methods. These methods are available on the one hand for statisticians and on the other hand for users from various fields in the sense that all the values derived from the multiblock processing are available. Some relevant interpretation tools are also developed. Finally the main results are summarized using overall graphical displays. This paper is organized following the different steps of a standard multiblock process, each corresponding to specific R functions. All these steps are illustrated by the analysis of real epidemiological datasets. © 2018, American Statistical Association. All rights reserved."
"The idm package: Incremental decomposition methods in R",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053250655&doi=10.18637%2fjss.v086.c04&partnerID=40&md5=49e0677cdb16e3eccdedf0c944a57164","In modern applications large amounts of data are produced at a high rate and are characterized by relationship structures changing over time. Principal component analysis (PCA) and multiple correspondence analysis (MCA) are well established dimension reduction methods to explore relationships within a set of variables. A critical step of the PCA and MCA algorithms is a singular value decomposition (SVD) or an eigenvalue decomposition (EVD) of a suitably transformed matrix. The high computational and memory requirements of ordinary SVD and EVD make their application impractical on massive or sequential data sets. A series of incremental SVD/EVD approaches are available to address these issues. The idm R package is introduced that implements two efficient incremental SVD approaches. The procedures in question share desirable properties that ease their embedding in PCA and MCA. The package also provides functions for producing animated visualizations of the obtained solutions. A comparison of online MCA implementations in terms of accuracy is also included. © 2018, American Statistical Association. All rights reserved."
"mbonsai: Application package for sequence classification by tree methodology",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053244178&doi=10.18637%2fjss.v086.i06&partnerID=40&md5=f691964851cc5543c121365677bc04bf","In many applications such as transaction data analysis, the classification of long chains of sequences is required. For example, brand purchase history in customer transaction data is in a form like AABCABAA, where A, B, and C are brands of a consumer product. The decision tree-based package mbonsai is designed to handle sequence data of varying lengths using one or multiple variables of interest as predictor variables. This software package uses tree growing and pruning strategies adopted from C4.5 and CART algorithms, and includes new features for handling sequence data and indexing for classification purpose. The software uses a simple command line program for learning and predicting processes, and has the ability to generate user-friendly graphics depicting decision trees. The underlying C++ codes are designed to efficiently process large data sets in ASCII files. Two examples from transaction data sets are used to illustrate the application of mbonsai. © 2018, American Statistical Association. All rights reserved."
"Logbin: An R package for relative risk regression using the log-binomial model",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053243344&doi=10.18637%2fjss.v086.i09&partnerID=40&md5=303587cbdefbfbceb5a6536625f6d72a","Relative risk regression using a log-link binomial generalized linear model (GLM) is an important tool for the analysis of binary outcomes. However, Fisher scoring, which is the standard method for fitting GLMs in statistical software, may have difficulties in converging to the maximum likelihood estimate due to implicit parameter constraints. logbin is an R package that implements several algorithms for fitting relative risk regression models, allowing stable maximum likelihood estimation while ensuring the required parameter constraints are obeyed. We describe the logbin package and examine its stability and speed for different computational algorithms. We also describe how the package may be used to include flexible semi-parametric terms in relative risk regression models. © 2018, American Statistical Association. All rights reserved."
"Calculating probabilistic excursion sets and related quantities using excursions",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053241479&doi=10.18637%2fjss.v086.i05&partnerID=40&md5=6551abb6497b27cd6915cb4adffb33d2","The R software package excursions contains methods for calculating probabilistic excursion sets, contour credible regions, and simultaneous confidence bands for latent Gaussian stochastic processes and fields. It also contains methods for uncertainty quantification of contour maps and computation of Gaussian integrals. This article describes the theoretical and computational methods used in the package. The main functions of the package are introduced and two examples illustrate how the package can be used. © 2018, American Statistical Association. All rights reserved."
"Downscale: An R package for downscaling species occupancy from coarse-grain data to predict occupancy at fine-grain sizes",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053237339&doi=10.18637%2fjss.v086.c03&partnerID=40&md5=11d795264af02aabe7c1883460173c5c","The geographical area occupied by a species is a valuable measure for assessing its conservation status. Coarse-grained occupancy maps are available for many taxa, e.g., as atlases, but often at spatial resolutions too coarse for conservation use. However, mapping occupancy at fine spatial resolution across the entire extent of the species’ distribution is often prohibitively expensive for the majority of species. Occupancy downscaling is a technique to estimate finer scale occupancy from coarse scale maps, by using the occupancy-area relationship (OAR) which reflects how the proportion of area occupied increases with spatial grain size. Models that describe the OAR are fitted to observed occupancies at the available coarse-grain sizes and then extrapolated to predict occupancy at the finer grain sizes required. The downscale package in the R programming environment provides users with easy-to-use functions for downscaling occupancy with ten published models. First, upgrain calculates occupancy for multiple grain sizes larger than the input data. Normal methods for aggregating raster data increase the extent of the focal area as grain size increases which is undesirable, so the function fixes the extent for all grain sizes, assigning unsampled cells as absences. Four suggested methods are provided to enable this and upgrain.threshold provides diagnostic plots that allow the user to explore the inherent trade-off between making assumptions about unsampled locations and discarding information from sampled locations. downscale fits nine possible models to the data generated from upgrain. hui.downscale fits the special case of the Hui model. predict and plot extrapolate the fitted models to predict and plot occupancy at finer grain sizes. Finally, ensemble.downscale simultaneously fits two or more of the downscaling models and calculates mean predicted occupancy across all selected models. Here we describe the package and apply the functions to atlas data of a hypothetical UK species. © 2018, American Statistical Association. All rights reserved."
"mipfp: An R package for multidimensional array fitting and simulating multivariate bernoulli distributions",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053233595&doi=10.18637%2fjss.v086.c02&partnerID=40&md5=72e3b2a9a977136ebba0f3a0a47571e1","This paper explains the mipfp package for R with the core functionality of updating an d-dimensional array with respect to given target marginal distributions, which in turn can be multi-dimensional. The implemented methods include the iterative proportional fitting procedure (IPFP), the maximum likelihood method, the minimum chi-square and least squares procedures. The package also provides an application of the IPFP to simulate data from a multivariate Bernoulli distribution. The functionalities of the package are illustrated through two practical examples: the update of a 3-dimensional contingency table to match the targets for a synthetic population and the estimation and simulation of the joint distribution of the binary attribute impaired pulmonary function as used by Qaqish, Zink, and Preisser (2012). © 2018, American Statistical Association. All rights reserved."
"Image segmentation, registration and characterization in R with simpleITK",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053232385&doi=10.18637%2fjss.v086.i08&partnerID=40&md5=eb74b409d6f2b7a6990144acb3dbca2c","Many types of medical and scientific experiments acquire raw data in the form of images. Various forms of image processing and image analysis are used to transform the raw image data into quantitative measures that are the basis of subsequent statistical analysis. In this article we describe the SimpleITK R package. SimpleITK is a simplified interface to the insight segmentation and registration toolkit (ITK). ITK is an open source C++ toolkit that has been actively developed over the past 18 years and is widely used by the medical image analysis community. SimpleITK provides packages for many interpreter environments, including R. Currently, it includes several hundred classes for image analysis including a wide range of image input and output, filtering operations, and higher level components for segmentation and registration. Using SimpleITK, development of complex combinations of image and statistical analysis procedures is feasible. This article includes several examples of computational image analysis tasks implemented using SimpleITK, including spherical marker localization, multi-modal image registration, segmentation evaluation, and cell image analysis. © 2018, American Statistical Association. All rights reserved."
"Automated general-to-specific (GETS) regression modeling and indicator saturation for outliers and structural breaks",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053231924&doi=10.18637%2fjss.v086.i03&partnerID=40&md5=2aaf6daef275c479e80338d1b0b30ed1","This paper provides an overview of the R package gets, which contains facilities for automated general-to-specific (GETS) modeling of the mean and variance of a regression, and indicator saturation (IS) methods for the detection and modeling of outliers and structural breaks. The mean can be specified as an autoregressive model with covariates (an “AR-X” model), and the variance can be specified as an autoregressive log-variance model with covariates (a “log-ARCH-X” model). The covariates in the two specifications need not be the same, and the classical linear regression model is obtained as a special case when there is no dynamics, and when there are no covariates in the variance equation. The four main functions of the package are arx, getsm, getsv and isat. The first function estimates an AR-X model with log-ARCH-X errors. The second function undertakes GETS modeling of the mean specification of an ‘arx’ object. The third function undertakes GETS modeling of the log-variance specification of an ‘arx’ object. The fourth function undertakes GETS modeling of an indicator-saturated mean specification allowing for the detection of outliers and structural breaks. The usage of two convenience functions for export of results to EViews and Stata are illustrated, and LATEX code of the estimation output can readily be generated. © 2018, American Statistical Association. All rights reserved."
"Developer-friendly and computationally efficient predictive modeling without information leakage: The emil package for R",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052589474&doi=10.18637%2fjss.v085.i13&partnerID=40&md5=639f46b108ac1e1f299c9ee58df1177d","Data driven machine learning for predictive modeling problems (classification, regression, or survival analysis) typically involves a number of steps beginning with data preprocessing and ending with performance evaluation. A large number of packages providing tools for the individual steps are available for R, but there is a lack of tools for facilitating rigorous performance evaluation of the complete procedures assembled from them by means of cross-validation, bootstrap, or similar methods. Such a tool should strictly prevent test set observations from influencing model training and meta-parameter tuning, so-called information leakage, in order to not produce overly optimistic performance estimates. Here we present a new package for R denoted emil (evaluation of modeling without information leakage) that offers this form of performance evaluation. It provides a transparent and highly customizable framework for facilitating the assembly, execution, performance evaluation, and interpretation of complete procedures for classification, regression, and survival analysis. The components of package emil have been designed to be as modular and general as possible to allow users to combine, replace, and extend them if needed. Package emil was also developed with scalability in mind and has a small computational overhead, which is a key requirement for analyzing the very big data sets now available in fields like medicine, physics, and finance. First package emil’s functionality and usage is explained. Then three specific application examples are presented to show its potential in terms of parallelization, customization for survival analysis, and development of ensemble models. Finally a brief comparison to similar software is provided. © 2018, American Statistical Association. All rights reserved."
"ContaminatedMixt: An R package for fitting parsimonious mixtures of multivariate contaminated normal distributions",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052587682&doi=10.18637%2fjss.v085.i10&partnerID=40&md5=273ce9783fc39929db87849c74105a2a","We introduce the R package ContaminatedMixt, conceived to disseminate the use of mixtures of multivariate contaminated normal distributions as a tool for robust clustering and classification under the common assumption of elliptically contoured groups. Thirteen variants of the model are also implemented to introduce parsimony. The expectationconditional maximization algorithm is adopted to obtain maximum likelihood parameter estimates, and likelihood-based model selection criteria are used to select the model and the number of groups. Parallel computation can be used on multicore PCs and computer clusters, when several models have to be fitted. Differently from the more popular mixtures of multivariate normal and t distributions, this approach also allows for automatic detection of mild outliers via the maximum a posteriori probabilities procedure. To exemplify the use of the package, applications to artificial and real data are presented. © 2018, American Statistical Association. All rights reserved."
"NeuralNetTools: Visualization and analysis tools for neural networks",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052574617&doi=10.18637%2fjss.v085.i11&partnerID=40&md5=c18ddf6db4f8ff271e4fdfda72b361c2","Supervised neural networks have been applied as a machine learning technique to identify and predict emergent patterns among multiple variables. A common criticism of these methods is the inability to characterize relationships among variables from a fitted model. Although several techniques have been proposed to “illuminate the black box”, they have not been made available in an open-source programming environment. This article describes the NeuralNetTools package that can be used for the interpretation of supervised neural network models created in R. Functions in the package can be used to visualize a model using a neural network interpretation diagram, evaluate variable importance by disaggregating the model weights, and perform a sensitivity analysis of the response variables to changes in the input variables. Methods are provided for objects from many of the common neural network packages in R, including caret, neuralnet, nnet, and RSNNS. The article provides a brief overview of the theoretical foundation of neural networks, a description of the package structure and functions, and an applied example to provide a context for model development with NeuralNetTools. Overall, the package provides a toolset for neural networks that complements existing quantitative techniques for data-intensive exploration. © 2018, American Statistical Association. All rights reserved."
"Generalization, combination and extension of functional clustering algorithms: The R package funcy",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052523257&doi=10.18637%2fjss.v085.i09&partnerID=40&md5=8edfad3f524bca84ad1758bace1ed90f","Clustering functional data is mostly based on the projection of the curves onto an adequate basis and building random effects models of the basis coefficients. The parameters can be fitted with an EM algorithm. Alternatively, distance models based on the coefficients are used in the literature. Similar to the case of clustering multidimensional data, a variety of derivations of different models has been published. Although their calculation procedure is similar, their implementations are very different including distinct hyperparameters and data formats as input. This makes it difficult for the user to apply and particularly to compare them. Furthermore, they are mostly limited to specific basis functions. This paper aims to show the common elements between existing models in highly cited articles, first on a theoretical basis. Later their implementation is analyzed and it is illustrated how they could be improved and extended to a more general level. A special consideration is given to those models designed for sparse measurements. The work resulted in the R package funcy which was built to integrate the modified and extended algorithms into a unique framework. © 2018, American Statistical Association. All rights reserved."
"Rmcfs: An R package for monte carlo feature selection and interdependency discovery",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052522478&doi=10.18637%2fjss.v085.i12&partnerID=40&md5=e46c2e6058175b4cd6c9b2437c34bdc0","We describe the R package rmcfs that implements an algorithm for ranking features from high dimensional data according to their importance for a given supervised classification task. The ranking is performed prior to addressing the classification task per se. This R package is the new and extended version of the MCFS (Monte Carlo feature selection) algorithm where an early version was published in 2005. The package provides an easy R interface, a set of tools to review results and the new ID (interdependency discovery) component. The algorithm can be used on continuous and/or categorical features (e.g., gene expression and phenotypic data) to produce an objective ranking of features with a statistically well-defined cutoff between informative and non-informative ones. Moreover, the directed ID graph that presents interdependencies between informative features is provided. © 2018, American Statistical Association. All rights reserved."
"Flexcwm: A flexible framework for cluster-weighted models",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050112048&doi=10.18637%2fjss.v086.i02&partnerID=40&md5=dede89863294f7c9224b058f6f981f03","Cluster-weighted models (CWMs) are mixtures of regression models with random covariates. However, besides having recently become rather popular in statistics and data mining, there is still a lack of support for CWMs within the most popular statistical suites. In this paper, we introduce flexCWM, an R package specifically conceived for fitting CWMs. The package supports modeling the conditioned response variable by means of the most common distributions of the exponential family and by the t distribution. Covariates are allowed to be of mixed-type and parsimonious modeling of multivariate normal covariates, based on the eigenvalue decomposition of the component covariance matrices, is supported. Furthermore, either the response or the covariates distributions can be omitted, yielding to mixtures of distributions and mixtures of regression models with fixed covariates, respectively. The expectation-maximization (EM) algorithm is used to obtain maximum-likelihood estimates of the parameters and likelihood-based information criteria are adopted to select the number of groups and/or a parsimonious model. For the component regression coefficients, standard errors and significance tests are also provided. Parallel computation can be used on multicore PCs and computer clusters, when several models have to be fitted. To exemplify the use of the package, applications to artificial and real datasets, included in the package, are presented. © 2018, American Statistical Association. All rights reserved."
"Beanz: An R package for Bayesian analysis of heterogeneous treatment effects with a graphical user interface",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049527835&doi=10.18637%2fjss.v085.i07&partnerID=40&md5=7ce7c44e55deb9e1cc071b155993419a","In patient-centered outcomes research, it is essential to assess the heterogeneity of treatment effects (HTE) when making health care decisions for an individual patient or a group of patients. Nevertheless, it remains challenging to evaluate HTE based on information collected from clinical studies that are often designed and conducted to evaluate the efficacy of a treatment for the overall population. The Bayesian framework offers a principled and flexible approach to estimate and compare treatment effects across subgroups of patients defined by their characteristics. In this paper, we describe the package beanz which facilitates the conduct of Bayesian analysis of HTE by allowing users to explore a wide range of Bayesian HTE analysis models and produce posterior inferences about HTE. The package beanz also provides a web-based graphical user interface (GUI) for users to conduct the Bayesian analysis of HTE in an interactive and user-friendly manner. With the GUI feature, package beanz can also be used by analysts not familiar with the R environment. We demonstrate package beanz using data from a randomized controlled trial on angiotensin converting enzyme inhibitor for treating congestive heart failure (N = 2569). © 2018, American Statistical Association. All rights reserved."
"Bayesian linear mixed models with polygenic effects",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048784977&doi=10.18637%2fjss.v085.i06&partnerID=40&md5=a943c172572e5fe7189602b5a3a77879","We considered Bayesian estimation of polygenic effects, in particular heritability in relation to a class of linear mixed models implemented in R (R Core Team 2018). Our approach is applicable to both family-based and population-based studies in human genetics with which a genetic relationship matrix can be derived either from family structure or genome-wide data. Using a simulated and a real data, we demonstrate our implementation of the models in the generic statistical software systems JAGS (Plummer 2017) and Stan (Carpenter et al. 2017) as well as several R packages. In doing so, we have not only provided facilities in R linking standalone programs such as GCTA (Yang, Lee, Goddard, and Visscher 2011) and other packages in R but also addressed some technical issues in the analysis. Our experience with a host of general and special software systems will facilitate investigation into more complex models for both human and nonhuman genetics. © 2018, American Statistical Association. All rights reserved."
"RRreg: An R package for correlation and regression analyses of randomized response data",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048589544&doi=10.18637%2fjss.v085.i02&partnerID=40&md5=80c8c9562973bd9d9f7f170af71feb90","The randomized response (RR) technique was developed to improve the validity of measures assessing attitudes, behaviors, and attributes threatened by social desirability bias. The RR removes any direct link between individual responses and the sensitive attribute to maximize the anonymity of respondents and, in turn, to elicit more honest responding. Since multivariate analyses are no longer feasible using standard methods, we present the R package RRreg that allows for multivariate analyses of RR data in a user-friendly way. We show how to compute bivariate correlations, how to predict an RR variable in an adapted logistic regression framework (with or without random effects), and how to use RR predictors in a modified linear regression. In addition, the package allows for power analysis and robustness simulations. To facilitate the application of these methods, we illustrate the benefits of multivariate methods for RR variables using an empirical example. © 2018, American Statistical Association. All rights reserved."
"blavaan: bayesian structural equation models via parameter expansion",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048588061&doi=10.18637%2fjss.v085.i04&partnerID=40&md5=0147703a5a99306982bce9655e5b17d9","This article describes blavaan, an R package for estimating Bayesian structural equation models (SEMs) via JAGS and for summarizing the results. It also describes a novel parameter expansion approach for estimating specific types of models with residual covariances, which facilitates estimation of these models in JAGS. The methodology and software are intended to provide users with a general means of estimating Bayesian SEMs, both classical and novel, in a straightforward fashion. Users can estimate Bayesian versions of classical SEMs with lavaan syntax, they can obtain state-of-the-art Bayesian fit measures associated with the models, and they can export JAGS code to modify the SEMs as desired. These features and more are illustrated by example, and the parameter expansion approach is explained in detail. © 2018, American Statistical Association. All rights reserved."
"ISAP-MATLAB package for sensitivity analysis of high-dimensional stochastic chemical networks",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048584428&doi=10.18637%2fjss.v085.i03&partnerID=40&md5=0d07b48cb04ab43833448816471d8856","Stochastic simulation and modeling play an important role to elucidate the fundamental mechanisms in complex biochemical networks. The parametric sensitivity analysis of reaction networks becomes a powerful mathematical and computational tool, yielding information regarding the robustness and the identifiability of model parameters. However, due to overwhelming computational cost, parametric sensitivity analysis is a extremely challenging problem for stochastic models with a high-dimensional parameter space and for which existing approaches are very slow. Here we present an information-theoretic sensitivity analysis in path-space (ISAP) MATLAB package that simulates stochastic processes with various algorithms and most importantly implements a gradient-free approach to quantify the parameter sensitivities of stochastic chemical reaction network dynamics using the pathwise Fisher information matrix (PFIM; Pantazis, Katsoulakis, and Vlachos 2013). The sparse, block-diagonal structure of the PFIM makes its computational complexity scale linearly with the number of model parameters. As a result of the gradientfree and the sparse nature of the PFIM, it is highly suitable for the sensitivity analysis of stochastic reaction networks with a very large number of model parameters, which are typical in the modeling and simulation of complex biochemical phenomena. Finally, the PFIM provides a fast sensitivity screening method (Arampatzis, Katsoulakis, and Pantazis 2015) which allows it to be combined with any existing sensitivity analysis software. © 2018, American Statistical Association. All rights reserved."
"hergm: Hierarchical exponential-family random graph models",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048560384&doi=10.18637%2fjss.v085.i01&partnerID=40&md5=05f734b82038f84cdf157c3e9a28a0b4","We describe the R package hergm that implements hierarchical exponential-family random graph models with local dependence. Hierarchical exponential-family random graph models with local dependence tend to be superior to conventional exponential-family random graph models with global dependence in terms of goodness-of-fit. The advantage of hierarchical exponential-family random graph models is rooted in the local dependence induced by them. We discuss the notion of local dependence and the construction of models with local dependence along with model estimation, goodness-of-fit, and simulation. Simulation results and three applications are presented. © 2018, American Statistical Association. All rights reserved."
"QXLA: adding upper quantiles for the studentized range to excel for multiple comparison procedures",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048535104&doi=10.18637%2fjss.v085.c01&partnerID=40&md5=de2d6240680fd29684541761fdf655c2","Microsoft Excel has some functionality in terms of basic statistics; however it lacks distribution functions built around the studentized range (Q). The developed Excel addin introduces two new user-defined functions, QDISTG and QINVG, based on the studentized range Q-distribution that expands the functionality of Excel for statistical analysis. A workbook example, demonstrating the Tukey, S-N-K, and REGWQ tests, has also been included. Compared with other options available, the method is fast with low error rates. © 2018, American Statistical Association. All rights reserved."
"R package DoE.base for factorial experiments",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048522209&doi=10.18637%2fjss.v085.i05&partnerID=40&md5=ea57a05ab524e93afdebdd52a5756f19","The R package DoE.base can be used for creating full factorial designs and general factorial experiments based on orthogonal arrays. Besides design creation, some analysis functionality is also available, particularly (augmented) half-normal effects plots. In addition to this specific functionality, the package provides convenience features for analyzing experimental designs and the infrastructure for a suite of further packages on designing and analyzing experiments. This infrastructure is available for use also by further design of experiments packages. © 2018, American Statistical Association. All rights reserved."
"Epimodel: An R package for mathematical modeling of infectious disease over networks",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046156784&doi=10.18637%2fjss.v084.i08&partnerID=40&md5=f63acdae45a2c53302894ba0a95a03ed","Package EpiModel provides tools for building, simulating, and analyzing mathematical models for the population dynamics of infectious disease transmission in R. Several classes of models are included, but the unique contribution of this software package is a general stochastic framework for modeling the spread of epidemics on networks. EpiModel integrates recent advances in statistical methods for network analysis (temporal exponential random graph models) that allow the epidemic modeling to be grounded in empirical data on contacts that can spread infection. This article provides an overview of both the modeling tools built into EpiModel, designed to facilitate learning for students new to modeling, and the application programming interface for extending package EpiModel, designed to facilitate the exploration of novel research questions for advanced modelers. © 2018, American Statistical Association. All rights reserved."
"Kdecopula: An R package for the kernel estimation of bivariate copula densities",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046152433&doi=10.18637%2fjss.v084.i07&partnerID=40&md5=1ae5f1b3b532b56651c037d3e4dd8bfb","We describe the R package kdecopula (current version 0.9.2), which provides fast implementations of various kernel estimators for the copula density. Due to a variety of available plotting options it is particularly useful for the exploratory analysis of dependence structures. It can be further used for accurate nonparametric estimation of copula densities and resampling. The implementation features spline interpolation of the estimates to allow for fast evaluation of density estimates and integrals thereof. We utilize this for a fast renormalization scheme that ensures that estimates are bona fide copula densities and additionally improves the estimators’ accuracy. The performance of the methods is illustrated by simulations. © 2018, American Statistical Association. All rights reserved."
"Stampr: Spatial-temporal analysis of moving polygons in R",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046137412&doi=10.18637%2fjss.v084.c01&partnerID=40&md5=1cb1537d38a047a8ba68fc0529cbc56f","The R package stampr implements functions for analyzing movement in mapped polygon data. Methods described in this paper include deriving change events based on spatial relationships, plotting change events, summarizing measures of distance and direction of movement, characterizing changes in polygon shape changes, and characterizing sequences of polygons over time using graphs. Two examples are used to demonstrate the core functionality available in the stampr package. © 2018, American Statistical Association. All rights reserved."
"Spatio-temporal areal unit modeling in R with conditional autoregressive priors using the CARBayesST package",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046133060&doi=10.18637%2fjss.v084.i09&partnerID=40&md5=1b9f1d25b3cd9ea1ffb1d8163ea40d47","Spatial data relating to non-overlapping areal units are prevalent in fields such as economics, environmental science, epidemiology and social science, and a large suite of modeling tools have been developed for analysing these data. Many utilize conditional autoregressive (CAR) priors to capture the spatial autocorrelation inherent in these data, and software packages such as CARBayes and R-INLA have been developed to make these models easily accessible to others. Such spatial data are typically available for multiple time periods, and the development of methodology for capturing temporally changing spatial dynamics is the focus of much current research. A sizeable proportion of this literature has focused on extending CAR priors to the spatio-temporal domain, and this article presents the R package CARBayesST, which is the first dedicated software package for spatio-temporal areal unit modeling with conditional autoregressive priors. The software package allows to fit a range of models focused on different aspects of spacetime modeling, including estimation of overall space and time trends, and the identification of clusters of areal units that exhibit elevated values. This paper outlines the class of models that the software package implement, before applying them to simulated and two real examples from the fields of epidemiology and housing market analysis. © 2018, American Statistical Association. All rights reserved."
"SpaSM: A MATLAB toolbox for sparse statistical modeling",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046132601&doi=10.18637%2fjss.v084.i10&partnerID=40&md5=4552801f340168e79e0e0d6050c66b09","Applications in biotechnology such as gene expression analysis and image processing have led to a tremendous development of statistical methods with emphasis on reliable solutions to severely underdetermined systems. Furthermore, interpretations of such solutions are of importance, meaning that the surplus of inputs has been reduced to a concise model. At the core of this development are methods which augment the standard linear models for regression, classification and decomposition such that sparse solutions are obtained. This toolbox aims at making public available carefully implemented and well-tested variants of the most popular of such methods for the MATLAB programming environment. These methods consist of easy-to-read yet efficient implementations of various coefficient-path following algorithms and implementations of sparse principal component analysis and sparse discriminant analysis which are not available in MATLAB. The toolbox builds on code made public in 2005 and which has since been used in several studies. © 2018, American Statistical Association. All rights reserved."
"Tmap: Thematic maps in R",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046131065&doi=10.18637%2fjss.v084.i06&partnerID=40&md5=875dfdf3296d3d289b2addf1e81151dc","Thematic maps show spatial distributions. The theme refers to the phenomena that is shown, which is often demographical, social, cultural, or economic. The best known thematic map type is the choropleth, in which regions are colored according to the distribution of a data variable. The R package tmap offers a coherent plotting system for thematic maps that is based on the layered grammar of graphics. Thematic maps are created by stacking layers, where per layer, data can be mapped to one or more aesthetics. It is also possible to generate small multiples. Thematic maps can be further embellished by configuring the map layout and by adding map attributes, such as a scale bar and a compass. Besides plotting thematic maps on the graphics device, they can also be made interactive as an HTML widget. In addition, the R package tmaptools contains several convenient functions for reading and processing spatial data. © 2018, American Statistical Association. All rights reserved."
"Multipanelfigure: Simple assembly of multiple plots and images into a compound figure",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046130291&doi=10.18637%2fjss.v084.c03&partnerID=40&md5=9310538967e226a2bca770e03b789b6c","In scholarly publications, multiple graphical elements such as plots or raster images are traditionally combined into a single compound figure using panels arranged in a grid. Package multipanelfigure is a GPL-3 licensed R package that eases assembly of such compound figures, allowing for straightforward integration of R specific plots using base graphics, lattice and ggplot2 plots, as well as grid grobs in general. Also provided are facilities to incorporate R external graphical output stored as SVG, PNG, JPEG, or TIFF formatted files. © 2018, American Statistical Association. All rights reserved."
"Evmix: An R package for extreme value mixture modeling, threshold estimation and boundary corrected kernel density estimation",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045672590&doi=10.18637%2fjss.v084.i05&partnerID=40&md5=669756f5311758df17e412fed984cd18","evmix is an R package (R Core Team 2017) with two interlinked toolsets: i) for extreme value modeling and ii) kernel density estimation. A key issue in univariate extreme value modeling is the choice of threshold beyond which the asymptotically motivated extreme value models provide a suitable tail approximation. The package implements almost all existing extreme value mixture models, which permit objective threshold estimation and uncertainty quantification. Some traditional diagnostic plots for threshold choice are provided. Kernel density estimation with a range of kernels is provided, including cross-validation maximum likelihood inference for the bandwidth. A key contribution over existing kernel smoothing packages in R is that a wide range of boundary corrected kernel density estimators are implemented, which are designed for populations with bounded support. These non-parametric density estimators are also incorporated into the extreme value mixture model framework to describe the density below the threshold. The quartet of density, distribution, quantile and random number generation functions is provided along with parameter estimation by likelihood inference and standard model fit diagnostics, for both the mixture models and kernel density estimators. The key features of the mixture models and (boundary corrected) kernel density estimators are described and their implementation using the package demonstrated. © 2018, American Statistical Association. All rights reserved."
"Clustvarsel: A package implementing variable selection for Gaussian model-based clustering in R",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045670576&doi=10.18637%2fjss.v084.i01&partnerID=40&md5=b0e0ed2f3a6d1c6add97e2b7a4363415","Finite mixture modeling provides a framework for cluster analysis based on parsimonious Gaussian mixture models. Variable or feature selection is of particular importance in situations where only a subset of the available variables provide clustering information. This enables the selection of a more parsimonious model, yielding more efficient estimates, a clearer interpretation and, often, improved clustering partitions. This paper describes the R package clustvarsel which performs subset selection for model-based clustering. An improved version of the Raftery and Dean (2006) methodology is implemented in the new release of the package to find the (locally) optimal subset of variables with group/cluster information in a dataset. Search over the solution space is performed using either a stepwise greedy search or a headlong algorithm. Adjustments for speeding up these algorithms are discussed, as well as a parallel implementation of the stepwise search. Usage of the package is presented through the discussion of several data examples. © 2018, American Statistical Association. All rights reserved."
"StMoMo: Stochastic mortality modeling in R",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045662037&doi=10.18637%2fjss.v084.i03&partnerID=40&md5=39571ca75b0a86ff7e8d7b1cb23a8334","In this paper we mirror the framework of generalized (non-)linear models to define the family of generalized age-period-cohort stochastic mortality models which encompasses the vast majority of stochastic mortality projection models proposed to date, including the well-known Lee-Carter and Cairns-Blake-Dowd models. We also introduce the R package StMoMo which exploits the unifying framework of the generalized age-period-cohort family to provide tools for fitting stochastic mortality models, assessing their goodness-of-fit and performing mortality projections. We illustrate some of the capabilities of the package by performing a comparison of several stochastic mortality models applied to the England and Wales population. © 2018, American Statistical Association. All rights reserved."
"Weighted cox regression using the R package coxphw",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045644098&doi=10.18637%2fjss.v084.i02&partnerID=40&md5=c8bf3386ea1a74f451210283e9c4771b","Cox’s regression model for the analysis of survival data relies on the proportional hazards assumption. However, this assumption is often violated in practice and as a consequence the average relative risk may be under- or overestimated. Weighted estimation of Cox regression is a parsimonious alternative which supplies well interpretable average effects also in case of non-proportional hazards. We provide the R package coxphw implementing weighted Cox regression. By means of two biomedical examples appropriate analyses in the presence of non-proportional hazards are exemplified and advantages of weighted Cox regression are discussed. Moreover, using package coxphw, time-dependent effects can be conveniently estimated by including interactions of covariates with arbitrary functions of time. © 2018, American Statistical Association. All rights reserved."
"Multiple response variables regression models in R: The mcglm package",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045619179&doi=10.18637%2fjss.v084.i04&partnerID=40&md5=3fbc46d4b7777a52450e45ccb1109cf1","This article describes the R package mcglm implemented for fitting multivariate covariance generalized linear models (McGLMs). McGLMs provide a general statistical modeling framework for normal and non-normal multivariate data analysis, designed to handle multivariate response variables, along with a wide range of temporal and spatial correlation structures defined in terms of a covariance link function and a matrix linear predictor involving known symmetric matrices. The models take non-normality into account in the conventional way by means of a variance function, and the mean structure is modeled by means of a link function and a linear predictor. The models are fitted using an estimating function approach based on second-moment assumptions. This provides a unified approach to a wide variety of different types of response variables and covariance structures, including multivariate extensions of repeated measures, time series, longitudinal, genetic, spatial and spatio-temporal structures. The mcglm package allows a flexible specification of the mean and covariance structures, and explicitly deals with multivariate response variables, through a user friendly formula interface similar to the ordinary glm function. Illustrations in this article cover a wide range of applications from the traditional one response variable Gaussian mixed models to multivariate spatial models for areal data using the multivariate Tweedie distribution. Additional features, such as robust and bias-corrected standard errors for regression parameters, residual analysis, measures of goodness-of-fit and model selection using the score information criterion are discussed through six worked examples. The mcglm package is a full R implementation based on the Matrix package which provides efficient access to BLAS (basic linear algebra subrou-tines), Lapack (dense matrix), TAUCS (sparse matrix) and UMFPACK (sparse matrix) routines for efficient linear algebra in R. © 2018, American Statistical Association. All rights reserved."
"Estimation of transition probabilities for the illness-death model: Package TP.idm",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042729984&doi=10.18637%2fjss.v083.i10&partnerID=40&md5=8b8689a58c11a96f1f142832796ca01e","In this paper the R package TP.idm to compute an empirical transition probability matrix for the illness-death model is introduced. This package implements a novel nonparametric estimator which is particularly well suited for non-Markov processes observed under right censoring. Variance estimates and confidence limits are also implemented in the package. © 2018, American Statistical Association. All rights reserved."
"kamila: Clustering mixed-type data in R and hadoop",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042715326&doi=10.18637%2fjss.v083.i13&partnerID=40&md5=cd5fbecd9438a4d4f561badd901359d9","In this paper we discuss the challenge of equitably combining continuous (quantita-tive) and categorical (qualitative) variables for the purpose of cluster analysis. Existing techniques require strong parametric assumptions, or difficult-to-specify tuning parameters. We describe the kamila package, which includes a weighted k-means approach to clustering mixed-type data, a method for estimating weights for mixed-type data (Modha-Spangler weighting), and an additional semiparametric method recently proposed in the literature (KAMILA). We include a discussion of strategies for estimating the number of clusters in the data, and describe the implementation of one such method in the current R package. Background and usage of these clustering methods are presented. We then show how the KAMILA algorithm can be adapted to a map-reduce framework, and implement the resulting algorithm using Hadoop for clustering very large mixed-type data sets. © 2018, American Statistical Association. All rights reserved."
"Pptreeviz: An R package for visualizing projection pursuit classification trees",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042701953&doi=10.18637%2fjss.v083.i08&partnerID=40&md5=6b76059d766e12a740f448a32197f71e","PPtreeViz, an R package, was developed to explore projection pursuit methods for classification. It provides functions to calculate various projection pursuit indices for classification and to explore the results in the space of projection. It also provides functions for the projection pursuit classification tree. The visualization methods of the tree structure and the features of each node in PPtreeViz can be used to easily explore the projection pursuit classification tree structure and determine the characteristics of each class. To calculate the projection pursuit indices and optimize these indices, we use the Rcpp and RcppArmadillo packages in R to improve the speed. © 2018, American Statistical Association. All rights reserved."
"Mplot: An r package for graphical model stability and variable selection procedures",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042696817&doi=10.18637%2fjss.v083.i09&partnerID=40&md5=8fe108170dd96357c6b05222368138c9","The mplot package provides an easy to use implementation of model stability and variable inclusion plots (Müller and Welsh 2010; Murray, Heritier, and Müller 2013) as well as the adaptive fence (Jiang, Rao, Gu, and Nguyen 2008; Jiang, Nguyen, and Rao 2009) for linear and generalized linear models. We provide a number of innovations on the standard procedures and address many practical implementation issues including the addition of redundant variables, interactive visualizations and the approximation of logistic models with linear models. An option is provided that combines our bootstrap approach with glmnet for higher dimensional models. The plots and graphical user interface leverage state of the art web technologies to facilitate interaction with the results. The speed of implementation comes from the leaps package and cross-platform multicore support. © 2018, American Statistical Association. All rights reserved."
"Constructing multivariate survival trees: The MST package for R",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042696294&doi=10.18637%2fjss.v083.i12&partnerID=40&md5=326e36d704d2bcfa116be0cd94eac918","Multivariate survival trees require few statistical assumptions, are easy to interpret, and provide meaningful diagnosis and prediction rules. Trees can handle a large number of predictors with mixed types and do not require predictor variable transformation or selection. These are useful features in many application fields and are often required in the current era of big data. The aim of this article is to introduce the R package MST. This package constructs multivariate survival trees using marginal model and frailty model based approaches. It allows the user to control and see how the trees are constructed. The package can also simulate high-dimensional, multivariate survival data from marginal and frailty models. © 2018, American Statistical Association. All rights reserved."
"Teigen: An R package for model-based clustering and classification via the multivariate t distribution",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042693105&doi=10.18637%2fjss.v083.i07&partnerID=40&md5=4436bc10a24d0b225355e5a30906c814","The teigen R package is introduced and utilized for model-based clustering and classification. The tEIGEN family of mixtures of multivariate t distributions is formed via an eigen-decomposition of the component covariance matrices and subsequent component-wise constraints. The teigen package implements all previously published tEIGEN family members as well as eight additional models: four multivariate and four univariate. The resulting family of 32 mixture models is implemented in both serial and parallel, with useful dedicated functions. Methodology and examples that illustrate teigen’s functionality are presented. © 2018, American Statistical Association. All rights reserved."
"epinet: An R package to analyze epidemics spread across contact networks",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042686753&doi=10.18637%2fjss.v083.i11&partnerID=40&md5=8dc889deb3bdb55cf7ac4eae4bd7c996","We present the R package epinet, which provides tools for analyzing the spread of epidemics through populations. We assume that the relationships among individuals in a population are modeled by a contact network described by an exponential-family random graph model and that the disease being studied spreads across the edges of this network from infectious to susceptible individuals. We use a susceptible-exposed-infectious-removed compartmental model to describe the progress of the disease within each host. We describe the functionality of the package, which consists of routines that perform simulation, plotting, and inference. The main inference routine utilizes a Bayesian approach and a Markov chain Monte Carlo algorithm. We demonstrate the use of the package through two examples, one involving simulated data and one using data from an actual measles outbreak. © 2018, American Statistical Association. All rights reserved."
"EMMIXcskew: An R package for the fitting of a mixture of canonical fundamental skew t-distributions",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042516064&doi=10.18637%2fjss.v083.i03&partnerID=40&md5=235509c9e93498cec3fe10c0e27938b5","This paper presents the R package EMMIXcskew for the fitting of the canonical fundamental skew t-distribution (CFUST) and finite mixtures of CFUST distributions (FM-CFUST) via maximum likelihood (ML). The CFUST distribution provides a flexible family to model non-normal data, with parameters for capturing skewness and heavy-tails in the data. It formally encompasses the normal, t, and skew normal distributions as special and/or limiting cases. A few other versions of the skew t-distributions are also nested within the CFUST distribution. In this paper, an expectation-maximization (EM) algorithm is described for computing the ML estimates of the parameters of the FM-CFUST model, and different strategies for initializing the algorithm are discussed and illustrated. The methodology is implemented in the EMMIXcskew package, and examples are presented using two real datasets. The EMMIXcskew package contains functions to fit the FM-CFUST model, including procedures for generating different initial values. Additional features include random sample generation and contour visualization in 2D and 3D. © 2018, American Statistical Association. All rights reserved."
"Temporal exponential random graph models with btergm: Estimation and bootstrap confidence intervals",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042507076&doi=10.18637%2fjss.v083.i06&partnerID=40&md5=bddb5509f025aaa50f1677e6d7bd4525","The xergm package is an implementation of extensions to the exponential random graph model (ERGM). It acts as a meta-package for multiple constituent packages. One of these packages is btergm, which implements bootstrap methods for the temporal ERGM estimated by maximum pseudolikelihood. Here, we illustrate the temporal exponential random graph model and its implementation in the package btergm using data on international alliances and a longitudinally observed friendship network in a Dutch school. © 2018, American Statistical Association. All rights reserved."
"Sptest: An R package implementing nonparametric tests of isotropy",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042504172&doi=10.18637%2fjss.v083.i04&partnerID=40&md5=358ccfa19f0ab82835f8937de7561a3f","An important step in modeling spatially-referenced data is appropriately specifying the second order properties of the random field. A scientist developing a model for spatial data has a number of options regarding the nature of the dependence between observations. One of these options is deciding whether or not the dependence between observations depends on direction, or, in other words, whether or not the spatial covariance function is isotropic. Isotropy implies that spatial dependence is a function of only the distance and not the direction of the spatial separation between sampling locations. A researcher may use graphical techniques, such as directional sample semivariograms, to determine whether an assumption of isotropy holds. These graphical diagnostics can be difficult to assess, subject to personal interpretation, and potentially misleading as they typically do not include a measure of uncertainty. In order to escape these issues, a hypothesis test of the assumption of isotropy may be more desirable. To avoid specification of the covariance function, a number of nonparametric tests of isotropy have been developed using both the spatial and spectral representations of random fields. Several of these nonparametric tests are implemented in the R package spTest, available on CRAN. We demonstrate how graphical techniques and the hypothesis tests programmed in package spTest can be used in practice to assess isotropy properties. © 2018, American Statistical Association. All rights reserved."
"VNM: An R package for finding multiple-objective optimal designs for the 4-parameter logistic model",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042475443&doi=10.18637%2fjss.v083.i05&partnerID=40&md5=6f08173b5ff481c08ad7571733fed84f","A multiple-objective optimal design is useful for dose-response studies because it can incorporate several objectives at the design stage. Objectives can be of varying interests and a properly constructed multiple-objective optimal design can provide user-specified efficiencies, delivering higher efficiencies for the more important objectives. In this work, we introduce the VNM package written in R for finding 3-objective locally optimal designs for the 4-parameter logistic (4PL) model widely used in education, bioscience and in the manufacturing industry. The package implements the methodology to construct multiple-objective optimal designs in Hyun and Wong (2015). As illustrative examples, we focus on a biomedical application where our objectives are to estimate: (1) the shape of the dose-response curve, (2) the median effective dose level (ED50) and (3) the minimum effective dose level (MED) in the 4PL model. Our VNM package uses a state-of-the-art algorithm to generate multiple-objective optimal designs that meet the user-specified efficiency requirement for each objective, provides tools for calculating the efficiency of the generated design under each objective and also a plot for confirming optimality of the VNM-generated design. The package can also be used to determine an optimal scheme for allocating subjects to the various doses when the number and doses of the drug are fixed in advance. © 2018, American Statistical Association. All rights reserved."
"Meta4diag: Bayesian bivariate meta-analysis of diagnostic test studies for routine practice",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042469222&doi=10.18637%2fjss.v083.i01&partnerID=40&md5=2cd9deddcfa58d2f55063d6d570a4405","This paper introduces the R package meta4diag for implementing Bayesian bivariate meta-analyses of diagnostic test studies. Our package meta4diag is a purpose-built front end of the R package INLA. While INLA offers full Bayesian inference for the large set of latent Gaussian models using integrated nested Laplace approximations, meta4diag extracts the features needed for bivariate meta-analysis and presents them in an intuitive way. It allows the user a straightforward model specification and offers user-specific prior distributions. Further, the newly proposed penalized complexity prior framework is supported, which builds on prior intuitions about the behaviors of the variance and correlation parameters. Accurate posterior marginal distributions for sensitivity and specificity as well as all hyperparameters, and covariates are directly obtained without Markov chain Monte Carlo sampling. Further, univariate estimates of interest, such as odds ratios, as well as the summary receiver operating characteristic (SROC) curve and other common graphics are directly available for interpretation. An interactive graphical user interface provides the user with the full functionality of the package without requiring any R programming. The package is available from the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=meta4diag/ and its usage will be illustrated using three real data examples. © 2018, American Statistical Association. All rights reserved."
"SIS: An R package for sure independence screening in ultrahigh-dimensional statistical models",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042466528&doi=10.18637%2fjss.v083.i02&partnerID=40&md5=e9c9f1cf3d482b23ed743019dfdaa302","We revisit sure independence screening procedures for variable selection in generalized linear models and the Cox proportional hazards model. Through the publicly available R package SIS, we provide a unified environment to carry out variable selection using iterative sure independence screening (ISIS) and all of its variants. For the regularization steps in the ISIS recruiting process, available penalties include the LASSO, SCAD, and MCP while the implemented variants for the screening steps are sample splitting, data-driven thresholding, and combinations thereof. Performance of these feature selection techniques is investigated by means of real and simulated data sets, where we find considerable improvements in terms of model selection and computational time between our algorithms and traditional penalized pseudo-likelihood methods applied directly to the full set of covariates. © 2018, American Statistical Association. All rights reserved."
"randomizeR: An R package for the assessment and implementation of randomization in clinical trials",2018,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041182313&doi=10.18637%2fjss.v085.i08&partnerID=40&md5=3e98cdee118a9967f89bf0b0b59477a9","Randomization in clinical trials is the key design technique to ensure the comparability of treatment groups. Although there exist a large number of software products which assist the researcher to implement randomization, no tool which would cover a wide range of procedures and allow the comparative evaluation of the procedures under practical restrictions has been proposed in the literature so far. The R package randomizeR addresses this need. The paper includes a detailed description of the randomizeR package that serves as a tutorial for the generation of randomization sequences and the assessment of randomization procedures. © 2018, American Statistical Association. All rights reserved."
"Nash optimal party positions: The nopp R package",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034108629&doi=10.18637%2fjss.v081.i11&partnerID=40&md5=2cae7a9c054656355bb6c656ea9e6d95","The R package nopp enables computing party/candidate ideological positions that correspond to a Nash equilibrium along a one-dimensional space. It accommodates alternative motivations in (each) party strategy while allowing estimation of the uncertainty around their optimal positions through Monte Carlo or bootstrap procedures. © 2017, American Statistical Association. All rights reserved."
"Half-normal plots and overdispersed models in R: The hnp package",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034064874&doi=10.18637%2fjss.v081.i10&partnerID=40&md5=de3d48a8c40dae8d72dea03b8a95bf3f","Count and proportion data may present overdispersion, i.e., greater variability than expected by the Poisson and binomial models, respectively. Different extended generalized linear models that allow for overdispersion may be used to analyze this type of data, such as models that use a generalized variance function, random-effects models, zero-inflated models and compound distribution models. Assessing goodness-of-fit and verifying assumptions of these models is not an easy task and the use of half-normal plots with a simulated envelope is a possible solution for this problem. These plots are a useful indicator of goodness-of-fit that may be used with any generalized linear model and extensions. For GLIM users, functions that generated these plots were widely used, however, in the open-source software R, these functions were not yet available on the Comprehensive R Archive Network (CRAN). We describe a new package in R, hnp, that may be used to generate the half-normal plot with a simulated envelope for residuals from different types of models. The function hnp() can be used together with a range of different model fitting packages in R that extend the basic generalized linear model fitting in glm() and is written so that it is relatively easy to extend it to new model classes and different diagnostics. We illustrate its use on a range of examples, including continuous and discrete responses, and show how it can be used to inform model selection and diagnose overdispersion. © 2017, American Statistical Association. All rights reserved."
"Banova: An R package for hierarchical Bayesian ANOVA",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034055672&doi=10.18637%2fjss.v081.i09&partnerID=40&md5=787baee502ad75c77fc5243015faf0bc","In this paper, we develop generalized hierarchical Bayesian ANOVA, to assist experimentalresearchers in the behavioral and social sciences in the analysis of experiments with within- and between-subjects factors. The method alleviates several limitations of classical ANOVA, still commonly employed in those fields of research. An accompanying R Package for BANOVA is developed. It offers statistical routines and several easy-to-use functions for estimation of hierarchical Bayesian ANOVA models that are tailored to the analysis of experimental research. MCMC simulation is used to simulate posterior samples of the parameters of each model specified by the user. The core program is written in R and JAGS. After preparing the data in the required format, users simply select an appropriate model, and can estimate it without any advanced coding being required. The main aim of the R package is to offer freely accessible resources for hierarchical Bayesian ANOVA analysis, which makes it easy to use for applied researchers. © 2017, American Statistical Association. All rights reserved."
"Local likelihood estimation for covariance functions with spatially-varying parameters: The convoSPAT package for R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034038944&doi=10.18637%2fjss.v081.i14&partnerID=40&md5=58b597cca2a488eff3ec26c5b14e774a","In spite of the interest in and appeal of convolution-based approaches for nonstationary spatial modeling, off-the-shelf software for model fitting does not as of yet exist. Convolution-based models are highly flexible yet notoriously difficult to fit, even with relatively small data sets. The general lack of pre-packaged options for model fitting makes it difficult to compare new methodology in nonstationary modeling with other existing methods, and as a result most new models are simply compared to stationary models. Using a convolution-based approach, we present a new nonstationary covariance function for spatial Gaussian process models that allows for efficient computing in two ways: first, by representing the spatially-varying parameters via a discrete mixture or “mixture component” model, and second, by estimating the mixture component parameters through a local likelihood approach. In order to make computations for a convolutionbased nonstationary spatial model readily available, this paper also presents and describes the convoSPAT package for R. The nonstationary model is fit to both a synthetic data set and a real data application involving annual precipitation to demonstrate the capabilities of the package. © 2017, American Statistical Association. All rights reserved."
"Onearmphasetwostudy: An R package for planning, conducting, and analysing single-arm phase II studies",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034037585&doi=10.18637%2fjss.v081.i08&partnerID=40&md5=d758a138a3674b3c8cc125f1e98b8a00","In clinical phase II studies, the efficacy of a promising therapy is tested in patients for the first time. Based on the results, it is decided whether the development programme should be stopped or whether the benefit-risk profile is promising enough to justify the initiation of large phase III studies. In oncology, phase II trials are commonly conducted as single-arm trials with planned interim analyses to allow for an early stopping for futility. The specification of an adequate study design that guarantees control of the type I and II error rates is a key task in the planning stage of such a trial. A variety of statistical methods exists which can be used to optimise the planning and analysis of such studies. However, there are currently neither commercial nor non-commercial software tools available that support the practical application of these methods comprehensively. The R package OneArmPhaseTwoStudy was implemented to fill this gap. The package allows determining an adequate study design for the particular situation at hand as well as monitoring the progress of the study and evaluating the results with valid and efficient analyses methods. This article describes the features of the R package and its application. © 2017, American Statistical Association. All rights reserved."
"Stochastic frontier analysis using SFAMB for Ox",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033727854&doi=10.18637%2fjss.v081.i06&partnerID=40&md5=3c31062e0b75826e429f1238a0cdc430","SFAMB is a flexible econometric tool designed for the estimation of stochastic frontier models. Ox is a matrix language used in different modules, with a console version freely available to academic users. This article provides a brief introduction to the field of stochastic frontier analysis, with examples of code (input and output) as well as a technical documentation of member functions. SFAMB provides frontier models for both crosssectional data and panel data (focusing on fixed effects models). Member functions can be extended depending on the needs of the user. © 2017, American Statistical Association. All rights reserved."
"Intsvy: An R package for analyzing international large-scale assessment data",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033705785&doi=10.18637%2fjss.v081.i07&partnerID=40&md5=370305ea4496509f390950e14134fbd0","This paper introduces intsvy, an R package for working with international assessment data (e.g., PISA, TIMSS, PIRLS). The package includes functions for importing data, performing data analysis, and visualizing results. The paper describes the underlying methodology and provides real data examples. Tools for importing data allow useRs to select variables from student, home, school, and teacher survey instruments as well as for specific countries. Data analysis functions take into account the complex sample design (with replicate weights) and rotated test forms (with plausible values of achievement scores) in the calculation of point estimates and standard errors of means, standard deviations, regression coefficients, correlation coefficients, and frequency tables. Visualization tools present data aggregates in standardized graphical form. © 2017, American Statistical Association. All rights reserved."
"Rtadf: Testing for bubbles with EViews",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033698548&doi=10.18637%2fjss.v081.c01&partnerID=40&md5=77fd459a2bebd2503ced882ec7342c7b","This paper presents Rtadf (right-tail augmented Dickey-Fuller), an EViews add-in that facilitates the performance of time series based tests that help detect and date-stamp asset price bubbles. The detection strategy is based on a right-tail variation of the standard augmented Dickey-Fuller (ADF) test where the alternative hypothesis is of a mildly explosive process. Rejection of the null in each of these tests may serve as empirical evidence for an asset price bubble. The add-in implements four types of tests: standard ADF, rolling window ADF, supremum ADF (SADF; Phillips, Wu, and Yu 2011) and generalized SADF (GSADF; Phillips, Shi, and Yu 2015). It calculates the test statistics for each of the above four tests, simulates the corresponding exact finite sample critical values and p values via Monte Carlo methods, under the assumption of Gaussian innovations, and produces a graphical display of the date stamping procedure. © 2017, American Statistical Association. All rights reserved."
"svt: Singular value thresholding in MATLAB",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033679507&doi=10.18637%2fjss.v081.c02&partnerID=40&md5=cd5a446f8e407d0e14e382f0552d810e","Many statistical learning methods such as matrix completion, matrix regression, and multiple response regression estimate a matrix of parameters. The nuclear norm regularization is frequently employed to achieve shrinkage and low rank solutions. To minimize a nuclear norm regularized loss function, a vital and most time-consuming step is singular value thresholding, which seeks the singular values of a large matrix exceeding a threshold and their associated singular vectors. Currently MATLAB lacks a function for singular value thresholding. Its built-in svds function computes the top r singular values/vectors by Lanczos iterative method but is only efficient for sparse matrix input, while aforementioned statistical learning algorithms perform singular value thresholding on dense but structured matrices. To address this issue, we provide a MATLAB wrapper function svt that implements singular value thresholding. It encompasses both top singular value decomposition and thresholding, handles both large sparse matrices and structured matrices, and reduces the computation cost in matrix learning algorithms. © 2017, American Statistical Association. All rights reserved."
"Feature selection with the r package mxm: Discovering statistically equivalent feature subsets",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089684667&doi=10.18637%2fJSS.V080.I07&partnerID=40&md5=6fabd5878247c2cb04f2671c3ec364ad","The statistically equivalent signature (SES) algorithm is a method for feature selection inspired by the principles of constraint-based learning of Bayesian networks. Most of the currently available feature selection methods return only a single subset of features, supposedly the one with the highest predictive power. We argue that in several domains multiple subsets can achieve close to maximal predictive accuracy, and that arbitrarily providing only one has several drawbacks. The SES method attempts to identify multiple, predictive feature subsets whose performances are statistically equivalent. In that respect the SES algorithm subsumes and extends previous feature selection algorithms, like the max-min parent children algorithm. The SES algorithm is implemented in an homonym function included in the R package MXM, standing for mens ex machina, meaning ‘mind from the machine’ in Latin. The MXM implementation of SES handles several data analysis tasks, namely classification, regression and survival analysis. In this paper we present the SES algorithm, its implementation, and provide examples of use of the SES function in R. Furthermore, we analyze three publicly available data sets to illustrate the equivalence of the signatures retrieved by SES and to contrast SES against the state-of-the-art feature selection method LASSO. Our results provide initial evidence that the two methods perform comparably well in terms of predictive accuracy and that multiple, equally predictive signatures are actually present in real world data. © 2017, American Statistical Association. All rights reserved."
"PrevMap: An R package for prevalence mapping",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020841168&doi=10.18637%2fjss.v078.i08&partnerID=40&md5=16885557952591caea1d6708f8212608","In this paper we introduce a new R package, PrevMap, for the analysis of spatially referenced prevalence data, including both classical maximum likelihood and Bayesian approaches to parameter estimation and plug-in or Bayesian prediction. More specifically, the new package implements fitting of geostatistical models for binomial data, based on two distinct approaches. The first approach uses a generalized linear mixed model with logistic link function, binomial error distribution and a Gaussian spatial process as a stochastic component in the linear predictor. A simpler, but approximate, alternative approach consists of fitting a linear Gaussian model to empirical-logit-transformed data. The package also includes implementations of convolution-based low-rank approximations to the Gaussian spatial process to enable computationally efficient analysis of large spatial datasets. We illustrate the use of the package through the analysis of Loa loa prevalence data from Cameroon and Nigeria. We illustrate the use of the low rank approximation using a simulated geostatistical dataset. © 2017, American Statistical Association. All rights reserved."
"Cquad: An R and stata package for conditional maximum likelihood estimation of dynamic binary panel data models",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020812126&doi=10.18637%2fjss.v078.i07&partnerID=40&md5=9762a56eb87a8a5da7f85d73279d7d1f","We illustrate the R package cquad for conditional maximum likelihood estimation of the quadratic exponential (QE) model proposed by Bartolucci and Nigro (2010) for the analysis of binary panel data. The package also allows us to estimate certain modified versions of the QE model, which are based on alternative parametrizations, and it includes a function for the pseudo-conditional likelihood estimation of the dynamic logit model, as proposed by Bartolucci and Nigro (2012). We also illustrate a reduced version of this package that is available in Stata. The use of the main functions of this package is based on examples using labor market data. © 2017, American Statistical Association. All rights reserved."
"MsBp: An R package to perform bayesian nonparametric inference using multiscale bernstein polynomials mixtures",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020788504&doi=10.18637%2fjss.v078.i06&partnerID=40&md5=fc6ed3bab5b2826ba2b2de1b1118c353","msBP is an R package that implements a new method to perform Bayesian multiscale nonparametric inference introduced by Canale and Dunson (2016). The method, based on mixtures of multiscale beta dictionary densities, overcomes the drawbacks of Pólya trees and inherits many of the advantages of Dirichlet process mixture models. The key idea is that an infinitely-deep binary tree is introduced, with a beta dictionary density assigned to each node of the tree. Using a multiscale stick-breaking characterization, stochastically decreasing weights are assigned to each node. The result is an infinite mixture model. The package msBP implements a series of basic functions to deal with this family of priors such as random densities and numbers generation, creation and manipulation of binary tree objects, and generic functions to plot and print the results. In addition, it implements the Gibbs samplers for posterior computation to perform multiscale density estimation and multiscale testing of group differences described in Canale and Dunson (2016). © 2017, American Statistical Association. All rights reserved."
"Somoclu: An efficient parallel library for self-organizing maps",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020788114&doi=10.18637%2fjss.v078.i09&partnerID=40&md5=aaa95bfc5de22e990df9972f598330d7","Somoclu is a massively parallel tool for training self-organizing maps on large data sets written in C++. It builds on OpenMP for multicore execution, and on MPI for distributing the workload across the nodes in a cluster. It is also able to boost training by using CUDA if graphics processing units are available. A sparse kernel is included, which is useful for high-dimensional but sparse data, such as the vector spaces common in text mining workflows. Python, R and MATLAB interfaces facilitate interactive use. Apart from fast execution, memory use is highly optimized, enabling training large emergent maps even on a single computer. © 2017, American Statistical Association. All rights reserved."
"KFAS: Exponential family state space models in R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020772151&doi=10.18637%2fjss.v078.i10&partnerID=40&md5=d03f918ace22c5ae1183fe3a65f73154","State space modeling is an efficient and flexible method for statistical inference of a broad class of time series and other data. This paper describes the R package KFAS for state space modeling with the observations from an exponential family, namely Gaussian, Poisson, binomial, negative binomial and gamma distributions. After introducing the basic theory behind Gaussian and non-Gaussian state space models, an illustrative example of Poisson time series forecasting is provided. Finally, a comparison to alternative R packages suitable for non-Gaussian time series modeling is presented. © 2017, American Statistical Association. All rights reserved."
"Multivariate-From-Univariate MCMC sampler: The R package MfUsampler",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020769332&doi=10.18637%2fjss.v078.c01&partnerID=40&md5=554c59e05a33dc6dfe0d0475dc95d693","The R package MfUSampler provides Markov chain Monte Carlo machinery for generating samples from multivariate probability distributions using univariate sampling algorithms such as the slice sampler and the adaptive rejection sampler. The multivariate wrapper performs a full cycle of univariate sampling steps, one coordinate at a time. In each step, the latest sample values obtained for other coordinates are used to form the conditional distributions. The concept is an extension of Gibbs sampling where each step involves, not an independent sample from the conditional distribution, but a Markov transition for which the conditional distribution is invariant. The software relies on proportionality of conditional distributions to the joint distribution to implement a thin wrapper for producing conditionals. Examples illustrate basic usage as well as methods for improving performance. By encapsulating the multivariate-from-univariate logic, package MfUSampler provides a reliable package for rapid prototyping of custom Bayesian models while allowing for incremental performance optimizations such as taking advantage of conditional independence, and high-performance implementation of function evaluations. Utility functions for MCMC diagnostics as well as sample-based construction of predictive posterior distributions are provided in MfUSampler. © 2017, American Statistical Association. All rights reserved."
"R package gdistance: Distances and routes on geographical grids",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015250895&doi=10.18637%2fjss.v076.i13&partnerID=40&md5=a2b0b8140889aab0880a2acaa0ddc32d","The R package gdistance provides classes and functions to calculate various distance measures and routes in heterogeneous geographic spaces represented as grids. Least-cost distances as well as more complex distances based on (constrained) random walks can be calculated. Also the corresponding routes or probabilities of passing each cell can be determined. The package implements classes to store the data about the probability or cost of transitioning from one cell to another on a grid in a memory-efficient sparse format. These classes make it possible to manipulate the values of cell-to-cell movement directly, which offers flexibility and the possibility to use asymmetric values. The novel distances implemented in the package are used in geographical genetics (applying circuit theory), but also have applications in other fields of geospatial analysis. © 2017, American Statistical Association. All rights reserved."
"Introduction to stream: An extensible framework for data stream clustering research with R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015218025&doi=10.18637%2fjss.v076.i14&partnerID=40&md5=49ff9f903a5af21a404ba78fd3de1df3","In recent years, data streams have become an increasingly important area of research for the computer science, database and statistics communities. Data streams are ordered and potentially unbounded sequences of data points created by a typically non-stationary data generating process. Common data mining tasks associated with data streams include clustering, classification and frequent pattern mining. New algorithms for these types of data are proposed regularly and it is important to evaluate them thoroughly under standardized conditions. In this paper we introduce stream, a research tool that includes modeling and simulating data streams as well as an extensible framework for implementing, interfacing and experimenting with algorithms for various data stream mining tasks. The main advantage of stream is that it seamlessly integrates with the large existing infrastructure provided by R. In addition to data handling, plotting and easy scripting capabilities, R also provides many existing algorithms and enables users to interface code written in many programming languages popular among data mining researchers (e.g., C/C++, Java and Python). In this paper we describe the architecture of stream and focus on its use for data stream clustering research. stream was implemented with extensibility in mind and will be extended in the future to cover additional data stream mining tasks like classification and frequent pattern mining. © 2017, American Statistical Association. All rights reserved."
"Medflex: An R package for flexible mediation analysis using natural effect models",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015184230&doi=10.18637%2fjss.v076.i11&partnerID=40&md5=0b2f3da049ec8ebb475c80c1be7af2d6","Mediation analysis is routinely adopted by researchers from a wide range of applied disciplines as a statistical tool to disentangle the causal pathways by which an exposure or treatment affects an outcome. The counterfactual framework provides a language for clearly defining path-specific effects of interest and has fostered a principled extension of mediation analysis beyond the context of linear models. This paper describes medflex, an R package that implements some recent developments in mediation analysis embedded within the counterfactual framework. The medflex package offers a set of ready-made functions for fitting natural effect models, a novel class of causal models which directly parameterize the path-specific effects of interest, thereby adding flexibility to existing software packages for mediation analysis, in particular with respect to hypothesis testing and parsimony. In this paper, we give a comprehensive overview of the functionalities of the medflex package. © 2017, American Statistical Association. All rights reserved."
"Identifying causal effects with the R package causaleffect",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015155329&doi=10.18637%2fjss.v076.i12&partnerID=40&md5=32a0be6a604757581906bdb739cf230f","Do-calculus is concerned with estimating the interventional distribution of an action from the observed joint probability distribution of the variables in a given causal structure. All identifiable causal effects can be derived using the rules of do-calculus, but the rules themselves do not give any direct indication whether the effect in question is identifiable or not. Shpitser and Pearl (2006b) constructed an algorithm for identifying joint interventional distributions in causal models, which contain unobserved variables and induce directed acyclic graphs. This algorithm can be seen as a repeated application of the rules of do-calculus and known properties of probabilities, and it ultimately either derives an expression for the causal distribution, or fails to identify the effect, in which case the effect is non-identifiable. In this paper, the R package causaleffect is presented, which provides an implementation of this algorithm. Functionality of causaleffect is also demonstrated through examples. © 2017, American Statistical Association. All rights reserved."
"Interactive dendrograms: The R packages idendro and idendr0",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015154760&doi=10.18637%2fjss.v076.i10&partnerID=40&md5=f1e71da4d99d1f328997f162d6e6f581","Hierarchical cluster analysis is a valuable tool for exploring data by describing their structure using a dendrogram. However, proper visualization and interactive inspection of the dendrogram are needed to unlock the information in the data. We describe a new R package, idendro, that enables the user to inspect dendrograms interactively: to select and color clusters, to zoom and pan the dendrogram, and to visualize the clustered data not only in a built-in heat map, but also in any interactive plot implemented in the cranvas package. A lightweight version idendr0 with reduced dependencies is also available from the Comprehensive R Archive Network. © 2017, American Statistical Association. All rights reserved."
"Npregfast: An R package for nonparametric estimation and inference in life sciences",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046796296&doi=10.18637%2fjss.v082.i12&partnerID=40&md5=e046f271c295ecf23a63f0cec8190b17","We present the R npregfast package via some applications involved with the study of living organisms. The package implements nonparametric estimation procedures in regression models with or without factor-by-curve interactions. The main feature of the package is its ability to perform inference regarding these models. Namely, the implementation of different procedures to test features of the estimated regression curves: on the one hand, the comparisons between curves which may vary across groups defined by levels of a categorical variable or factor; on the other hand, the comparisons of some critical points of the curve (e.g., maxima, minima or inflection points), studying for this purpose the derivatives of the curve. © 2017, American Statistical Association. All rights reserved."
"ThresholdROC: Optimum threshold estimation tools for continuous diagnostic tests in R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037378649&doi=10.18637%2fjss.v082.i04&partnerID=40&md5=12af6db6bb15e5a14d4c82f73a473e93","We introduce an R package that estimates decision thresholds in diagnostic settings with a continuous marker and two or three underlying states. The package implements parametric and non-parametric estimation methods based on minimizing an overall cost function, as well as confidence interval estimation approaches to account for the sampling variability of the cut-off. Further features of the package include sample size determination and estimation of diagnostic accuracy measures. We used randomly generated data and two real datasets to illustrate the capabilities and characteristics of the package. © 2017, American Statistical Association. All rights reserved."
"Enhancing reproducibility and collaboration via management of R package cohorts",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037377516&doi=10.18637%2fjss.v082.i01&partnerID=40&md5=d3acf0f2c9b6dc53502b72f0ef600693","Science depends on collaboration, result reproduction, and the development of supporting software tools. Each of these requires careful management of software versions. We present a unified model for installing, managing, and publishing software contexts in R. It introduces the package manifest as a central data structure for representing version-specific, decentralized package cohorts. The manifest points to package sources on arbitrary hosts and in various forms, including tarballs and directories under version control. We provide a high-level interface for creating and switching between side-by-side package libraries derived from manifests. Finally, we extend package installation to support the retrieval of exact package versions as indicated by manifests, and to maintain provenance for installed packages. The provenance information enables the user to publish libraries or sessions as manifests, hence completing the loop between publication and deployment. We have implemented this model across three software packages, switchr, switchrGist and GRANBase, and have released the source code under the Artistic 2.0 license. © 2017, American Statistical Association. All rights reserved."
"Robust standard error estimators for panel models: A unifying approach",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037376720&doi=10.18637%2fjss.v082.i03&partnerID=40&md5=67f4df43b89f1df85be29eec97b2b872","The different robust estimators for the standard errors of panel models used in applied econometric practice can all be written and computed as combinations of the same simple building blocks. A framework based on high-level wrapper functions for most common usage and basic computational elements to be combined at will, coupling user-friendliness with flexibility, is integrated in the plm package for panel data econometrics in R. Statistical motivation and computational approach are reviewed, and applied examples are provided. © 2017, American Statistical Association. All rights reserved."
"CopulaDTA: An R package for copula-based bivariate beta-binomial models for diagnostic test accuracy studies in a Bayesian framework",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037374753&doi=10.18637%2fjss.v082.c01&partnerID=40&md5=dcdc8c2f4758b45b5c399925439d79c5","The current statistical procedures implemented in statistical software packages for pooling of diagnostic test accuracy data include HSROC regression (Rutter and Gatsonis 2001) and the bivariate random-effects meta-analysis model (BRMA; Reitsma et al. 2005; Arends et al. 2008; Chu and Cole 2006; Riley et al. 2007b). However, these models do not report the overall mean but rather the mean for a central study with random-effect equal to zero and have difficulties estimating the correlation between sensitivity and specificity when the number of studies in the meta-analysis is small and/or when the between-study variance is relatively large (Riley et al. 2007a). This tutorial on advanced statistical methods for meta-analysis of diagnostic accuracy studies discusses and demonstrates Bayesian modeling using the R package CopulaDTA (Nyaga 2017) to fit different models to obtain the meta-analytic parameter estimates. The focus is on the joint modeling of sensitivity and specificity using a copula based bivariate beta distribution. Essentially, we extend the work of Nikoloulopoulos (2015) by: (i) presenting the Bayesian approach which offers the flexibility and ability to perform complex statistical modeling even with small data sets and (ii) including covariate information, and (iii) providing an easy to use code. The statistical methods are illustrated by re-analyzing data of two published meta-analyses. Modeling sensitivity and specificity using the bivariate beta distribution provides marginal as well as study-specific parameter estimates as opposed to using the bivariate normal distribution (e.g., in BRMA) which only yields study-specific parameter estimates. Moreover, copula based models offer greater flexibility in modeling different correlation structures in contrast to the normal distribution which allows for only one correlation structure. © 2017, American Statistical Association. All rights reserved."
"Tscount: An R package for analysis of count time series following generalized linear models",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037371169&doi=10.18637%2fjss.v082.i05&partnerID=40&md5=22f266b6e3f15493862d8106ce092a55","The R package tscount provides likelihood-based estimation methods for analysis and modeling of count time series following generalized linear models. This is a flexible class of models which can describe serial correlation in a parsimonious way. The conditional mean of the process is linked to its past values, to past observations and to potential covariate effects. The package allows for models with the identity and with the logarithmic link function. The conditional distribution can be Poisson or negative binomial. An important special case of this class is the so-called INGARCH model and its log-linear extension. The package includes methods for model fitting and assessment, prediction and intervention analysis. This paper summarizes the theoretical background of these methods. It gives details on the implementation of the package and provides simulation results for models which have not been studied theoretically before. The usage of the package is illustrated by two data examples. Additionally, we provide a review of R packages which can be used for count time series analysis. This includes a detailed comparison of tscount to those packages. © 2017, American Statistical Association. All rights reserved."
"Jmcm: An R package for joint mean-covariance modeling of longitudinal data",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037365859&doi=10.18637%2fjss.v082.i09&partnerID=40&md5=dd959f932aa4f9f02a19aa0b0468c42a","Longitudinal studies commonly arise in various fields such as psychology, social science, economics and medical research, etc. It is of great importance to understand the dynamics in the mean function, covariance and/or correlation matrices of repeated measurements. However, high-dimensionality (HD) and positive-definiteness (PD) constraints are two major stumbling blocks in modeling of covariance and correlation matrices. It is evident that Cholesky-type decomposition based methods are effective in dealing with HD and PD problems, but those methods were not implemented in statistical software yet, causing a difficulty for practitioners to use. In this paper, we first introduce recently developed Cholesky decomposition based methods for joint modeling of mean and covariance structures, namely modified Cholesky decomposition (MCD), alternative Cholesky decomposition (ACD) and hyperspherical parameterization of Cholesky factor (HPC). We then introduce our newly developed R package jmcm which is currently able to handle longitudinal data that follows a Gaussian distribution using the MCD, ACD and HPC methods. The use of package jmcm is illustrated and a comparison of those methods is made through the analysis of two real datasets. © 2017, American Statistical Association. All rights reserved."
"A recipe for inferference: Start with causal inference. Add interference. Mix well with R.",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037364857&doi=10.18637%2fjss.v082.i02&partnerID=40&md5=50891498e750ca9314bfd84404d48b75","In causal inference, interference occurs when the treatment of one subject affects the outcome of other subjects. Interference can distort research conclusions about causal effects when not accounted for properly. In the absence of interference, inverse probability weighted (IPW) estimators are commonly used to estimate causal effects from observational data. Recently, IPW estimators have been extended to handle interference. Tchetgen Tchetgen and VanderWeele (2012) proposed IPW methods to estimate direct and indirect (or spillover) effects that allow for interference between individuals within groups. In this paper, we present inferference, an R package that computes these IPW causal effect estimates when interference may be present within groups. We illustrate use of the package with examples from political science and infectious disease. © 2017, American Statistical Association. All rights reserved."
"SparseHessianFD: Estimating sparse Hessian matrices in R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037361575&doi=10.18637%2fjss.v082.i10&partnerID=40&md5=e70774cb2eb96a6275be9a8bc2bb2c6a","Sparse Hessian matrices occur often in statistics, and their fast and accurate estimation can improve efficiency of numerical optimization and sampling algorithms. By exploiting the known sparsity pattern of a Hessian, methods in the sparseHessianFD package require many fewer function or gradient evaluations than would be required if the Hessian were treated as dense. The package implements established graph coloring and linear substitution algorithms that were previously unavailable to R users, and is most useful when other numerical, symbolic or algorithmic methods are impractical, inefficient or unavailable. © 2017, American Statistical Association. All rights reserved."
"TrackeR: Infrastructure for running and cycling data from GPS-enabled tracking devices in R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037354425&doi=10.18637%2fjss.v082.i07&partnerID=40&md5=10aa659cfde44f3606210bdd7eb0ee47","The use of GPS-enabled tracking devices and heart rate monitors is becoming increasingly common in sports and fitness activities. The trackeR package aims to fill the gap between the routine collection of data from such devices and their analyses in R. The package provides methods to import tracking data into data structures which preserve units of measurement and are organized in sessions. The package implements core infrastructure for relevant summaries and visualizations, as well as support for handling units of measurement. There are also methods for relevant analytic tools such as time spent in zones, work capacity above critical power (known as W'), and distribution and concentration profiles. A case study illustrates how the latter can be used to summarize the information from training sessions and use it in more advanced statistical analyses. © 2017, American Statistical Association. All rights reserved."
"VdmR: Generating web-based visual data mining tools with R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037336437&doi=10.18637%2fjss.v082.i06&partnerID=40&md5=b1e7b2238e06bb2148331e5e29cc274d","The vdmR package generates web-based visual data mining tools by adding interactive functions to ggplot2 graphics. Brushing and linking between multiple plots is one of the main features of this package. Currently, scatter plots, histograms, parallel coordinate plots, and choropleth maps are supported in the vdmR package. In addition, identification on a plot is supported by linking the plot and the data table. © 2017, American Statistical Association. All rights reserved."
"REBayes: Empirical bayes mixture methods in R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037332804&doi=10.18637%2fjss.v082.i08&partnerID=40&md5=faf153916fb9e29d918927df7bb599d8","Models of unobserved heterogeneity, or frailty as it is commonly known in survival analysis, can often be formulated as semiparametric mixture models and estimated by maximum likelihood as proposed by Robbins (1950) and elaborated by Kiefer and Wolfowitz (1956). Recent developments in convex optimization, as noted by Koenker and Mizera (2014b), have led to dramatic improvements in computational methods for such models. In this vignette we describe an implementation contained in the R package RE-Bayes with applications to a wide variety of mixture settings: Gaussian location and scale, Poisson and binomial mixtures for discrete data, Weibull and Gompertz models for survival data, and several Gaussian models intended for longitudinal data. While the dimension of the nonparametric heterogeneity of these models is inherently limited by our present gridding strategy, we describe how additional fixed parameters can be relatively easily accommodated via profile likelihood. We also describe some nonparametric maximum likelihood methods for shape and norm constrained density estimation that employ related computational methods. © 2017, American Statistical Association. All rights reserved."
"Lmest: An R package for latent Markov models for longitudinal categorical data",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032392649&doi=10.18637%2fjss.v081.i04&partnerID=40&md5=cc1f51fe6d905f69affdf0909860e113","Latent Markov (LM) models represent an important class of models for the analysis of longitudinal data, especially when response variables are categorical. These models have a great potential of application in many fields, such as economics and medicine. We illustrate the R package LMest that is tailored to deal with the basic LM model and some extended formulations accounting for individual covariates and for the presence of unobserved clusters of units having the same initial and transition probabilities (mixed LM model). The main functions of the package are tailored to parameter estimation through the expectation-maximization algorithm, which is based on suitable forwardbackward recursions. The package also permits local and global decoding and to obtain standard errors for the parameter estimates. We illustrate the use of the package and its main features through some empirical examples in the fields of labour market, health, and criminology. © 2017, American Statistical Association. All Rights Reserved."
"Conditional visualization for statistical models: An introduction to the condvis package in R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032351702&doi=10.18637%2fjss.v081.i05&partnerID=40&md5=5122360134a5f75148b07b10d01d0d43","The condvis package is for interactive visualization of sections in data space, showing fitted models on the section, and observed data near the section. The primary goal is the interpretation of complex models, and showing how the observed data support the fitted model.There is a video accompaniment to this paper available at https: //www.youtube.com/watch?v=rKFq7xwgdX0. © 2017, American Statistical Association. All Rights Reserved."
"Itmle: An R package implementing targeted minimum loss-based estimation for longitudinal data",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032345686&doi=10.18637%2fjss.v081.i01&partnerID=40&md5=b444f4bd1410c15091fbbc56015ebf9f","In recent years, targeted minimum loss-based estimation methodology has been used to develop estimators of parameters in longitudinal data structures (Gruber and van der Laan 2012; Petersen, Schwab, Gruber, Blaser, Schomaker, and van der Laan 2014; Schnitzer, Moodie, van der Laan, Platt, and Klein 2013). These methods are implemented in the ltmle package for R. The ltmle package provides methods to estimate intervention-specific means and measures of association including the average treatment effect, causal odds ratio and causal risk ratio and parameters of a longitudinal working marginal structural model. The package allows for multiple time point treatments, time-varying covariates and right censoring of the outcome. In this paper we described the usage of the ltmle package and provide examples. © 2017, American Statistical Association. All Rights Reserved."
"Tutorial in joint modeling and prediction: A statistical software for correlated longitudinal outcomes, recurrent events and a terminal event",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032335932&doi=10.18637%2fjss.v081.i03&partnerID=40&md5=c95b026e588911cb08f0df3c7c4e4083","Extensions in the field of joint modeling of correlated data and dynamic predictions improve the development of prognosis research. The R package frailtypack provides estimations of various joint models for longitudinal data and survival events. In particular, it fits models for recurrent events and a terminal event (frailtyPenal), models for two survival outcomes for clustered data (frailtyPenal), models for two types of recurrent events and a terminal event (multivPenal), models for a longitudinal biomarker and a terminal event (longiPenal) and models for a longitudinal biomarker, recurrent events and a terminal event (trivPenal). The estimators are obtained using a standard and penalized maximum likelihood approach, each model function allows to evaluate goodness-of-fit analyses and provides plots of baseline hazard functions. Finally, the package provides individual dynamic predictions of the terminal event and evaluation of predictive accuracy. This paper presents the theoretical models with estimation techniques, applies the methods for predictions and illustrates frailtypack functions details with examples. © 2017, American Statistical Association. All Rights Reserved."
"Simcausal R package: Conducting transparent and reproducible simulation studies of causal effect estimation with complex longitudinal data",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032329594&doi=10.18637%2fjss.v081.i02&partnerID=40&md5=22c25ee0e258f7c9464120ea3738ae26","The simcausal R package is a tool for specification and simulation of complex longitudinal data structures that are based on non-parametric structural equation models. The package aims to provide a flexible tool for simplifying the conduct of transparent and reproducible simulation studies, with a particular emphasis on the types of data and interventions frequently encountered in real-world causal inference problems, such as, observational data with time-dependent confounding, selection bias, and random monitoring processes. The package interface allows for concise expression of complex functional dependencies between a large number of nodes, where each node may represent a measurement at a specific time point. The package allows for specification and simulation of counterfactual data under various user-specified interventions (e.g., static, dynamic, deterministic, or stochastic). In particular, the interventions may represent exposures to treatment regimens, the occurrence or non-occurrence of right-censoring events, or of clinical monitoring events. Finally, the package enables the computation of a selected set of user-specified features of the distribution of the counterfactual data that represent common causal quantities of interest, such as, treatment-specific means, the average treatment effects and coefficients from working marginal structural models. The applicability of simcausal is demonstrated by replicating the results of two published simulation studies. © 2017, American Statistical Association. All Rights Reserved."
"Parameter estimation in nonlinear mixed effect models using saemix, an R implementation of the SAEM algorithm",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028565107&doi=10.18637%2fjss.v080.i03&partnerID=40&md5=c3a0e431d8306c15c27eaade15f98f24","The saemix package for R provides maximum likelihood estimates of parameters in nonlinear mixed effect models, using a modern and efficient estimation algorithm, the stochastic approximation expectation maximisation (SAEM) algorithm. In the present paper we describe the main features of the package, and apply it to several examples to illustrate its use. Making use of S4 classes and methods to provide user-friendly interaction, this package provides a new estimation tool to the R community. © 2017, American Statistical Association. All rights reserved."
"brms: An R package for Bayesian multilevel models using Stan",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028555867&doi=10.18637%2fjss.v080.i01&partnerID=40&md5=36fbaa19c2a6ecb381607300a08bf308","The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit – among others – linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation. © 2017, American Statistical Association. All rights reserved."
"Dynamic treatment regimen estimation via regression-based techniques: Introducing R package reg",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028548554&doi=10.18637%2fjss.v080.i02&partnerID=40&md5=00953e3af1bd4662d34737ca69a5c062","Personalized medicine, whereby treatments are tailored to a specific patient rather than a general disease or condition, is an area of growing interest in the fields of biostatistics, epidemiology, and beyond. Dynamic treatment regimens (DTRs) are an integral part of this framework, allowing for personalized treatment of patients with long-term conditions while accounting for both their present circumstances and medical history. The identification of the optimal DTR in any given context, however, is a non-trivial problem, and so specialized methodologies have been developed for that purpose. Here we introduce the R package DTRreg which implements two regression-based approaches: G-estimation and dynamic weighted ordinary least squares regression. We outline the theory underlying these methods, discuss the implementation of DTRreg and demonstrate its use with hypothetical and real-world inspired simulated datasets. © 2017, American Statistical Association. All rights reserved."
"Performing arm-based network meta-analysis in R with the pcnetmeta package",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028540408&doi=10.18637%2fjss.v080.i05&partnerID=40&md5=5da42279133b9f4773e92e8f4c6df740","Network meta-analysis is a powerful approach for synthesizing direct and indirect evidence about multiple treatment comparisons from a collection of independent studies. At present, the most widely used method in network meta-analysis is contrast-based, in which a baseline treatment needs to be specified in each study, and the analysis focuses on modeling relative treatment effects (typically log odds ratios). However, populationaveraged treatment-specific parameters, such as absolute risks, cannot be estimated by this method without an external data source or a separate model for a reference treatment. Recently, an arm-based network meta-analysis method has been proposed, and the R package pcnetmeta provides user-friendly functions for its implementation. This package estimates both absolute and relative effects, and can handle binary, continuous, and count outcomes. © 2017, American Statistical Association. All rights reserved."
"COGARCH(p, q): Simulation and inference with the yuima package",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028528611&doi=10.18637%2fjss.v080.i04&partnerID=40&md5=2feb81a17eb83a92ec758931ab13423d","In this paper we show how to simulate and estimate a COGARCH(p, q) model in the R package yuima. Several routines for simulation and estimation are introduced. In particular, for the generation of a COGARCH(p, q) trajectory, the user can choose between two alternative schemes. The first is based on the Euler discretization of the stochastic differential equations that identify a COGARCH(p, q) model while the second considers the explicit solution of the equations defining the variance process. Estimation is based on the matching of the empirical with the theoretical autocorrelation function. Three different approaches are implemented: minimization of the mean squared error, minimization of the absolute mean error and the generalized method of moments where the weighting matrix is continuously updated. Numerical examples are given in order to explain methods and classes used in the yuima package. © 2017, American Statistical Association. All rights reserved."
"Coefficient-wise tree-based varying coefficient regression with vcrpart",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028522689&doi=10.18637%2fjss.v080.i06&partnerID=40&md5=92ab47434f39a5b2405076ac84b4738e","The tree-based TVCM algorithm and its implementation in the R package vcrpart are introduced for generalized linear models. The purpose of TVCM is to learn whether and how the coefficients of a regression model vary by moderating variables. A separate partition is built for each potentially varying coefficient, allowing the user to specify coefficient-specific sets of potential moderators, and allowing the algorithm to select moderators individually by coefficient. In addition to describing the algorithm, the TVCM is evaluated using a benchmark comparison and a simulation study and the R commands are demonstrated by means of empirical applications. © 2017, American Statistical Association. All rights reserved."
"DPB: Dynamic panel binary data models in gretl",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027280108&doi=10.18637%2fjss.v079.i08&partnerID=40&md5=956555244aa1abd8f3ddc4dddb5a61d5","This paper presents the gretl function package DPB for estimating dynamic binary models with panel data. The package contains routines for the estimation of the randomeffects dynamic probit model proposed by Heckman (1981b) and its generalisation by Hyslop (1999) and Keane and Sauer (2009) to accommodate AR(1) disturbances. The fixed-effects estimator by Bartolucci and Nigro (2010) is also implemented. DPB is available on the gretl function packages archive. © 2017, American Statistical Association. All rights reserved."
"npbr: A package for nonparametric boundary regression in R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027256620&doi=10.18637%2fjss.v079.i09&partnerID=40&md5=3580025643021964e1fa3286da716b3d","The package npbr is the first free specialized software for data edge and frontier analysis in the statistical literature. It provides a variety of functions for the best known and most innovative approaches to nonparametric boundary estimation. The selected methods are concerned with empirical, smoothed, unrestricted as well as constrained fits under both single and multiple shape constraints. They also cover data envelopment techniques as well as robust approaches to outliers. The routines included in npbr are user friendly and afford a large degree of flexibility in the estimation specifications. They provide smoothing parameter selection for the modern local linear and polynomial spline methods as well as for some promising extreme value techniques. Also, they seamlessly allow for Monte Carlo comparisons among the implemented estimation procedures. This package will be very useful for statisticians and applied researchers interested in employing nonparametric boundary regression models. Its use is illustrated with a number of empirical applications and simulated examples. © 2017, American Statistical Association. All rights reserved."
"Plotroc: A tool for plotting ROC curves",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027254932&doi=10.18637%2fjss.v079.c02&partnerID=40&md5=46296b539d80cbdd0eedcc4bb9ca8613","Plots of the receiver operating characteristic (ROC) curve are ubiquitous in medical research. Designed to simultaneously display the operating characteristics at every possible value of a continuous diagnostic test, ROC curves are used in oncology to evaluate screening, diagnostic, prognostic and predictive biomarkers. I reviewed a sample of ROC curve plots from the major oncology journals in order to assess current trends in usage and design elements. My review suggests that ROC curve plots are often ineffective as statistical charts and that poor design obscures the relevant information the chart is intended to display. I describe my new R package that was created to address the shortcomings of existing tools. The package has functions to create informative ROC curve plots, with sensible defaults and a simple interface, for use in print or as an interactive web-based plot. A web application was developed to reach a broader audience of scientists who do not use R. © 2017, American Statistical Association. All rights reserved."
"Simulation of synthetic complex data: The R package simPop",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027250920&doi=10.18637%2fjss.v079.i10&partnerID=40&md5=9d67ab53c265d1dae70eb78fdd8728ed","The production of synthetic datasets has been proposed as a statistical disclosure control solution to generate public use files out of protected data, and as a tool to create “augmented datasets” to serve as input for micro-simulation models. Synthetic data have become an important instrument for ex-ante assessments of policy impact. The performance and acceptability of such a tool relies heavily on the quality of the synthetic populations, i.e., on the statistical similarity between the synthetic and the true population of interest. Multiple approaches and tools have been developed to generate synthetic data. These approaches can be categorized into three main groups: synthetic reconstruction, combinatorial optimization, and model-based generation. We provide in this paper a brief overview of these approaches, and introduce simPop, an open source data synthesizer. simPop is a user-friendly R package based on a modular object-oriented concept. It provides a highly optimized S4 class implementation of various methods, including calibration by iterative proportional fitting and simulated annealing, and modeling or data fusion by logistic regression. We demonstrate the use of simPop by creating a synthetic population of Austria, and report on the utility of the resulting data. We conclude with suggestions for further development of the package. © 2017, American Statistical Association. All rights reserved."
"R package ASMap: Efficient genetic linkage map construction and diagnosis",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025167991&doi=10.18637%2fjss.v079.i06&partnerID=40&md5=ab6a96cf89ceaceff0561d25c4b27606","Although various forms of linkage map construction software are widely available, there is a distinct lack of packages for use in the R statistical computing environment (R Core Team 2017). This article introduces the ASMap linkage map construction R package which contains functions that use the efficient MSTmap algorithm (Wu, Bhat, Close, and Lonardi 2008) for clustering and optimally ordering large sets of markers. Additional to the construction functions, the package also contains a suite of tools to assist in the rapid diagnosis and repair of a constructed linkage map. The package functions can also be used for post linkage map construction techniques such as fine mapping or combining maps of the same population. To showcase the efficiency and functionality of ASMap, the complete linkage map construction process is demonstrated with a high density barley backcross marker data set. © 2017, American Statistical Association. All rights reserved."
"The R package MitISEM: Efficient and robust simulation procedures for Bayesian inference",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025167450&doi=10.18637%2fjss.v079.i01&partnerID=40&md5=5ad4668f6e9f0e97f4e6c7a4df9ae50c","This paper presents the R package MitISEM (mixture of t by importance sampling weighted expectation maximization) which provides an automatic and flexible two-stage method to approximate a non-elliptical target density kernel – typically a posterior density kernel – using an adaptive mixture of Student t densities as approximating density. In the first stage a mixture of Student t densities is fitted to the target using an expectation maximization algorithm where each step of the optimization procedure is weighted using importance sampling. In the second stage this mixture density is a candidate density for efficient and robust application of importance sampling or the Metropolis-Hastings (MH) method to estimate properties of the target distribution. The package enables Bayesian inference and prediction on model parameters and probabilities, in particular, for models where densities have multi-modal or other non-elliptical shapes like curved ridges. These shapes occur in research topics in several scientific fields. For instance, analysis of DNA data in bio-informatics, obtaining loans in the banking sector by heterogeneous groups in financial economics and analysis of education’s effect on earned income in labor economics. The package MitISEM provides also an extended algorithm, ‘sequential Mi-tISEM’, which substantially decreases computation time when the target density has to be approximated for increasing data samples. This occurs when the posterior or predictive density is updated with new observations and/or when one computes model probabilities using predictive likelihoods. We illustrate the MitISEM algorithm using three canonical statistical and econometric models that are characterized by several types of non-elliptical posterior shapes and that describe well-known data patterns in econometrics and finance. We show that MH using the candidate density obtained by MitISEM outperforms, in terms of numerical efficiency, MH using a simpler candidate, as well as the Gibbs sampler. The MitISEM approach is also used for Bayesian model comparison using predictive likelihoods. © 2017, American Statistical Association. All rights reserved."
"Locally adaptive tree-based thresholding using the treethresh package in R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025165547&doi=10.18637%2fjss.v078.c02&partnerID=40&md5=0c90b4c037af28d3a31973a959c6b874","This paper introduces the treethresh package offering accurate estimation, via thresholding, of potentially sparse heterogeneous signals and the denoising of images using wavelets. It gives considerably improved performance over other estimation methods if the underlying signal or image is not homogeneous throughout but instead has distinct regions with differing sparsity or strength characteristics. It aims to identify these different regions and perform separate estimation in each accordingly. The base algorithm offers code which can be applied directly to any one-dimensional potentially sparse sequence observed subject to noise. Also included are functions which allow two-dimensional images to be denoised following transformation to the wavelet domain. In addition to reconstructing the underlying signal or image, the package provides information on the believed partitioning of the signal or image into its differing regions. © 2017, American Statistical Association. All rights reserved."
"The cgeostat software for analyzing complex-valued random fields",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025163057&doi=10.18637%2fjss.v079.i05&partnerID=40&md5=839ca0f5b9f9cd75d9da25a1dba928d9","Given a vectorial data set in two dimensions, a representation on a complex domain is often convenient. This representation is rarely considered in geostatistics, although interesting applications can be found in environmental sciences and meteorology (e.g., for wind fields). In such a case, some computational difficulties are related to the lack of software for estimating and modeling a complex covariance function, for predicting complex variables as well as for representing the output results. In this paper, the new Fortran software cgeostat for geostatistical analysis of complex-valued random fields is presented and an application is demonstrated. © 2017, American Statistical Association. All rights reserved."
"Kernel-based regularized least squares in R (KRLS) and Stata (KRLS)",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025161582&doi=10.18637%2fjss.v079.i03&partnerID=40&md5=1dea23dd591e654be68817f5925fc99b","The Stata package krls as well as the R package KRLS implement kernel-based regularized least squares (KRLS), a machine learning method described in Hainmueller and Hazlett (2014) that allows users to tackle regression and classification problems without strong functional form assumptions or a specification search. The flexible KRLS estimator learns the functional form from the data, thereby protecting inferences against misspecification bias. Yet it nevertheless allows for interpretability and inference in ways similar to ordinary regression models. In particular, KRLS provides closed-form estimates for the predicted values, variances, and the pointwise partial derivatives that characterize the marginal effects of each independent variable at each data point in the covariate space. The method is thus a convenient and powerful alternative to ordinary least squares and other generalized linear models for regression-based analyses. © 2017, American Statistical Association. All rights reserved."
"GFD: An R package for the analysis of general factorial designs",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025134732&doi=10.18637%2fjss.v079.c01&partnerID=40&md5=98cfc55b8c225c46abf3f8ef59254820","Factorial designs are widely used tools for modeling statistical experiments in all kinds of disciplines, e.g., biology, psychology, econometrics and medicine. For testing null hypotheses in this framework, ANOVA methods are widely used. However, the corresponding F tests are only valid for normally distributed data with equal variances, two assumptions which are often not met in practice. The R package GFD provides an implementation of the Wald-type statistic (WTS), the ANOVA-type statistic (ATS) and a studentized permutation version of the WTS. Both the WTS and the permuted WTS do not require normally distributed data or variance homogeneity, whereas the ATS assumes normality. All methods are available for general crossed or nested designs and all main and interaction effects can be plotted. Additionally, the package is equipped with an optional graphical user interface to facilitate application for a wide range of users. We illustrate the implemented methods for a range of different designs. © 2017, American Statistical Association. All rights reserved."
"SmoothHazard: An R package for fitting regression models to interval-censored observations of illness-death models",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025130857&doi=10.18637%2fjss.v079.i07&partnerID=40&md5=51aa2f548494348ce54d2533cfa9d74b","The irreversible illness-death model describes the pathway from an initial state to an absorbing state either directly or through an intermediate state. This model is frequently used in medical applications where the intermediate state represents illness and the absorbing state represents death. In many studies, disease onset times are not known exactly. This happens for example if the disease status of a patient can only be assessed at follow-up visits. In this situation the disease onset times are interval-censored. This article presents the SmoothHazard package for R. It implements algorithms for simultaneously fitting regression models to the three transition intensities of an illness-death model where the transition times to the intermediate state may be interval-censored and all the event times can be right-censored. The package parses the individual data structure of the subjects in a data set to find the individual contributions to the likelihood. The three baseline transition intensity functions are modelled by Weibull distributions or alternatively by M-splines in a semi-parametric approach. For a given set of covariates, the estimated transition intensities can be combined into predictions of cumulative event probabilities and life expectancies. © 2017, American Statistical Association. All rights reserved."
"Multinomial logit models with continuous and discrete individual heterogeneity in R: The gmnl package",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025127864&doi=10.18637%2fjss.v079.i02&partnerID=40&md5=ed71aa701caad9aac14da35885e4f6cf","This paper introduces the package gmnl in R for estimation of multinomial logit models with unobserved heterogeneity across individuals for cross-sectional and panel (longitudinal) data. Unobserved heterogeneity is modeled by allowing the parameters to vary randomly over individuals according to a continuous, discrete, or discrete-continuous mixture distribution, which must be chosen a priori by the researcher. In particular, the models supported by gmnl are the multinomial or conditional logit, the mixed multinomial logit, the scale heterogeneity multinomial logit, the generalized multinomial logit, the latent class logit, and the mixed-mixed multinomial logit. These models are estimated using either the maximum likelihood estimator or the maximum simulated likelihood estimator. This article describes and illustrates with real databases all functionalities of gmnl, including the derivation of individual conditional estimates of both the random parameters and willingness-to-pay measures. © 2017, American Statistical Association. All rights reserved."
"Rgbp: An R package for gaussian, poisson, and binomial random effects models with frequency coverage evaluations",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020483842&doi=10.18637%2fjss.v078.i05&partnerID=40&md5=b587f15aeb12c91daf5c2b0746350d2c","Rgbp is an R package that provides estimates and verifiable confidence intervals for random effects in two-level conjugate hierarchical models for overdispersed Gaussian, Poisson, and binomial data. Rgbp models aggregate data from k independent groups summarized by observed sufficient statistics for each random effect, such as sample means, possibly with covariates. Rgbp uses approximate Bayesian machinery with unique improper priors for the hyper-parameters, which leads to good repeated sampling coverage properties for random effects. A special feature of Rgbp is an option that generates synthetic data sets to check whether the interval estimates for random effects actually meet the nominal confidence levels. Additionally, Rgbp provides inference statistics for the hyper-parameters, e.g., regression coefficients. © 2017, American Statistical Association. All rights reserved."
"pyParticleest: A Python framework for particle-based estimation methods",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020435054&doi=10.18637%2fjss.v078.i03&partnerID=40&md5=0ac50028bce8069180fc4a7e3c25c1b3","Particle methods such as the particle filter and particle smoothers have proven very useful for solving challenging nonlinear estimation problems in a wide variety of fields during the last decade. However, there are still very few existing tools available to support and assist researchers and engineers in applying the vast number of methods in this field to their own problems. This paper identifies the common operations between the methods and describes a software framework utilizing this information to provide a flexible and extensible foundation which can be used to solve a large variety of problems in this domain, thereby allowing code reuse to reduce the implementation burden and lowering the barrier of entry for applying this exciting field of methods. The software implementation presented in this paper is freely available and permissively licensed under the GNU Lesser General Public License, and runs on a large number of hardware and software platforms, making it usable for a large variety of scenarios. © 2017, American Statistical Association. All rights reserved."
"Estimation of extended mixed models using latent classes and latent processes: The R package lcmm",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020429515&doi=10.18637%2fjss.v078.i02&partnerID=40&md5=dd50a13c6be757815855cea51108b7bf","The R package lcmm provides a series of functions to estimate statistical models based on linear mixed model theory. It includes the estimation of mixed models and latent class mixed models for Gaussian longitudinal outcomes (hlme), curvilinear and ordinal univariate longitudinal outcomes (lcmm) and curvilinear multivariate outcomes (multlcmm), as well as joint latent class mixed models (Jointlcmm) for a (Gaussian or curvilinear) longitudinal outcome and a time-to-event outcome that can be possibly left-truncated right-censored and defined in a competing setting. Maximum likelihood esimators are obtained using a modified Marquardt algorithm with strict convergence criteria based on the parameters and likelihood stability, and on the negativity of the second derivatives. The package also provides various post-fit functions including goodness-of-fit analyses, classification, plots, predicted trajectories, individual dynamic prediction of the event and predictive accuracy assessment. This paper constitutes a companion paper to the package by introducing each family of models, the estimation technique, some implementation details and giving examples through a dataset on cognitive aging. © 2017, American Statistical Association. All rights reserved."
"Libstable: Fast, parallel, and high-precision computation of α-stable distributions in R, C/C++, and MATLAB",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020421331&doi=10.18637%2fjss.v078.i01&partnerID=40&md5=97c982454e963fd0fbb344ec89ba56b5","α-stable distributions are a family of well-known probability distributions. However, the lack of closed analytical expressions hinders their application. Currently, several tools have been developed to numerically evaluate their density and distribution functions or to estimate their parameters, but available solutions either do not reach sufficient precision on their evaluations or are excessively slow for practical purposes. Moreover, they do not take full advantage of the parallel processing capabilities of current multi-core machines. Other solutions work only on a subset of the α-stable parameter space. In this paper we present an R package and a C/C++ library with a MATLAB front-end that permit parallelized, fast and high precision evaluation of density, distribution and quantile functions, as well as random variable generation and parameter estimation of α -stable distributions in their whole parameter space. The described library can be easily integrated into third party developments. © 2017, American Statistical Association. All rights reserved."
"pvclass: An R package for p values for classification",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020381407&doi=10.18637%2fjss.v078.i04&partnerID=40&md5=6ff947ad54ef36175d3f0c1feddf3c2f","Let (X, Y) be a random variable consisting of an observed feature vector X and an unobserved class label Y ∈ {1, 2,…, L} with unknown joint distribution. In addition, let D be a training data set consisting of n completely observed independent copies of (X, Y). Instead of providing point predictors (classifiers) for Y, we compute for each b ∈ {1, 2,…, L} a p value πb(X, D) for the null hypothesis that Y = b, treating Y temporarily as a fixed parameter, i.e., we construct a prediction region for Y with a certain confidence. The advantages of this approach over more traditional ones are reviewed briefly. In principle, any reasonable classifier can be modified to yield nonparametric p values. We describe the R package pvclass which computes nonparametric p values for the potential class memberships of new observations as well as cross-validated p values for the training data. Additionally, it provides graphical displays and quantitative analyses of the p values. © 2017, American Statistical Association. All rights reserved."
"Fitting nonlinear structural equation models in R with package nlsem",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018449844&doi=10.18637%2fjss.v077.i07&partnerID=40&md5=5ae94564ae716cc423d0a89da3ace16f","Structural equation mixture modeling (SEMM) has become a standard procedure in latent variable modeling over the last two decades (Jedidi, Jagpal, and DeSarbo 1997b; Muthén and Shedden 1999; Muthén 2001, 2004; Muthén and Asparouhov 2009). SEMM was proposed as a technique for the approximation of nonlinear latent variable relationships by finite mixtures of linear relationships (Bauer 2005, 2007; Bauer, Baldasaro, and Gottfredson 2012). In addition to this semiparametric approach to nonlinear latent variable modeling, there are numerous parametric nonlinear approaches for normally distributed variables (e.g., LMS in Mplus; Klein and Moosbrugger 2000). Recently, an additional semiparametric nonlinear structural equation mixture modeling (NSEMM) approach was proposed by Kelava, Nagengast, and Brandt (2014) that is capable of dealing with nonnormal predictors. In the nlsem package presented here, the SEMM, two distribution analytic (QML and LMS) and NSEMM approaches can be specified and estimated. We provide examples of how to use the package in the context of nonlinear latent variable modeling. © 2017, American Statistical Association. All rights reserved."
"Iterative bias reduction multivariate smoothing in R: The ibr package",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018441153&doi=10.18637%2fjss.v077.i09&partnerID=40&md5=1b23fcc4963459cb0ed4c66ce0892f11","In multivariate nonparametric analysis curse of dimensionality forces one to use large smoothing parameters. This leads to a biased smoother. Instead of focusing on optimally selecting the smoothing parameter, we fix it to some reasonably large value to ensure an over-smoothing of the data. The resulting base smoother has a small variance but a substantial bias. In this paper, we propose an R package named ibr to iteratively correct the initial bias of the (base) estimator by an estimate of the bias obtained by smoothing the residuals. After a brief description of iterated bias reduction smoothers, we examine the base smoothers implemented in the package: Nadaraya-Watson kernel smoothers, Duchon splines smoothers and their low rank counterparts. Then, we explain the stopping rules available in the package and their implementation. Finally we illustrate the package on two examples: a toy example in ℝ2and the original Los Angeles ozone dataset. © 2017, American Statistical Association. All rights reserved."
"Spatio-temporal analysis of epidemic phenomena using the R package surveillance",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018436277&doi=10.18637%2fjss.v077.i11&partnerID=40&md5=2d8c2e3c1a529c6b66ebff0fdf0f0fe4","The availability of geocoded health data and the inherent temporal structure of communicable diseases have led to an increased interest in statistical models and software for spatio-temporal data with epidemic features. The open source R package surveillance can handle various levels of aggregation at which infective events have been recorded: individual-level time-stamped geo-referenced data (case reports) in either continuous space or discrete space, as well as counts aggregated by period and region. For each of these data types, the surveillance package implements tools for visualization, likelihoood inference and simulation from recently developed statistical regression frameworks capturing endemic and epidemic dynamics. Altogether, this paper is a guide to the spatio-temporal modeling of epidemic phenomena, exemplified by analyses of public health surveillance data on measles and invasive meningococcal disease. © 2017, American Statistical Association. All rights reserved."
"Software for distributed computation on medical databases: A demonstration project",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018419145&doi=10.18637%2fjss.v077.i13&partnerID=40&md5=704ad42b743be722ba40e05722a42626","Bringing together the information latent in distributed medical databases promises to personalize medical care by enabling reliable, stable modeling of outcomes with rich feature sets (including patient characteristics and treatments received). However, there are barriers to aggregation of medical data, due to lack of standardization of ontologies, privacy concerns, proprietary attitudes toward data, and a reluctance to give up control over end use. Aggregation of data is not always necessary for model fitting. In models based on maximizing a likelihood, the computations can be distributed, with aggregation limited to the intermediate results of calculations on local data, rather than raw data. Distributed fitting is also possible for singular value decomposition. There has been work on the technical aspects of shared computation for particular applications, but little has been published on the software needed to support the “social networking” aspect of shared computing, to reduce the barriers to collaboration. We describe a set of software tools that allow the rapid assembly of a collaborative computational project, based on the flexible and extensible R statistical software and other open source packages, that can work across a heterogeneous collection of database environments, with full transparency to allow local officials concerned with privacy protections to validate the safety of the method. We describe the principles, architecture, and successful test results for the site-stratified Cox model and rank-k singular value decomposition. © 2017, American Statistical Association. All rights reserved."
"Gaussian copula regression in R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018405877&doi=10.18637%2fjss.v077.i08&partnerID=40&md5=a1a380050112a3870d0527f4212daa03","This article describes the R package gcmr for fitting Gaussian copula marginal regression models. The Gaussian copula provides a mathematically convenient framework to handle various forms of dependence in regression models arising, for example, in time series, longitudinal studies or spatial data. The package gcmr implements maximum likelihood inference for Gaussian copula marginal regression. The likelihood function is approximated with a sequential importance sampling algorithm in the discrete case. The package is designed to allow a flexible specification of the regression model and the dependence structure. Illustrations include negative binomial modeling of longitudinal count data, beta regression for time series of rates and logistic regression for spatially correlated binomial data. © 2017, American Statistical Association. All rights reserved."
"Nonpmodelcheck: An R package for nonparametric lack-of-fit testing and variable selection",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018390793&doi=10.18637%2fjss.v077.i10&partnerID=40&md5=e0a5997957386108d76f284210230da4","We describe the R package NonpModelCheck for hypothesis testing and variable selection in nonparametric regression. This package implements functions to perform hypothesis testing for the significance of a predictor or a group of predictors in a fully nonparametric heteroscedastic regression model using high-dimensional one-way ANOVA. Based on the p values from the test of each covariate, three different algorithms allow the user to perform variable selection using false discovery rate corrections. A function for classical local polynomial regression is implemented for the multivariate context, where the degree of the polynomial can be as large as needed and bandwidth selection strategies are built in. © 2017, American Statistical Association. All rights reserved."
"Anthropometry: An R package for analysis of anthropometric data",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018373538&doi=10.18637%2fjss.v077.i06&partnerID=40&md5=e14e94737cdb48d16b713d51ec6c6314","The development of powerful new 3D scanning techniques has enabled the generation of large up-to-date anthropometric databases which provide highly valued data to improve the ergonomic design of products adapted to the user population. As a consequence, Ergonomics and Anthropometry are two increasingly quantitative fields, so advanced statistical methodologies and modern software tools are required to get the maximum benefit from anthropometric data. This paper presents a new R package, called Anthropometry, which is available on the Comprehensive R Archive Network. It brings together some statistical methodologies concerning clustering, statistical shape analysis, statistical archetypal analysis and the statistical concept of data depth, which have been especially developed to deal with anthropometric data. They are proposed with the aim of providing effective solutions to some common anthropometric problems, such as clothing design or workstation design (focusing on the particular case of aircraft cockpits). The utility of the package is shown by analyzing the anthropometric data obtained from a survey of the Spanish female population performed in 2006 and from the 1967 United States Air Force survey. This manuscript is also contained in Anthropometry as a vignette. © 2017, American Statistical Association. All rights reserved."
"QoLR: An R package for the longitudinal analysis of health-related quality of life in oncology",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018368523&doi=10.18637%2fjss.v077.i12&partnerID=40&md5=3b17d4e14a98547c820656cff9952757","Health-related quality of life has become increasingly important in clinical trials over the past two decades. The R package QoLR is a recently developed package for the longitudinal analysis of health-related quality of life in oncology. This package contains the scoring of most of the European Organisation for Research and Treatment of Cancer quality of life questionnaires and some programs to analyze the time to a health-related quality of life score deterioration as a modality of longitudinal analysis in oncology. © 2017, American Statistical Association. All rights reserved."
"A SAS macro for covariate-constrained randomization of general cluster-randomized and unstratified designs",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017617668&doi=10.18637%2fjss.v077.c01&partnerID=40&md5=dd682e956133ac888f0ce55d4e9a37c1","Ivers et al. (2012) have recently stressed the importance to both statistical power and face validity of balancing allocations to study arms on relevant covariates. While several techniques exist (e.g., minimization, pair-matching, stratification), the covariate-constrained randomization (CCR) approach proposed by Moulton (2004) is favored when clusters can be recruited prior to randomization. CCRA V1.0, a macro published by Chaudhary and Moulton (2006), provides a SAS implementation of CCR for a particular subset of possible designs (those with two arms, small numbers of strata and clusters, an equal number of clusters within each stratum, and constraints that can be expressed as absolute mean differences between arms). This paper presents a more comprehensive macro, CCR, that is applicable across a wider variety of designs and provides statistics describing the range of possible allocations meeting the constraints in addition to performing the actual random assignment. © 2017, American Statistical Association. All rights reserved."
"Continuous time structural equation modeling with r package ctsem",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017594702&doi=10.18637%2fjss.v077.i05&partnerID=40&md5=857965078fb5246e21d9c9c9b8e7ad44","We introduce ctsem, an R package for continuous time structural equation modeling of panel (N > 1) and time series (N = 1) data, using full information maximum likelihood. Most dynamic models (e.g., cross-lagged panel models) in the social and behavioural sciences are discrete time models. An assumption of discrete time models is that time intervals between measurements are equal, and that all subjects were assessed at the same intervals. Violations of this assumption are often ignored due to the difficulty of accounting for varying time intervals, therefore parameter estimates can be biased and the time course of effects becomes ambiguous. By using stochastic differential equations to estimate an underlying continuous process, continuous time models allow for any pattern of measurement occasions. By interfacing to OpenMx, ctsem combines the flexible specification of structural equation models with the enhanced data gathering opportunities and improved estimation of continuous time models. ctsem can estimate relationships over time for multiple latent processes, measured by multiple noisy indicators with varying time intervals between observations. Within and between effects are estimated simultaneously by modeling both observed covariates and unobserved heterogeneity. Exogenous shocks with different shapes, group differences, higher order diffusion effects and oscillating processes can all be simply modeled. We first introduce and define continuous time models, then show how to specify and estimate a range of continuous time models using ctsem. © 2017, American Statistical Association. All rights reserved."
"Bayesian network constraint-based structure learning algorithms: Parallel and optimized implementations in the bnlearn R package",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016818725&doi=10.18637%2fjss.v077.i02&partnerID=40&md5=96fdabfd8331e0dbf32e56218a89c2de","It is well known in the literature that the problem of learning the structure of Bayesian networks is very hard to tackle: Its computational complexity is super-exponential in the number of nodes in the worst case and polynomial in most real-world scenarios.Efficient implementations of score-based structure learning benefit from past and current research in optimization theory, which can be adapted to the task by using the network score as the objective function to maximize. This is not true for approaches based on conditional independence tests, called constraint-based learning algorithms. The only optimization in widespread use, backtracking, leverages the symmetries implied by the definitions of neighborhood and Markov blanket. In this paper we illustrate how backtracking is implemented in recent versions of the bnlearn R package, and how it degrades the stability of Bayesian network structure learning for little gain in terms of speed. As an alternative, we describe a software architecture and framework that can be used to parallelize constraint-based structure learning algorithms (also implemented in bnlearn) and we demonstrate its performance using four reference networks and two real-world data sets from genetics and systems biology. We show that on modern multi-core or multiprocessor hardware parallel implementations are preferable over backtracking, which was developed when single-processor machines were the norm. © 2017, American Statistical Association. All rights reserved."
"Wsrf: An R package for classification with scalable weighted subspace random forests",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016808167&doi=10.18637%2fjss.v077.i03&partnerID=40&md5=e433d0b6009913444bc36922fcfc0627","We describe a parallel implementation in R of the weighted subspace random forest algorithm (Xu, Huang, Williams, Wang, and Ye 2012) available as the wsrf package. A novel variable weighting method is used for variable subspace selection in place of the traditional approach of random variable sampling. This new approach is particularly useful in building models for high dimensional data - often consisting of thousands of variables. Parallel computation is used to take advantage of multi-core machines and clusters of machines to build random forest models from high dimensional data in considerably shorter times. A series of experiments presented in this paper demonstrates that wsrf is faster than existing packages whilst retaining and often improving on the classification performance, particularly for high dimensional data. © 2017, American Statistical Association. All rights reserved."
"Ranger: A fast implementation of random forests for high dimensional data in C++ and R",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016782791&doi=10.18637%2fjss.v077.i01&partnerID=40&md5=047e03a46315f6af633dd58a85d8d7a6","We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study. © 2017, American Statistical Association. All rights reserved."
"Spatsurv: An R package for bayesian inference with spatial survival models",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016749560&doi=10.18637%2fjss.v077.i04&partnerID=40&md5=6071d3ee135b55c40598f815b54edd1c","Survival methods are used for the statistical modelling of time-to-event data. Survival data are characterized by a set of complete records, in which the time of the event is known; and a set of censored records, in which the event was known to have occurred in an interval. When survival data are spatially referenced, the spatial variation in survival times may be of scientific interest. In this article, we introduce a new R package, spatsurv, for inference with spatially referenced survival data. The specific type of model fitted by this package is a parametric proportional hazards model in which the spatially correlated frailties are modelled by a log-Gaussian stochastic process. The package is extensible in that it allows the user to easily create new models for the baseline hazard function and spatial covariance function. The package implements an advanced adaptive Markov chain Monte Carlo algorithm to deliver Bayesian inference with minimal input from the user. A particular feature of the new package is the ability to handle large datasets via the use of auxiliary frailties on a regular grid and the technique of circulant embedding for fast matrix computations. We demonstrate the new package on a real-life dataset. © 2017, American Statistical Association. All rights reserved."
"Blockcluster: An R package for model-based co-clustering",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016079487&doi=10.18637%2fjss.v076.i09&partnerID=40&md5=4a878cc38910937a46650deb7c325275","Simultaneous clustering of rows and columns, usually designated by bi-clustering, coclustering or block clustering, is an important technique in two way data analysis. A new standard and efficient approach has been recently proposed based on the latent block model (Govaert and Nadif 2003) which takes into account the block clustering problem on both the individual and variable sets. This article presents our R package blockcluster for co-clustering of binary, contingency and continuous data based on these very models. In this document, we will give a brief review of the model-based block clustering methods, and we will show how the R package blockcluster can be used for co-clustering. © 2017, American Statistical Association. All rights reserved."
"CEoptim: Cross-entropy R package for optimization",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016071171&doi=10.18637%2fjss.v076.i08&partnerID=40&md5=e0e008f579adf717a83dcf380d6bf89f","The cross-entropy (CE) method is a simple and versatile technique for optimization, based on Kullback-Leibler (or cross-entropy) minimization. The method can be applied to a wide range of optimization tasks, including continuous, discrete, mixed and constrained optimization problems. The new package CEoptim provides the R implementation of the CE method for optimization. We describe the general CE methodology for optimization and well as some useful modifications. The usage and efficacy of CEoptim is demonstrated through a variety of optimization examples, including model fitting, combinatorial optimization, and maximum likelihood estimation. © 2017, American Statistical Association. All rights reserved."
"BayesQR: A bayesian approach to quantile regression",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016044377&doi=10.18637%2fjss.v076.i07&partnerID=40&md5=c113968ab603a5fe2ad9027767b7076a","After its introduction by Koenker and Basset (1978), quantile regression has become an important and popular tool to investigate the conditional response distribution in regression. The R package bayesQR contains a number of routines to estimate quantile regression parameters using a Bayesian approach based on the asymmetric Laplace distribution. The package contains functions for the typical quantile regression with continuous dependent variable, but also supports quantile regression for binary dependent variables. For both types of dependent variables, an approach to variable selection using the adaptive lasso approach is provided. For the binary quantile regression model, the package also contains a routine that calculates the fitted probabilities for each vector of predictors. In addition, functions for summarizing the results, creating traceplots, posterior histograms and drawing quantile plots are included. This paper starts with a brief overview of the theoretical background of the models used in the bayesQR package. The main part of this paper discusses the computational problems that arise in the implementation of the procedure and illustrates the usefulness of the package through selected examples. © 2017, American Statistical Association. All rights reserved."
"Httk: R package for high-throughput toxicokinetics",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015103855&doi=10.18637%2fjss.v079.i04&partnerID=40&md5=391a37ad5bc2c73cc5048a400801ea55","Thousands of chemicals have been profiled by high-throughput screening programs such as ToxCast and Tox21; these chemicals are tested in part because most of them have limited or no data on hazard, exposure, or toxicokinetics. Toxicokinetic models aid in predicting tissue concentrations resulting from chemical exposure, and a “reverse dosimetry” approach can be used to predict exposure doses sufficient to cause tissue concentrations that have been identified as bioactive by high-throughput screening. We have created four toxicokinetic models within the R software package httk. These models are designed to be parameterized using high-throughput in vitro data (plasma protein binding and hepatic clearance), as well as structure-derived physicochemical properties and species-specific physiological data. The package contains tools for Monte Carlo sampling and reverse dosimetry along with functions for the analysis of concentration vs. time simulations. The package can currently use human in vitro data to make predictions for 553 chemicals in humans, rats, mice, dogs, and rabbits, including 94 pharmaceuticals and 415 ToxCast chemicals. For 67 of these chemicals, the package includes rat-specific in vitro data. This package is structured to be augmented with additional chemical data as they become available. Package httk enables the inclusion of toxicokinetics in the statistical analysis of chemicals undergoing high-throughput screening. © 2017, American Statistical Association. All Rights Resreved."
"Mixed non-parametric and parametric estimation techniques in R package etasFLP for earthquakes’ description",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009917822&doi=10.18637%2fjss.v076.i03&partnerID=40&md5=22de37e9eb18b78393b4f265d97f9010","etasFLP is an R package which fits an epidemic type aftershock sequence (ETAS) model to an earthquake catalog; non-parametric background seismicity can be estimated through a forward predictive likelihood approach, while parametric components of triggered seismicity are estimated through maximum likelihood; estimation steps are alternated until convergence is obtained and for each event the probability of being a background event is estimated. The package includes options which allow its wide use. Methods for plot, summary and profile are defined for the main output class object. The paper provides examples of the package’s use with description of the underlying R and Fortran routines. © 2017, American Statistical Association. All rights reserved."
"A panel data toolbox for MATLAB",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009914905&doi=10.18637%2fjss.v076.i06&partnerID=40&md5=5f6fcccd63d093b871a838df13f9ca70","Panel Data Toolbox is a new package for MATLAB that includes functions to estimate the main econometric methods of balanced and unbalanced panel data analysis. The package includes code for the standard fixed, between and random effects estimation methods, as well as for the existing instrumental panels and a wide array of spatial panels. A full set of relevant tests is also included. This paper describes the methodology and implementation of the functions and illustrates their use with well-known examples. We perform numerical checks against other popular commercial and free software to show the validity of the results. © 2017 American Statistical Association. All rights reserved."
"Stan: A probabilistic programming language",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009889748&doi=10.18637%2fjss.v076.i01&partnerID=40&md5=b36b0a4bae7b1a11a7ec100d59443e48","Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting. © 2017 American Statistical Association. All rights reserved."
"Nonparametric inference for multivariate data: The R package npmv",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009877468&doi=10.18637%2fjss.v076.i04&partnerID=40&md5=407b92c76c3565e691157cb3fa2c2065","We introduce the R package npmv that performs nonparametric inference for the comparison of multivariate data samples and provides the results in easy-to-understand, but statistically correct, language. Unlike in classical multivariate analysis of variance, multivariate normality is not required for the data. In fact, the different response variables may even be measured on different scales (binary, ordinal, quantitative). p values are calculated for overall tests (permutation tests and F approximations), and, using multiple testing algorithms which control the familywise error rate, significant subsets of response variables and factor levels are identified. The package may be used for low- or high-dimensional data with small or with large sample sizes and many or few factor levels. © 2017 American Statistical Association. All rights reserved."
"Blind source separation based on joint diagonalization in R: The packages JADE and BSSasymp",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009865391&doi=10.18637%2fjss.v076.i02&partnerID=40&md5=0fcd8d0b024ce3d491e10e468d0316d7","Blind source separation (BSS) is a well-known signal processing tool which is used to solve practical data analysis problems in various fields of science. In BSS, we assume that the observed data consists of linear mixtures of latent variables. The mixing system and the distributions of the latent variables are unknown. The aim is to find an estimate of an unmixing matrix which then transforms the observed data back to latent sources. In this paper we present the R packages JADE and BSSasymp. The package JADE offers several BSS methods which are based on joint diagonalization. Package BSSasymp contains functions for computing the asymptotic covariance matrices as well as their data-based estimates for most of the BSS estimators included in package JADE. Several simulated and real datasets are used to illustrate the functions in these two packages. © 2017 American Statistical Association. All rights reserved."
"DFIT: An R package for Raju’s differential functioning of items and tests framework",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009861394&doi=10.18637%2fjss.v076.i05&partnerID=40&md5=b0a8ed1b24fbc4f9c00f0330b012a3c4","This paper presents DFIT, an R package that implements the differential functioning of items and tests framework as well as the Monte Carlo item parameter replication approach for producing cut-off points for differential item functioning indices. Furthermore, it illustrates how to use the package to calculate power for the NCDIF index, both post hoc, as has regularly been the case in differential item functioning empirical and simulation studies, as well as a priori given certain item parameters. The version reviewed here implements all DFIT indices and Raju’s area measures for tests comprised of items modeled with the same parametric item response unidimensional model (1-, 2-, and 3-parameters, generalized partial credit model or graded response model), the Mantel-Haenszel statistic with an underlying dichotomous item response model, and the item parameter replication method for any of the estimated indices with dichotomous item response models. © 2017 American Statistical Association. All rights reserved."
"Computerized adaptive testing with R: Recent updates of the package catR",2017,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009833757&doi=10.18637%2fjss.v076.c01&partnerID=40&md5=6bfb7fe9ee053f356c493f9a4b2b5128","The purpose of this paper is to list the recent updates of the R package catR. This package allows for generating response patterns under a computerized adaptive testing (CAT) framework with underlying item response theory (IRT) models. Among the most important updates, well-known polytomous IRT models are now supported by catR; several item selection rules have been added; and it is now possible to perform post-hoc simulations. Some functions were also rewritten or withdrawn to improve the usefulness and performances of the package. © 2017 American Statistical Association. All rights reserved."
"Just another Gibbs additive Modeler: Interfacing JAGS and mgcv",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007362148&doi=10.18637%2fjss.v075.i07&partnerID=40&md5=d8c705938d4dc2ce2c36d1753cefc2d0","The BUGS language offers a very flexible way of specifying complex statistical models for the purposes of Gibbs sampling, while its JAGS variant offers very convenient R integration via the rjags package. However, including smoothers in JAGS models can involve some quite tedious coding, especially for multivariate or adaptive smoothers. Further, if an additive smooth structure is required then some care is needed, in order to centre smooths appropriately, and to find appropriate starting values. R package mgcv implements a wide range of smoothers, all in a manner appropriate for inclusion in JAGS code, and automates centring and other smooth setup tasks. The purpose of this note is to describe an interface between mgcv and JAGS, based around an R function, jagam, which takes a generalized additive model (GAM) as specified in mgcv and automatically generates the JAGS model code and data required for inference about the model via Gibbs sampling. Although the auto-generated JAGS code can be run as is, the expectation is that the user would wish to modify it in order to add complex stochastic model components readily specified in JAGS. A simple interface is also provided for visualisation and further inference about the estimated smooth components using standard mgcv functionality. The methods described here will be un-necessarily inefficient if all that is required is fully Bayesian inference about a standard GAM, rather than the full flexibility of JAGS. In that case the BayesX package would be more efficient. © 2016 Journal of Statistical Software. All rights reserved."
"bayesPop: Probabilistic population projections",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007355234&doi=10.18637%2fjss.v075.i05&partnerID=40&md5=5d722adaebba27bedc791da824755638","We describe bayesPop, an R package for producing probabilistic population projections for all countries. This uses probabilistic projections of total fertility and life expectancy generated by Bayesian hierarchical models. It produces a sample from the joint posterior predictive distribution of future age-and sex-specific population counts, fertility rates and mortality rates, as well as future numbers of births and deaths. It provides graphical ways of summarizing this information, including trajectory plots and various kinds of probabilistic population pyramids. An expression language is introduced which allows the user to produce the predictive distribution of a wide variety of derived population quantities, such as the median age or the old age dependency ratio. The package produces aggregated projections for sets of countries, such as UN regions or trading blocs. The methodology has been used by the United Nations to produce their most recent official population projections for all countries, published in the World Population Prospects. © 2016 Journal of Statistical Software. All rights reserved."
"Memetic algorithms with local search chains in R: The Rmalschains package",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007341908&doi=10.18637%2fjss.v075.i04&partnerID=40&md5=8a90874f92bddb358b8434a470c4d3db","Global optimization is an important field of research both in mathematics and computer sciences. It has applications in nearly all fields of modern science and engineering. Memetic algorithms are powerful problem solvers in the domain of continuous optimization, as they offer a trade-off between exploration of the search space using an evolutionary algorithm scheme, and focused exploitation of promising regions with a local search algorithm. In particular, we describe the memetic algorithms with local search chains (MA-LS-Chains) paradigm, and the R package Rmalschains, which implements them. MA-LS-Chains has proven to be effective compared to other algorithms, especially in high-dimensional problem solving. In an experimental study, we demonstrate the advantages of using Rmalschains for high-dimension optimization problems in comparison to other optimization methods already available in R. © 2016 Journal of Statistical Software. All rights reserved."
"Robustlmm: An R package for Robust estimation of linear Mixed-Effects models",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007341510&doi=10.18637%2fjss.v075.i06&partnerID=40&md5=af664d1cf34f9479d982375fe44a6f15","As any real-life data, data modeled by linear mixed-effects models often contain outliers or other contamination. Even little contamination can drive the classic estimates far away from what they would be without the contamination. At the same time, datasets that require mixed-effects modeling are often complex and large. This makes it difficult to spot contamination. Robust estimation methods aim to solve both problems: to provide estimates where contamination has only little influence and to detect and flag contamination. We introduce an R package, robustlmm, to robustly fit linear mixed-effects models. The package’s functions and methods are designed to closely equal those offered by lme4, the R package that implements classic linear mixed-effects model estimation in R. The robust estimation method in robustlmm is based on the random effects contamination model and the central contamination model. Contamination can be detected at all levels of the data. The estimation method does not make any assumption on the data’s grouping structure except that the model parameters are estimable. robustlmm supports hierarchical and non-hierarchical (e.g., crossed) grouping structures. The robustness of the estimates and their asymptotic efficiency is fully controlled through the function interface. Individual parts (e.g., fixed effects and variance components) can be tuned independently. In this tutorial, we show how to fit robust linear mixed-effects models using robustlmm, how to assess the model fit, how to detect outliers, and how to compare different fits. © 2016 Journal of Statistical Software. All rights reserved."
"Simulation-based power calculations for mixed effects modeling: Ipdpower in stata",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994654335&doi=10.18637%2fjss.v074.i12&partnerID=40&md5=279ed57452b2ed2a1139a8f770c48e33","Simulations are a practical and reliable approach to power calculations, especially for multi-level mixed effects models where the analytic solutions can be very complex. In addition, power calculations are model-specific and multi-level mixed effects models are defined by a plethora of parameters. In other words, model variations in this context are numerous and so are the tailored algebraic calculations. This article describes ipdpower in Stata, a new simulations-based command that calculates power for mixed effects two-level data structures. Although the command was developed having individual patient data meta-analyses and primary care databases analyses in mind, where patients are nested within studies and general practices respectively, the methods apply to any two-level structure. © 2016, American Statistical Association. All rights reserved."
"Synthpop: Bespoke creation of synthetic data in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994641689&doi=10.18637%2fjss.v074.i11&partnerID=40&md5=4d7cd63314d81b22de099de0fd93f11d","In many contexts, confidentiality constraints severely restrict access to unique and valuable microdata. Synthetic data which mimic the original observed data and preserve the relationships between variables but do not contain any disclosive records are one possible solution to this problem. The synthpop package for R, introduced in this paper, provides routines to generate synthetic versions of original data sets. We describe the methodology and its consequences for the data characteristics. We illustrate the package features using a survey data example. © 2016, American Statistical Association. All rights reserved."
"Stochastic newton sampler: The R package sns",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994576207&doi=10.18637%2fjss.v074.c02&partnerID=40&md5=fbb709399c650d0639d07944b874dec3","The R package sns implements the stochastic Newton sampler (SNS), a Metropolis- Hastings Markov chain Monte Carlo (MCMC) algorithm where the proposal density function is a multivariate Gaussian based on a local, second-order Taylor-series expansion of log-density. The mean of the proposal function is the full Newton step in the Newton- Raphson optimization algorithm. Taking advantage of the local, multivariate geometry captured in log-density Hessian allows SNS to be more efficient than univariate samplers, approaching independent sampling as the density function increasingly resembles a multivariate Gaussian. SNS requires the log-density Hessian to be negative-definite everywhere in order to construct a valid proposal function. This property holds, or can be easily checked, for many GLM-like models. When the initial point is far from density peak, running SNS in non-stochastic mode by taking the Newton step - augmented with line search - allows the MCMC chain to converge to high-density areas faster. For high-dimensional problems, partitioning the state space into lower-dimensional subsets, and applying SNS to the subsets within a Gibbs sampling framework can significantly improve the mixing of SNS chains. In addition to the above strategies for improving convergence and mixing, sns offers utilities for diagnostics and visualization, sample-based calculation of Bayesian predictive posterior distributions, numerical differentiation, and log-density validation. © 2016, American Statistical Association. All rights reserved."
"PerMallows: An R package for mallows and generalized mallows models",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982719319&doi=10.18637%2fjss.v071.i12&partnerID=40&md5=a932b44cd804e5fb4bd86d3025e0dd03","In this paper we present the R package PerMallows, which is a complete toolbox to work with permutations, distances and some of the most popular probability models for permutations: Mallows and the Generalized Mallows models. The Mallows model is an exponential location model, considered as analogous to the Gaussian distribution. It is based on the definition of a distance between permutations. The Generalized Mallows model is its best-known extension. The package includes functions for making inference, sampling and learning such distributions. The distances considered in PerMallows are Kendall’s τ, Cayley, Hamming and Ulam. © 2016, American Statistical Association. All rights reserved."
"simplexreg: An R package for regression analysis of proportional data using the simplex distribution",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982711417&doi=10.18637%2fjss.v071.i11&partnerID=40&md5=c8c475d0b267cc113293e8d065202e99","Outcomes of continuous proportions arise in many applied areas. Such data are typically measured as percentages, rates or proportions confined in the unitary interval. In this paper, the R package simplexreg which provides dispersion model fitting of the simplex distribution is introduced to model such proportional outcomes. The maximum likelihood method and generalized estimating equations techniques are available for parameter estimation in cross-sectional and longitudinal studies, respectively. This paper presents methods and algorithms implemented in the package, including parameter estimation, model checking as well as density, cumulative distribution, quantile and random number generating functions of the simplex distribution. The package is applied to real data sets for illustration. © 2016, American Statistical Association. All rights reserved."
"Copula regression spline sample selection models: The R package SemiParSampleSel",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982070004&doi=10.18637%2fjss.v071.i06&partnerID=40&md5=7b143d71e0c383fb205dc83712d395ad","Sample selection models deal with the situation in which an outcome of interest is observed for a restricted non-randomly selected sample of the population. The estimation of these models is based on a binary equation, which describes the selection process, and an outcome equation, which is used to examine the substantive question of interest. Classic sample selection models assume a priori that continuous covariates have a linear or pre-specified non-linear relationship to the outcome, and that the distribution linking the two equations is bivariate normal. We introduce the R package SemiParSampleSel which implements copula regression spline sample selection models. The proposed implementation can deal with non-random sample selection, non-linear covariate-response relationships, and non-normal bivariate distributions between the model equations. We provide details of the model and algorithm and describe the implementation in SemiParSampleSel. The package is illustrated using simulated and real data examples. © 2016, American Statistical Association. All rights reserved."
"FuzzyStatProb: An R package for the estimation of fuzzy stationary probabilities from a sequence of observations of an unknown Markov Chain",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982686529&doi=10.18637%2fjss.v071.i08&partnerID=40&md5=b45b6f1d19be5be78a1fafc832a32f0f","Markov chains are well-established probabilistic models of a wide variety of real systems that evolve along time. Countless examples of applications of Markov chains that successfully capture the probabilistic nature of real problems include areas as diverse as biology, medicine, social science, and engineering. One interesting feature which characterizes certain kinds of Markov chains is their stationary distribution, which stands for the global fraction of time the system spends in each state. The computation of the stationary distribution requires precise knowledge of the transition probabilities. When the only information available is a sequence of observations drawn from the system, such probabilities have to be estimated. Here we review an existing method to estimate fuzzy transition probabilities from observations and, with them, obtain the fuzzy stationary distribution of the resulting fuzzy Markov chain. The method also works when the user directly provides fuzzy transition probabilities. We provide an implementation in the R environment that is the first available to the community and serves as a proof of concept. We demonstrate the usefulness of our proposal with computational experiments on a toy problem, namely a time-homogeneous Markov chain that guides the randomized movement of an autonomous robot that patrols a small area. © 2016, American Statistical Association. All rights reserved."
"SNPMClust: Bivariate gaussian genotype clustering and calling for illumina microarrays",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980373428&doi=10.18637%2fjss.v071.c02&partnerID=40&md5=aa1d7ae647f3db3aa3a2ffc79d048996","SNPMClust is an R package for genotype clustering and calling with Illumina microarrays. It was originally developed for studies using the GoldenGate custom genotyping platform but can be used with other Illumina platforms, including Infinium BeadChip. The algorithm first rescales the fluorescent signal intensity data, adds empirically derived pseudo-data to minor allele genotype clusters, then uses the package mclust for bivariate Gaussian model fitting. We compared the accuracy and sensitivity of SNPMClust to that of GenCall, Illumina’s proprietary algorithm, on a data set of 94 whole-genome amplified buccal (cheek swab) DNA samples. These samples were genotyped on a custom panel which included 1064 SNPs for which the true genotype was known with high confidence. SNPMClust produced uniformly lower false call rates over a wide range of overall call rates. © 2016, American Statistical Association. All rights reserved."
"Generating adaptive and non-adaptive test interfaces for multidimensional item response theory applications",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979766168&doi=10.18637%2fjss.v071.i05&partnerID=40&md5=adbcfda0e49274373e8bf85dd3868da1","Computerized adaptive testing (CAT) is a powerful technique to help improve measurement precision and reduce the total number of items required in educational, psychological, and medical tests. In CATs, tailored test forms are progressively constructed by capitalizing on information available from responses to previous items. CAT applications primarily have relied on unidimensional item response theory (IRT) to help select which items should be administered during the session. However, multidimensional CATs may be constructed to improve measurement precision and further reduce the number of items required to measure multiple traits simultaneously. A small selection of CAT simulation packages exist for the R environment; namely, catR (Magis and Raîche 2012), catIrt (Nydick 2014), and MAT (Choi and King 2014). However, the ability to generate graphical user interfaces for administering CATs in realtime has not been implemented in R to date, support for multidimensional CATs have been limited to the multidimensional three-parameter logistic model, and CAT designs were required to contain IRT models from the same modeling family. This article describes a new R package for implementing unidimensional and multidimensional CATs using a wide variety of IRT models, which can be unique for each respective test item, and demonstrates how graphical user interfaces and Monte Carlo simulation designs can be constructed with the mirtCAT package. © 2016, American Statistical Association. All rights reserved."
"runjags: An R package providing interface utilities, model templates, parallel computing methods and additional distributions for MCMC models in JAGS",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982701522&doi=10.18637%2fjss.v071.i09&partnerID=40&md5=2b4849a10db9504c28b5c81dd804bdf9","The runjags package provides a set of interface functions to facilitate running Markov chain Monte Carlo models in JAGS from within R. Automated calculation of appropriate convergence and sample length diagnostics, user-friendly access to commonly used graphical outputs and summary statistics, and parallelized methods of running JAGS are provided. Template model specifications can be generated using a standard lme4-style formula interface to assist users less familiar with the BUGS syntax. Automated simulation study functions are implemented to facilitate model performance assessment, as well as drop-k type cross-validation studies, using high performance computing clusters such as those provided by parallel. A module extension for JAGS is also included within runjags, providing the Pareto family of distributions and a series of minimally-informative priors including the DuMouchel and half-Cauchy priors. This paper outlines the primary functions of this package, and gives an illustration of a simulation study to assess the sensitivity of two equivalent model formulations to different prior distributions. © 2016, American Statistical Association. All rights reserved."
"rft1d: Smooth one-dimensional random field upcrossing probabilities in Python",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982696451&doi=10.18637%2fjss.v071.i07&partnerID=40&md5=e20e891231e35ae9ecfb75a3aedc1e0e","Through topological expectations regarding smooth, thresholded n-dimensional Gaussian continua, random field theory (RFT) describes probabilities associated with both the field-wide maximum and threshold-surviving upcrossing geometry. A key application of RFT is a correction for multiple comparisons which affords field-level hypothesis testing for both univariate and multivariate fields. For unbroken isotropic fields just one parameter in addition to the mean and variance is required: the ratio of a field’s size to its smoothness. Ironically the simplest manifestation of RFT (1D unbroken fields) has rarely surfaced in the literature, even during its foundational development in the late 1970s. This Python package implements 1D RFT primarily for exploring and validating RFT expectations, but also describes how it can be applied to yield statistical inferences regarding sets of experimental 1D fields. © 2016, American Statistical Association. All rights reserved."
"LARF: Instrumental variable estimation of causal effects through local average response functions",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979729787&doi=10.18637%2fjss.v071.c01&partnerID=40&md5=4b5290f6c8a15e91df67a4689ef4a728","LARF is an R package that provides instrumental variable estimation of treatment effects when both the endogenous treatment and its instrument (i.e., the treatment inducement) are binary. The method (Abadie 2003) involves two steps. First, pseudo-weights are constructed from the probability of receiving the treatment inducement. By default LARF estimates the probability by a probit regression. It also provides semiparametric power series estimation of the probability and allows users to employ other external methods to estimate the probability. Second, the pseudo-weights are used to estimate the local average response function conditional on treatment and covariates. LARF provides both least squares and maximum likelihood estimates of the conditional treatment effects. © 2016, American Statistical Association. All rights reserved."
"Monitoring count time series in R: Aberration detection in public health surveillance",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978906802&doi=10.18637%2fjss.v070.i10&partnerID=40&md5=00028849fb92d5a75d143147a07d25bb","Public health surveillance aims at lessening disease burden by, e.g., timely recognizing emerging outbreaks in case of infectious diseases. Seen from a statistical perspective, this implies the use of appropriate methods for monitoring time series of aggregated case reports. This paper presents the tools for such automatic aberration detection offered by the R package surveillance. We introduce the functionalities for the visualization, modeling and monitoring of surveillance time series. With respect to modeling we focus on univariate time series modeling based on generalized linear models (GLMs), multivariate GLMs, generalized additive models and generalized additive models for location, shape and scale. Applications of such modeling include illustrating implementational improvements and extensions of the well-known Farrington algorithm, e.g., by spline-modeling or by treating it in a Bayesian context. Furthermore, we look at categorical time series and address overdispersion using beta-binomial or Dirichlet-multinomial modeling. With respect to monitoring we consider detectors based on either a Shewhart-like single timepoint comparison between the observed count and the predictive distribution or by likelihoodratio based cumulative sum methods. Finally, we illustrate how surveillance can support aberration detection in practice by integrating it into the monitoring workflow of a public health institution. Altogether, the present article shows how well surveillance can support automatic aberration detection in a public health surveillance context. © 2016, American Statistical Association. All rights reserved."
"Flexsurv: A platform for parametric survival modeling in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978919943&doi=10.18637%2fjss.v070.i08&partnerID=40&md5=9db837324b79aba6a33882228168ded0","Flexsurv is an R package for fully-parametric modeling of survival data. Any parametric time-to-event distribution may be fitted if the user supplies a probability density or hazard function, and ideally also their cumulative versions. Standard survival distributions are built in, including the three and four-parameter generalized gamma and F distributions. Any parameter of any distribution can be modeled as a linear or log-linear function of covariates. The package also includes the spline model of Royston and Parmar (2002), in which both baseline survival and covariate effects can be arbitrarily flexible parametric functions of time. The main model-fitting function, flexsurvreg, uses the familiar syntax of survreg from the standard survival package (Therneau 2016). Censoring or left-truncation are specified in ‘Surv’ objects. The models are fitted by maximizing the full log-likelihood, and estimates and confidence intervals for any function of the model parameters can be printed or plotted. flexsurv also provides functions for fitting and predicting from fully-parametric multi-state models, and connects with the mstate package (de Wreede, Fiocco, and Putter 2011). This article explains the methods and design principles of the package, giving several worked examples of its use. © 2016, American Statistical Association. All rights reserved."
"Ggmcmc: Analysis of MCMC samples and Bayesian inference",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978898093&doi=10.18637%2fjss.v070.i09&partnerID=40&md5=27b4eb9d9fd63e7b4ca2050217294b66","Ggmcmc is an R package for analyzing Markov chain Monte Carlo simulations from Bayesian inference. By using a well known example of hierarchical/multilevel modeling, the article reviews the potential uses and options of the package, ranging from classical convergence tests to caterpillar plots or posterior predictive checks. © 2016, American Statistical Association. All rights reserved."
"Robust estimation of the generalized loggamma model: The R package robustloggamma",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978804188&doi=10.18637%2fjss.v070.i07&partnerID=40&md5=8a3b8f7543d051793433a77f4f32ba7a","Robustloggamma is an R package for robust estimation and inference in the generalized loggamma model. We briefly introduce the model, the estimation procedures and the computational algorithms. Then, we illustrate the use of the package with the help of a real data set. © 2016, American Statistical Association. All rights reserved."
"CircNNTSR: An R package for the statistical analysis of circular, multivariate circular, and spherical data using nonnegative Trigonometric sums",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966415670&doi=10.18637%2fjss.v070.i06&partnerID=40&md5=c80d085bc744847e2f8c11faaa19d7c7","The statistical analysis of circular, multivariate circular, and spherical data is very important in different areas, such as paleomagnetism, astronomy and biology. The use of nonnegative trigonometric sums allows for the construction of flexible probability models for these types of data to model datasets with skewness and multiple modes. The R package CircNNTSR includes functions to plot, fit by maximum likelihood, and simulate models based on nonnegative trigonometric sums for circular, multivariate circular, and spherical data. For maximum likelihood estimation of the models for the three different types of data an efficient Newton-like algorithm on a hypersphere is used. Examples of applications of the functions provided in the CircNNTSR package to actual and simulated datasets are presented and it is shown how the package can be used to test for uniformity, homogeneity, and independence using likelihood ratio tests. © 2016, American Statistical Association. All rights reserved."
"TableMaker: An excel macro for publication-quality tables",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966356823&doi=10.18637%2fjss.v070.c03&partnerID=40&md5=e254d1957728eb79152cf885fbafb0b9","This article introduces TableMaker, a Microsoft Excel macro that produces publicationquality tables and includes them as new sheets in workbooks. The macro provides an intuitive graphical user interface that allows for the full customization of all table features. It also allows users to save and load table templates, and thus allows layouts to be both reproducible and transferable. It is distributed in a single computer file. As such, the macro is easy to share, as well as accessible to even beginning and casual users of Excel. Since it allows for the quick creation of reproducible and fully customizable tables, TableMaker can be very useful to academics, policy-makers and businesses by making the presentation and formatting of results faster and more efficient. © 2016, American Statistical Association. All rights reserved."
"Generating correlated and/or overdispersed count data: A SAS implementation",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963818607&doi=10.18637%2fjss.v070.c01&partnerID=40&md5=4132b22d549e45fc87aa40cbd77a06c8","Analysis of longitudinal count data has, for long, been done using a generalized linear mixed model (GLMM), in its Poisson-normal version, to account for correlation by specifying normal random effects. Univariate counts are often handled with the negativebinomial (NEGBIN) model taking into account overdispersion by use of gamma random effects. Inherently though, longitudinal count data commonly exhibit both features of correlation and overdispersion simultaneously, necessitating analysis methodology that can account for both. The introduction of the combined model (CM) by Molenberghs, Verbeke, and Demétrio (2007) and Molenberghs, Verbeke, Demétrio, and Vieira (2010) serves this purpose, not only for count data but for the general exponential family of distributions. Here, a Poisson model is specified as the parent distribution of the data with a normally distributed random effect at the subject or cluster level and/or a gamma distribution at observation level. The GLMM and NEGBIN model are special cases. Data can be simulated from (1) the general CM, with random effects, or, (2) its marginal version directly. This paper discusses an implementation of (1) in SAS software (SAS Inc. 2011). One needs to reflect on the mean of both the combined (hierarchical) and marginal models in order to generate correlated and/or overdispersed counts. A pre-specification of the desired marginal mean (in terms of covariates and marginal parameters), a marginal variance-covariance structure and the hierarchical mean (in terms of covariates and regression parameters) is required. The implied hierarchical parameters, the variance-covariance matrix of the random effects, and the variance-covariance matrix of the overdispersion part are then derived from which correlated Poisson data are generated. Sample calls of the SAS macro are presented as well as output. © 2016, American Statistical Association. All rights reserved."
"Confidence band for the differences between two direct adjusted survival curves",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963804711&doi=10.18637%2fjss.v070.c02&partnerID=40&md5=da6df1b335a358be5fa5ad638be644ff","This paper focuses on comparing the survival outcomes of two treatments while adjusting for multiple demographic and clinical factors. We wish to answer the question: “At what times do the survival outcomes of two treatments differ?”. Previous work by Zhang and Klein (2001) addressed this issue by considering a stratified Cox model and by constructing the confidence band for the differences between survival functions of treatments, for a given subject. In this study, we utilize direct adjusted survivals of treatments, so that treatment comparison can be performed on a general basis. The confidence band for the differences between direct adjusted survivals was constructed using the simulation method. We developed two SAS macros that compute the band. The first macro was developed for data sets consisting of fixed covariates only. The second macro was developed based on the piecewise proportional hazards Cox model and allows for timedependent variables. We illustrate both macros by analyzing the survival data of a cohort of lymphoma patients. © 2016, American Statistical Association. All rights reserved."
"gsbDesign: An R package for evaluating the operating characteristics of a group sequential bayesian design",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962086864&doi=10.18637%2fjss.v069.i11&partnerID=40&md5=6ecc89d8758cb1ac39182a3f0d845a3b","The R package gsbDesign provides functions to evaluate the operating characteristics of Bayesian group sequential clinical trial designs. More specifically, we consider clinical trials with interim analyses, which compare a treatment with a control, and where the endpoint is normally distributed. Prior information can either be specified for the difference of treatment and control, or separately for the effects in the treatment and the control groups. At each interim analysis, the decision to stop or continue the trial is based on the posterior distribution of the difference between treatment and control. The decision at the final analysis is also based on this posterior distribution. Multiple success and/or futility criteria can be specified to reflect adequately medical decision-making. We describe methods to evaluate the operating characteristics of such designs for scenarios corresponding to different true treatment and control effects. The characteristics of main interest are the probabilities of success and futility at each interim analysis, and the expected sample size. We illustrate the use of gsbDesign with a detailed case study. © 2016, American Statistical Association. All Rights Reserved."
"Newdistns: An R package for new families of distributions",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962077677&doi=10.18637%2fjss.v069.i10&partnerID=40&md5=f3eca61c92b1ff9db911e2d356e8624a","The contributed R package Newdistns written by the authors is introduced. This package computes the probability density function, cumulative distribution function, quantile function, random numbers and some measures of inference for nineteen families of distributions. Each family is flexible enough to encompass a large number of structures. The use of the package is illustrated using a real data set. Also robustness of random number generation is checked by simulation. © 2016, American Statistical Association. All Rights Reserved."
"RobPer: An R package to calculate periodograms for light curves based on robust regression",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961989557&doi=10.18637%2fjss.v069.i09&partnerID=40&md5=6c239ccdcebafb89ec49fda6851251c0","An important task in astroparticle physics is the detection of periodicities in irregularly sampled time series, called light curves. The classic Fourier periodogram cannot deal with irregular sampling and with the measurement accuracies that are typically given for each observation of a light curve. Hence, methods to fit periodic functions using weighted regression were developed in the past to calculate periodograms. We present the R package RobPer which allows to combine different periodic functions and regression techniques to calculate periodograms. Possible regression techniques are least squares, least absolute deviations, least trimmed squares, M-, S-and τ-regression. Measurement accuracies can be taken into account including weights. Our periodogram function covers most of the approaches that have been tried earlier and provides new model-regression-combinations that have not been used before. To detect valid periods, RobPer applies an outlier search on the periodogram instead of using fixed critical values that are theoretically only justified in case of least squares regression, independent periodogram bars and a null hypothesis allowing only normal white noise. Finally, the package also includes a generator to generate artificial light curves. © 2016, American Statistical Association. All rights reserved."
"Statistical inference for partially observed markov processes via the R package pomp",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962110822&doi=10.18637%2fjss.v069.i12&partnerID=40&md5=84f4e1792cb1eaa897ecfa8e17247aa7","Partially observed Markov process (POMP) models, also known as hidden Markov models or state space models, are ubiquitous tools for time series analysis. The R package pomp provides a very flexible framework for Monte Carlo statistical investigations using nonlinear, non-Gaussian POMP models. A range of modern statistical methods for POMP models have been implemented in this framework including sequential Monte Carlo, iterated filtering, particle Markov chain Monte Carlo, approximate Bayesian computation, maximum synthetic likelihood estimation, nonlinear forecasting, and trajectory matching. In this paper, we demonstrate the application of these methodologies using some simple toy problems. We also illustrate the specification of more complex POMP models, using a nonlinear epidemiological model with a discrete population, seasonality, and extra-demographic stochasticity. We discuss the specification of user-defined models and the development of additional methods within the programming environment provided by pomp. © 2016, American Statistical Association. All Rights Reserved."
"Recovering a basic space from issue scales in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961267122&doi=10.18637%2fjss.v069.i07&partnerID=40&md5=cf0c0844d90c6edb1870ddd4e94e21a0","Basicspace is an R package that conducts Aldrich-McKelvey and Blackbox scaling to recover estimates of the underlying latent dimensions of issue scale data. We illustrate several applications of the package to survey data commonly used in the social sciences. Monte Carlo tests demonstrate that the procedure can recover latent dimensions and reproduce the matrix of responses at moderate levels of error and missing data. © 2016, American Statistical Association. All rights reserved."
"Global, parameterwise and joint shrinkage factor estimation",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961262651&doi=10.18637%2fjss.v069.i08&partnerID=40&md5=a9c30501795a6d784bc80f93fbfb9468","The predictive value of a statistical model can often be improved by applying shrinkage methods. This can be achieved, e.g., by regularized regression or empirical Bayes approaches. Various types of shrinkage factors can also be estimated after a maximum likelihood fit has been obtained: while global shrinkage modifies all regression coefficients by the same factor, parameterwise shrinkage factors differ between regression coefficients. The latter ones have been proposed especially in the context of variable selection. With variables which are either highly correlated or associated with regard to contents, such as dummy variables coding a categorical variable, or several parameters describing a nonlinear effect, parameterwise shrinkage factors may not be the best choice. For such cases, we extend the present methodology by so-called ‘joint shrinkage factors’, a compromise between global and parameterwise shrinkage. Shrinkage factors are often estimated using leave-one-out resampling. We also discuss a computationally simple and much faster approximation to resampling-based shrinkage factor estimation, can be easily obtained in most standard software packages for regression analyses. This alternative may be relevant for simulation studies and other computerintensive investigations. Furthermore, we provide an R package shrink implementing the mentioned shrinkage methods for models fitted by linear, generalized linear, or Cox regression, even if these models involve fractional polynomials or restricted cubic splines to estimate the influence of a continuous variable by a nonlinear function. The approaches and usage of the package shrink are illustrated by means of two examples. © 2016, American Statistical Association. All rights reserved."
"Mean and variance modeling of under- and overdispersed count data",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961136681&doi=10.18637%2fjss.v069.i06&partnerID=40&md5=fdd70f133c7bc1fe67c1ad842e270600","This article describes the R package CountsEPPM and its use in determining maximum likelihood estimates of the parameters of extended Poisson process models. These provide a Poisson process based family of flexible models that can handle both underdispersion and overdispersion in observed count data, with the negative binomial and Poisson distributions being special cases. Within CountsEPPM models with mean and variance related to covariates are constructed to match a generalized linear model formulation. Use of the package is illustrated by application to several published datasets. © 2016, American Statistical Association. All rights reserved."
"Parallel and other simulations in R made easy: An end-to-end study",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959431534&doi=10.18637%2fjss.v069.i04&partnerID=40&md5=4d01c5419834b5f4b90046a280fcdce0","It is shown how to set up, conduct, and analyze large simulation studies with the new R package simsalapar (= simulations simplified and launched parallel). A simulation study typically starts with determining a collection of input variables and their values on which the study depends. Computations are desired for all combinations of these variables. If conducting these computations sequentially is too time-consuming, parallel computing can be applied over all combinations of select variables. The final result object of a simulation study is typically an array. From this array, summary statistics can be derived and presented in terms of flat contingency or LATEX tables or visualized in terms of matrix-like figures. The R package simsalapar provides several tools to achieve the above tasks. Warnings and errors are dealt with correctly, various seeding methods are available, and run time is measured. Furthermore, tools for analyzing the results via tables or graphics are provided. In contrast to rather minimal examples typically found in R packages or vignettes, an end-to-end, not-so-minimal simulation problem from the realm of quantitative risk management is given. The concepts presented and solutions provided by simsalapar may be of interest to students, researchers, and practitioners as a how-to for conducting realistic, large-scale simulation studies in R. © 2016, American Statistical Association. All rights reserved."
"Dealing with stochastic volatility in time series using the R package stochvol",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959378628&doi=10.18637%2fjss.v069.i05&partnerID=40&md5=7aaeb2eec83956a59b70fdfb4cb0293d","The R package stochvol provides a fully Bayesian implementation of heteroskedasticity modeling within the framework of stochastic volatility. It utilizes Markov chain Monte Carlo (MCMC) samplers to conduct inference by obtaining draws from the posterior distribution of parameters and latent variables which can then be used for predicting future volatilities. The package can straightforwardly be employed as a stand-alone tool; moreover, it allows for easy incorporation into other MCMC samplers. The main focus of this paper is to show the functionality of stochvol. In addition, it provides a brief mathematical description of the model, an overview of the sampling schemes used, and several illustrative examples using exchange rate data. © 2016, American Statistical Association. All rights reserved."
"PoweR: A reproducible research tool to ease monte carlo power simulation studies for goodness-of-fit tests in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958527370&doi=10.18637%2fjss.v069.i03&partnerID=40&md5=ce63547896e07917535c4fefe7da81e7","The PoweR package aims to help obtain or verify empirical power studies for goodnessof-fit tests for independent and identically distributed data. The current version of our package is only valid for simple null hypotheses or for pivotal test statistics for which the set of critical values does not depend on a particular choice of a null distribution (and on nuisance parameters) under the non-simple null case. We also assume that the distribution of the test statistic is continuous. As a reproducible research computational tool it can be viewed as helping to simply reproducing (or detecting errors in) simulation results already published in the literature. Using our package helps also in designing new simulation studies. The empirical levels and powers for many statistical test statistics under a wide variety of alternative distributions can be obtained quickly and accurately using a C/C++ and R environment. The parallel package can be used to parallelize computations when a multicore processor is available. The results can be displayed using LATEX tables or specialized graphs, which can be directly incorporated into a report. This article gives an overview of the main design aims and principles of our package, as well as strategies for adaptation and extension. Hands-on illustrations are presented to help new users in getting started. © 2016, American Statistical Association. All rights reserved."
"Cooccur: Probabilistic species co-occurrence analysis in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958191218&doi=10.18637%2fjss.v069.c02&partnerID=40&md5=455d1a5402e9b1b8980895bb9fa156ee","The observation that species may be positively or negatively associated with each other is at least as old as the debate surrounding the nature of community structure which began in the early 1900’s with Gleason and Clements. Since then investigating species co-occurrence patterns has taken a central role in understanding the causes and consequences of evolution, history, coexistence mechanisms, competition, and environment for community structure and assembly. This is because co-occurrence among species is a measurable metric in community datasets that, in the context of phylogeny, geography, traits, and environment, can sometimes indicate the degree of competition, displacement, and phylogenetic repulsion as weighed against biotic and environmental effects promoting correlated species distributions. Historically, a multitude of different co-occurrence metrics have been developed and most have depended on data randomization procedures to produce null distributions for significance testing. Here we improve upon and present an R implementation of a recently published model that is metric-free, distribution-free, and randomization-free. The R package, cooccur, is highly accessible, easily integrates into common analyses, and handles large datasets with high performance. In the article we develop the package’s functionality and demonstrate aspects of co-occurrence analysis using three sample datasets. © 2016 American Statistical Association. All rights reserved."
"Penalized: A MATLAB toolbox for fitting generalized linear models with penalties",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044216956&doi=10.18637%2fjss.v072.i06&partnerID=40&md5=048795d03bb7d981423bc5d3ebe9e80c","penalized is a flexible, extensible, and efficient MATLAB toolbox for penalized maximum likelihood. penalized allows you to fit a generalized linear model (gaussian, logistic, poisson, or multinomial) using any of ten provided penalties, or none. The toolbox can be extended by creating new maximum likelihood models or new penalties. The toolbox also includes routines for cross-validation and plotting. © 2016, American Statistical Association. All rights reserved."
"ExtremeBounds: Extreme bounds analysis in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044213660&doi=10.18637%2fjss.v072.i09&partnerID=40&md5=ee32509d618202e6bcb09a8ed10d30d4","This article introduces the R package ExtremeBounds to perform extreme bounds analysis (EBA), a sensitivity test that examines how robustly the dependent variable of a regression model is related to a variety of possible determinants. ExtremeBounds supports Leamer’s EBA that focuses on the upper and lower extreme bounds of regression coefficients, as well as Sala-i-Martin’s EBA which considers their entire distribution. In contrast to existing alternatives, it can estimate models of a variety of user-defined sizes, use regression models other than ordinary least squares, incorporate non-linearities in the model specification, and apply custom weights and standard errors. To alleviate concerns about the multicollinearity and conceptual overlap of examined variables, ExtremeBounds allows users to specify sets of mutually exclusive variables, and can restrict the analysis to coefficients from regression models that yield a variance inflation factor within a prespecified limit. © 2016, American Statistical Association. All rights reserved."
"RSKC: An R package for a robust and sparse k-means clustering algorithm",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026390704&doi=10.18637%2fjss.v072.i05&partnerID=40&md5=f279d51bc9a3ce9f026c2cbd30afc4fa","Witten and Tibshirani (2010) proposed an algorithim to simultaneously find clusters and select clustering variables, called sparse K-means (SK-means). SK-means is particularly useful when the dataset has a large fraction of noise variables (that is, variables without useful information to separate the clusters). SK-means works very well on clean and complete data but cannot handle outliers nor missing data. To remedy these problems we introduce a new robust and sparse K-means clustering algorithm implemented in the R package RSKC. We demonstrate the use of our package on four datasets. We also conduct a Monte Carlo study to compare the performances of RSK-means and SK-means regarding the selection of important variables and identification of clusters. Our simulation study shows that RSK-means performs well on clean data and better than SK-means and other competitors on outlier-contaminated data. © 2016, American Statistical Association. All rights reserved."
"R2MLwiN: A package to run MLwiN from within R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013797286&doi=10.18637%2fjss.v072.i10&partnerID=40&md5=d6b8fa42e8c4025e811373e8f3f0ba05","R2MLwiN is a new package designed to run the multilevel modeling software program MLwiN from within the R environment. It allows for a large range of models to be specified which take account of a multilevel structure, including continuous, binary, proportion, count, ordinal and nominal responses for data structures which are nested, cross-classified and/or exhibit multiple membership. Estimation is available via iterative generalized least squares (IGLS), which yields maximum likelihood estimates, and also via Markov chain Monte Carlo (MCMC) estimation for Bayesian inference. As well as employing MLwiN’s own MCMC engine, users can request that MLwiN write BUGS model, data and initial values statements for use with WinBUGS or OpenBUGS (which R2MLwiN automatically calls via rbugs), employing IGLS starting values from MLwiN. Users can also take advantage of MLwiN’s graphical user interface: for example to specify models and inspect plots via its interactive equations and graphics windows. R2MLwiN is supported by a large number of examples, reproducing all the analyses conducted in MLwiN’s IGLS and MCMC manuals. © 2016, American Statistical Association. All rights reserved."
"The R package jmbayes for fitting joint models for longitudinal and time-to-event data using MCMC",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004061522&doi=10.18637%2fjss.v072.i07&partnerID=40&md5=c9971a1818d49e8dff73bd777ad74a4f","Joint models for longitudinal and time-to-event data constitute an attractive modeling framework that has received a lot of interest in the recent years. This paper presents the capabilities of the R package JMbayes for fitting these models under a Bayesian approach using Markov chain Monte Carlo algorithms. JMbayes can fit a wide range of joint models, including among others joint models for continuous and categorical longitudinal responses, and provides several options for modeling the association structure between the two outcomes. In addition, this package can be used to derive dynamic predictions for both outcomes, and offers several tools to validate these predictions in terms of discrimination and calibration. All these features are illustrated using a real data example on patients with primary biliary cirrhosis. © 2016, American Statistical Association. All rights reserved."
"Fast estimation of multinomial logit models: R package mnlogit",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998879856&doi=10.18637%2fjss.v075.i03&partnerID=40&md5=300b6ff1d3a3eb7c846d501cfc17ff66","We present the R package mnlogit for estimating multinomial logistic regression models, particularly those involving a large number of categories and variables. Compared to existing software, mnlogit offers speedups of 10-50 times for modestly sized problems and more than 100 times for larger problems. Running in parallel mode on a multicore machine gives up to 4 times additional speedup on 8 processor cores. mnlogit achieves its computational efficiency by drastically speeding up computation of the log-likelihood function’s Hessian matrix through exploiting structure in matrices that arise in intermediate calculations. This efficient exploitation of intermediate data structures allows mnlogit to utilize system memory much more efficiently, such that for most applications mnlogit requires less memory than comparable software by a factor that is proportional to the number of model categories. © 2016, American Statistical Association. All rights reserved."
"CLME: An R package for linear mixed effects models under inequality constraints",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998865481&doi=10.18637%2fjss.v075.i01&partnerID=40&md5=2c1f5c10d59583c042d02553a00e213e","In many applications researchers are typically interested in testing for inequality constraints in the context of linear fixed effects and mixed effects models. Although there exists a large body of literature for performing statistical inference under inequality constraints, user friendly statistical software implementing such methods is lacking, especially in the context of linear fixed and mixed effects models. In this article we introduce CLME, a package in the R language that can be used for testing a broad collection of inequality constraints. It uses residual bootstrap based methodology which is reasonably robust to non-normality as well as heteroscedasticity. The package is illustrated using two data sets. The package also contains a graphical user interface built using the shiny package. © 2016, American Statistical Association. All rights reserved."
"Collocinfer: Collocation inference in differential equation models",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998636622&doi=10.18637%2fjss.v075.i02&partnerID=40&md5=a38221a43fded71b4706cdd7f3441e69","This monograph details the implementation and use of the CollocInfer package in R for smoothing-based estimation of continuous-time nonlinear dynamic systems. These routines represent an extension of the generalized profiling methods in Ramsay, Hooker, Campbell, and Cao (2007) for estimating parameters in nonlinear ordinary differential equations. An interface to the fda package is included. The package also supports discretetime systems. We describe the methodological and computational framework and the necessary steps to use the software. Equivalent functionality is available in MATLAB. © 2016, American Statistical Association. All rights reserved."
"ERA: A sas macro for extended redundancy analysis",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994614087&doi=10.18637%2fjss.v074.c01&partnerID=40&md5=664a5b706ccfad31975afaf8dcfc99df","A new approach to structural equation modeling based on so-called extended redundancy analysis has been recently proposed in the literature, enhanced with the added characteristic of generalizing redundancy analysis and reduced-rank regression models for more than two blocks. In this approach, the relationships between the observed exogenous variables and the observed endogenous variables are moderated by the presence of unobservable composites that were estimated as linear combinations of exogenous variables, permitting a great flexibility to specify and fit a variety of structural relationships. In this paper, we propose the SAS macro %ERA to specify and fit structural relationships in the extended redundancy analysis (ERA) framework. Two examples (simulation and real data) are provided in order to reproduce results appearing in the original article where ERA was proposed. © 2016, American Statistical Association. All rights reserved."
"The R package CDM for cognitive diagnosis models",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992529993&doi=10.18637%2fjss.v074.i02&partnerID=40&md5=a2bd720c6bab2c904307d8239eb08be0","This paper introduces the R package CDM for cognitive diagnosis models (CDMs). The package implements parameter estimation procedures for two general CDM frameworks, the generalized-deterministic input noisy-and-gate (G-DINA) and the general diagnostic model (GDM). It contains additional functions for analyzing data under these frameworks, like tools for simulating and plotting data, or for evaluating global model and item fit. The paper describes the theoretical aspects of implemented CDM frameworks and it illustrates the usage of the package with empirical data of the common fraction subtraction test by Tatsuoka (1984). © 2016, American Statistical Association. All rights reserved."
"R package clickstream: Analyzing clickstream data with Markov chains",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992513180&doi=10.18637%2fjss.v074.i04&partnerID=40&md5=45aafa965a860945c5cf5fe45de2fe75","Clickstream analysis is a useful tool for investigating consumer behavior, market research and software testing. I present the clickstream package which provides functionality for reading, clustering, analyzing and writing clickstreams in R. The package allows for a modeling of lists of clickstreams as zero-, first-and higher-order Markov chains. I illustrate the application of clickstream for a list of representative clickstreams from an online store. © 2016, American Statistical Association. All rights reserved."
"ClickClust: An R package for model-based clustering of categorical sequences",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992504758&doi=10.18637%2fjss.v074.i09&partnerID=40&md5=557dad8215fe0884c4c71443f0dfb457","The R package ClickClust is a new piece of software devoted to finite mixture modeling and model-based clustering of categorical sequences. As a special kind of time series, categorical sequences, also known as categorical time series, exhibit a time-dependent nature and are traditionally modeled by means of Markov chains. Clustering categorical sequences is an important problem with multiple applications, but grouping sequences of sites or web-pages, also known as clickstreams, is one of the most well-known problems that helps discover common navigation patterns and routes taken by users. This popular application is recognized in the package title ClickClust. The paper discusses methodological and algorithmic foundations of the package based on finite mixtures of Markov models. The number of Markov chain states can often be large leading to high-dimensional transition probability matrices. The high number of model parameters can affect clustering performance severely. As a remedy to this problem, backward and forward selection algorithms are proposed for grouping states. This extends the original clustering problem to a biclustering framework. Among other capabilities of ClickClust, there are the estimation of the variance-covariance matrix corresponding to model parameter estimates, prediction of future states visited, and the construction of a display named click-plot that helps illustrate the obtained clustering solutions. All available functions and the utility of the package are thoroughly discussed and illustrated on multiple examples. © 2016, American Statistical Association. All rights reserved."
"pyJacqQ: Python implementation of Jacquez’s Q-statistics for space-time clustering of disease exposure in case-control studies",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992500294&doi=10.18637%2fjss.v074.i06&partnerID=40&md5=8944c3b0380b15b65ffb60cfd0fac486","Jacquez’s Q is a set of statistics for detecting the presence and location of space-time clusters of disease exposure. Until now, the only implementation was available in the proprietary SpaceStat software which is not suitable for a pipeline Linux environment. We have developed an open source implementation of Jacquez’s Q statistics in Python using an object-oriented approach. The most recent source code for the implementation is available at https://github.com/sjirjies/pyJacqQ under the GPL-3. It has a command line interface and a Python application programming interface. © 2016, American Statistical Association. All rights reserved."
"PerFit: An R package for person-fit analysis in IRT",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992498915&doi=10.18637%2fjss.v074.i05&partnerID=40&md5=488ec36078e6fd4f829d45c6f865d374","Checking the validity of test scores is important in both educational and psychological measurement. Person-fit analysis provides several statistics that help practitioners assessing whether individual item score vectors conform to a prespecified item response theory model or, alternatively, to a group of test takers. Software enabling easy access to most person-fit statistics was lacking up to now. The PerFit R package was written in order to fill in this void. A theoretical overview of relatively simple person-fit statistics is provided. A practical guide showing how the main functions of PerFit can be used is also given. Both numerical and graphical tools are described and illustrated using examples. The goal is to show how person-fit statistics can be easily applied to testing of questionnaire data. © 2016, American Statistical Association. All rights reserved."
"Discrete choice models with random parameters in R: The rchoice package",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992497928&doi=10.18637%2fjss.v074.i10&partnerID=40&md5=00e52fa23d53c66f815990e525e0dbfe","Rchoice is a package in R for estimating models with individual heterogeneity for both cross-sectional and panel (longitudinal) data. In particular, the package allows binary, ordinal and count response, as well as continuous and discrete covariates. Individual heterogeneity is modeled by allowing the parameter associated with each observed variable (e.g., its coefficient) to vary randomly across individuals according to some pre-specified distribution. Simulated maximum likelihood method is implemented for the estimation of the moments of the distributions. In addition, functions for plotting the conditional individual-specific coefficients and their confidence interval are provided. This article is a general description of Rchoice and all functionalities are illustrated using real databases. © 2016, American Statistical Association. All rights reserved."
"equate: An R package for observed-score linking and equating",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992486798&doi=10.18637%2fjss.v074.i08&partnerID=40&md5=f3e2ec824d8e01da90d0031bc3afd65d","The R package equate contains functions for observed-score linking and equating under single-group, equivalent-groups, and nonequivalent-groups with anchor test(s) designs. This paper introduces these designs and provides an overview of observed-score equating with details about each of the supported methods. Examples demonstrate the basic functionality of the equate package. © 2016, American Statistical Association. All rights reserved."
"GamboostLSS: An R package for model building and variable selection in the GAMLSS framework",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992482383&doi=10.18637%2fjss.v074.i01&partnerID=40&md5=0f14e2ad4fcf4c3bbb53a6e4a72499f3","Generalized additive models for location, scale and shape are a flexible class of regression models that allow to model multiple parameters of a distribution function, such as the mean and the standard deviation, simultaneously. With the R package gamboostLSS, we provide a boosting method to fit these models. Variable selection and model choice are naturally available within this regularized regression framework. To introduce and illustrate the R package gamboostLSS and its infrastructure, we use a data set on stunted growth in India. In addition to the specification and application of the model itself, we present a variety of convenience functions, including methods for tuning parameter selection, prediction and visualization of results. The package gamboostLSS is available from the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=gamboostLSS. © 2016, American Statistical Association. All rights reserved."
"Imputation with the R package VIM",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992476986&doi=10.18637%2fjss.v074.i07&partnerID=40&md5=1697209a2c8365e8a950907b781d8906","The package VIM (Templ, Alfons, Kowarik, and Prantner 2016) is developed to explore and analyze the structure of missing values in data using visualization methods, to impute these missing values with the built-in imputation methods and to verify the imputation process using visualization tools, as well as to produce high-quality graphics for publications. This article focuses on the different imputation techniques available in the package. Four different imputation methods are currently implemented in VIM, namely hot-deck imputation, k-nearest neighbor imputation, regression imputation and iterative robust model-based imputation (Templ, Kowarik, and Filzmoser 2011). All of these methods are implemented in a flexible manner with many options for customization. Furthermore in this article practical examples are provided to highlight the use of the implemented methods on real-world applications. In addition, the graphical user interface of VIM has been re-implemented from scratch resulting in the package VIMGUI (Schopfhauser, Templ, Alfons, Kowarik, and Prantner 2016) to enable users without extensive R skills to access these imputation and visualization methods. © 2016, American Statistical Association. All rights reserved."
"Flexible graphical assessment of experimental designs in R: The vdg package",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992476973&doi=10.18637%2fjss.v074.i03&partnerID=40&md5=cc2b660daf91b02a39013b51f191ade9","The R package vdg provides a flexible interface for producing various graphical summaries of the prediction variance associated with specific linear model specifications and experimental designs. These methods include variance dispersion graphs, fraction of design space plots and quantile plots which can assist in choosing between a catalogue of candidate experimental designs. Instead of restrictive optimization methods used in traditional software to explore design regions, vdg utilizes sampling methods to introduce more flexibility. The package takes advantage of R’s modern graphical abilities via ggplot2 (Wickham 2009), adds facilities for using a variety of distance methods, allows for more flexible model specifications and incorporates quantile regressions to help with model comparison. © 2016, American Statistical Association. All rights reserved."
"Biometrics and psychometrics: Origins, commonalities and differences",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987849519&doi=10.18637%2fjss.v073.i05&partnerID=40&md5=1482e285b9a1f6da1005081c7ea40e44","Starting with the common origins of biometrics and psychometrics at the beginning of the twentieth century, the paper compares and contrasts subsequent developments, informed by the author’s 35 years at Rothamsted Experimental Station followed by a period with the data theory group in Leiden and thereafter. Although the methods used by biometricians and psychometricians have much in common, there are important differences arising from the different fields of study. Similar differences arise wherever data are generated and may be regarded as a major driving force in the development of statistical ideas. © 2016, American Statistical Association. All rights reserved."
"Honoring the lion: A festschrift for Jan de Leeuw",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987824818&doi=10.18637%2fjss.v073.i01&partnerID=40&md5=b166b7b389f4470e7f8a64a2df22eb89","This special volume celebrates the 20th anniversary of the Journal of Statistical Software (JSS) and is a Festschrift for its founding editor Jan de Leeuw. Jan recently retired from his long-held position as founding chair of the Department of Statistics at the University of California, Los Angeles. The contributions to this special volume look back at some of his research interests and accomplishments during the half-century that he has been active in psychometrics and statistics. In this introduction, the guest editors also reminisce on their own first encounters with Jan, ten years ago. Since that time JSS has solidified its place as a leading journal of computational statistics, a fact that has a lot to do with Jan’s stewardship. We include a brief history of JSS. © 2016, American Statistical Association. All rights reserved."
"R and the Journal of Statistical Software",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987817816&doi=10.18637%2fjss.v073.i02&partnerID=40&md5=87c93738bfc121b6094ba6bb484cc0bd","The Journal of Statistical Software was founded by Jan de Leeuw in 1996, the year before the Comprehensive R Archive Network (CRAN) first made R and contributed R packages widely available on the Internet. Within a few years, R came increasingly to dominate contributions to JSS. We trace the continuing development of R and CRAN, and the representation of R and other statistical software in the pages of JSS. © 2016, American Statistical Association. All rights reserved."
"Jan de Leeuw and the French school of data analysis",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987813299&doi=10.18637%2fjss.v073.i06&partnerID=40&md5=93845140f51d2fa2b14ea3c2e8479d2a","The Dutch and the French schools of data analysis differ in their approaches to the question: How does one understand and summarize the information contained in a data set? The commonalities and discrepancies between the schools are explored here with a focus on methods dedicated to the analysis of categorical data, which are known either as homogeneity analysis (HOMALS) or multiple correspondence analysis (MCA). © 2016, American Statistical Association. All rights reserved."
"Looking back at the gifi system of nonlinear multivariate analysis",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987806890&doi=10.18637%2fjss.v073.i04&partnerID=40&md5=d45edbf00d0ec8c7847422bff60ea0d2","Gifi was the nom de plume for a group of researchers led by Jan de Leeuw at the University of Leiden. Between 1970 and 1990 the group produced a stream of theoretical papers and computer programs in the area of nonlinear multivariate analysis that were very innovative. In an informal way this paper discusses the so-called Gifi system of nonlinear multivariate analysis, that entails homogeneity analysis (which is closely related to multiple correspondence analysis) and generalizations. The history is discussed, giving attention to the scientific philosophy of this group, and links to machine learning are indicated. © 2016, American Statistical Association. All rights reserved."
"Jan de Leeuw and statistics at UCLA",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987790557&doi=10.18637%2fjss.v073.i09&partnerID=40&md5=ab91b25f7f522b8de1879f97e3b5de57","Jan de Leeuw came to University of California, Los Angeles (UCLA) Statistics at a crucial time in its history. We set out some details of what he found when he arrived on UCLA’s north campus in 1987, what was there when he left it some 27 years later, and how he fashioned the changes that are now so widely recognized. © 2016, American Statistical Association. All rights reserved."
"My multiway analysis: From Jan de Leeuw to TWPack and back",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987786788&doi=10.18637%2fjss.v073.i03&partnerID=40&md5=44a430f57b6d3ca50e326b712ec9e5c4","This paper is a mixture of my personal experiences of Jan de Leeuw as a supervisor of my master’s and Ph.D. theses, as well as a sketch of how three-way analysis, the subject Jan chose for me, developed over time. The emphasis is on where it is and was applied, and to what extent it stole the hearts of applied researchers in different disciplines. Furthermore, the paper contains some musings about how we should go about promoting the use of the techniques, especially in the social and behavioural sciences. Finally, an overview is provided of available software and attention is paid to how (three-way) software may be designed to encourage its use by the scientific community, as it befits a paper in the Journal of Statistical Software. © 2016, American Statistical Association. All rights reserved."
"ExtRemes 2.0: An extreme value analysis package in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986252738&doi=10.18637%2fjss.v072.i08&partnerID=40&md5=f28ff093fa55b99f1e28f81e59171854","This article describes the extreme value analysis (EVA) R package extRemes version 2.0, which is completely redesigned from previous versions. The functions primarily provide utilities for implementing univariate EVA, with a focus on weather and climate applications, including the incorporation of covariates, as well as some functionality for assessing bivariate tail dependence. © 2016, American Statistical Association. All rights reserved."
"Mixture experiments in R using mixexp",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983080816&doi=10.18637%2fjss.v072.c02&partnerID=40&md5=245b271961979e8d44239e9456153ff7","This article discusses the design and analysis of mixture experiments with R and illustrates the use of the recent package mixexp. This package provides functions for creating mixture designs composed of extreme vertices and edge and face centroids in constrained mixture regions where components are subject to upper, lower and linear constraints. These designs cannot be created by other R packages. mixexp also provides functions for graphical display of models fit to data from mixture experiments that cannot be created with other R packages. © 2016, American Statistical Association. All rights reserved."
"Bayesian nonparametric mixture estimation for time-indexed functional data in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983070883&doi=10.18637%2fjss.v072.i02&partnerID=40&md5=b1568c533e72f73137564a56ca7f74f6","We present growfunctions for R that offers Bayesian nonparametric estimation models for analysis of dependent, noisy time series data indexed by a collection of domains. This data structure arises from combining periodically published government survey statistics, such as are reported in the Current Population Study (CPS). The CPS publishes monthly, by-state estimates of employment levels, where each state expresses a noisy time series. Published state-level estimates from the CPS are composed from household survey responses in a model-free manner and express high levels of volatility due to insufficient sample sizes. Existing software solutions borrow information over a modeled time-based dependence to extract a de-noised time series for each domain. These solutions, however, ignore the dependence among the domains that may be additionally leveraged to improve estimation efficiency. The growfunctions package offers two fully nonparametric mixture models that simultaneously estimate both a time and domain-indexed dependence structure for a collection of time series: (1) A Gaussian process (GP) construction, which is parameterized through the covariance matrix, estimates a latent function for each domain. The covariance parameters of the latent functions are indexed by domain under a Dirichlet process prior that permits estimation of the dependence among functions across the domains: (2) An intrinsic Gaussian Markov random field prior construction provides an alternative to the GP that expresses different computation and estimation properties. In addition to performing denoised estimation of latent functions from published domain estimates, growfunctions allows estimation of collections of functions for observation units (e.g., households), rather than aggregated domains, by accounting for an informative sampling design under which the probabilities for inclusion of observation units are related to the response variable. growfunctions includes plot functions that allow visual assessments of the fit performance and dependence structure of the estimated functions. Computational efficiency is achieved by performing the sampling for estimation functions using compiled C++. © 2016, American Statistical Association. All rights reserved."
"LaGP: Large-scale spatial modeling via local approximate Gaussian processes in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983070573&doi=10.18637%2fjss.v072.i01&partnerID=40&md5=c806515363a639fb36532cde82eacebb","Gaussian process (GP) regression models make for powerful predictors in out of sample exercises, but cubic runtimes for dense matrix decompositions severely limit the size of data - training and testing - on which they can be deployed. That means that in computer experiment, spatial/geo-physical, and machine learning contexts, GPs no longer enjoy privileged status as data sets continue to balloon in size. We discuss an implementation of local approximate Gaussian process models, in the laGP package for R, that offers a particular sparse-matrix remedy uniquely positioned to leverage modern parallel computing architectures. The laGP approach can be seen as an update on the spatial statistical method of local kriging neighborhoods. We briefly review the method, and provide extensive illustrations of the features in the package through worked-code examples. The appendix covers custom building options for symmetric multi-processor and graphical processing units, and built-in wrapper routines that automate distribution over a simple network of workstations. © 2016, American Statistical Association. All rights reserved."
"Multivariate dose-response meta-analysis: The dosresmeta R package",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983059456&doi=10.18637%2fjss.v072.c01&partnerID=40&md5=a3ce80d96e873cf022cfa36f8d314247","An increasing number of quantitative reviews of epidemiological data includes a doseresponse analysis. Aims of this paper are to describe the main aspects of the methodology and to illustrate the novel R package dosresmeta developed for multivariate dose-response meta-analysis of summarized data. Specific topics covered are reconstructing covariances of correlated outcomes; pooling of study-specific trends; flexible modeling of the exposure; testing hypothesis; assessing statistical heterogeneity; and presenting in either a graphical or tabular way the overall dose-response association. © 2016, American Statistical Association. All rights reserved."
"Mixed frequency data sampling regression models: The R package midasr",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983028346&doi=10.18637%2fjss.v072.i04&partnerID=40&md5=f508deb50925b95b8e5e7c27b3ff7aa2","When modeling economic relationships it is increasingly common to encounter data sampled at different frequencies. We introduce the R package midasr which enables estimating regression models with variables sampled at different frequencies within a MIDAS regression framework put forward in work by Ghysels, Santa-Clara, and Valkanov (2002). In this article we define a general autoregressive MIDAS regression model with multiple variables of different frequencies and show how it can be specified using the familiar R formula interface and estimated using various optimization methods chosen by the researcher. We discuss how to check the validity of the estimated model both in terms of numerical convergence and statistical adequacy of a chosen regression specification, how to perform model selection based on a information criterion, how to assess forecasting accuracy of the MIDAS regression model and how to obtain a forecast aggregation of different MIDAS regression models. We illustrate the capabilities of the package with a simulated MIDAS regression model and give two empirical examples of application of MIDAS regression. © 2016, American Statistical Association. All rights reserved."
"Analyzing state sequences with probabilistic suffix trees: The PST R package",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983027806&doi=10.18637%2fjss.v072.i03&partnerID=40&md5=f3af6b918954cb98c538e8d8fb1aa694","This article presents the PST R package for categorical sequence analysis with probabilistic suffix trees (PSTs), i.e., structures that store variable-length Markov chains (VLMCs). VLMCs allow to model high-order dependencies in categorical sequences with parsimonious models based on simple estimation procedures. The package is specifically adapted to the field of social sciences, as it allows for VLMC models to be learned from sets of individual sequences possibly containing missing values; in addition, the package is extended to account for case weights. This article describes how a VLMC model is learned from one or more categorical sequences and stored in a PST. The PST can then be used for sequence prediction, i.e., to assign a probability to whole observed or artificial sequences. This feature supports data mining applications such as the extraction of typical patterns and outliers. This article also introduces original visualization tools for both the model and the outcomes of sequence prediction. Other features such as functions for pattern mining and artificial sequence generation are described as well. The PST package also allows for the computation of probabilistic divergence between two models and the fitting of segmented VLMCs, where sub-models fitted to distinct strata of the learning sample are stored in a single PST. © 2016, American Statistical Association. All rights reserved."
"RProtoBuf: Efficient cross-language data serialization in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978915902&doi=10.18637%2fjss.v071.i02&partnerID=40&md5=17da78abf8a2f181e9a218a51c13401e","Modern data collection and analysis pipelines often involve a sophisticated mix of applications written in general purpose and specialized programming languages. Many formats commonly used to import and export data between different programs or systems, such as CSV or JSON, are verbose, inefficient, not type-safe, or tied to a specific programming language. Protocol Buffers are a popular method of serializing structured data between applications - while remaining independent of programming languages or operating systems. They offer a unique combination of features, performance, and maturity that seems particularly well suited for data-driven applications and numerical computing. The RProtoBuf package provides a complete interface to Protocol Buffers from the R environment for statistical computing. This paper outlines the general class of data serialization requirements for statistical computing, describes the implementation of the RProtoBuf package, and illustrates its use with example applications in large-scale data collection pipelines and web services. © 2016, American Statistical Association. All rights reserved."
"gramEvol: Grammatical evolution in R",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978881311&doi=10.18637%2fjss.v071.i01&partnerID=40&md5=ba056e9b5349a91dbbbd5eca8dea3668","We describe an R package which implements grammatical evolution (GE) for automatic program generation. By performing an unconstrained optimization over a population of R expressions generated via a user-defined grammar, programs which achieve a desired goal can be discovered. The package facilitates the coding and execution of GE programs, and supports parallel execution. In addition, three applications of GE in statistics and machine learning, including hyper-parameter optimization, classification and feature generation are studied. © 2016, American Statistical Association. All rights reserved."
"JMFit: A SAS macro for joint models of longitudinal and survival data",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978808775&doi=10.18637%2fjss.v071.i03&partnerID=40&md5=38259413fbeacbb66c41a0f9bdd49ec5","Joint models for longitudinal and survival data now have a long history of being used in clinical trials or other studies in which the goal is to assess a treatment effect while accounting for a longitudinal biomarker such as patient-reported outcomes or immune responses. Although software has been developed for fitting the joint model, no software packages are currently available for simultaneously fitting the joint model and assessing the fit of the longitudinal component and the survival component of the model separately as well as the contribution of the longitudinal data to the fit of the survival model. To fulfill this need, we develop a SAS macro, called JMFit. JMFit implements a variety of popular joint models and provides several model assessment measures including the decomposition of AIC and BIC as well as ΔAIC and ΔBIC recently developed in Zhang, Chen, Ibrahim, Boye, Wang, and Shen (2014). Examples with real and simulated data are provided to illustrate the use of JMFit. © 2016, American Statistical Association. All rights reserved."
"GMCM: Unsupervised clustering and meta-analysis using gaussian mixture copula models",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962463325&doi=10.18637%2fjss.v070.i02&partnerID=40&md5=d73961d3c55c45e75fc649005564fdb2","Methods for clustering in unsupervised learning are an important part of the statistical toolbox in numerous scientific disciplines. Tewari, Giering, and Raghunathan (2011) proposed to use so-called Gaussian mixture copula models (GMCM) for general unsupervised learning based on clustering. Li, Brown, Huang, and Bickel (2011) independently discussed a special case of these GMCMs as a novel approach to meta-analysis in highdimensional settings. GMCMs have attractive properties which make them highly flexible and therefore interesting alternatives to other well-established methods. However, parameter estimation is hard because of intrinsic identifiability issues and intractable likelihood functions. Both aforementioned papers discuss similar expectation-maximization-like algorithms as their pseudo maximum likelihood estimation procedure. We present and discuss an improved implementation in R of both classes of GMCMs along with various alternative optimization routines to the EM algorithm. The software is freely available in the R package GMCM. The implementation is fast, general, and optimized for very large numbers of observations. We demonstrate the use of package GMCM through different applications. © 2016, American Statistical Association. All rights reserved."
"Quantile-based spectral analysis in an object-oriented framework and a reference implementation in R: The quantspec package",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962463293&doi=10.18637%2fjss.v070.i03&partnerID=40&md5=6aefb59a13415f000d9a52a3c91f6ad3","Quantile-based approaches to the spectral analysis of time series have recently attracted a lot of attention. Several methods for estimation have been proposed in the literature and their statistical properties were analyzed. Yet, so far, neither a systematic method for computation nor a comprehensive software implementation are available to date. This paper contains two main contributions. First, an extensible framework for quantile-based spectral analysis of time series is developed and documented using object-oriented models. A comprehensive, open source reference implementation of this framework is provided in the R package quantspec, which is available from the Comprehensive R Archive Network. The second contribution of the present paper is to provide a detailed tutorial, with worked examples, for this R package. A reader who is already familiar with quantile-based spectral analysis and whose primary interest is not the design of the quantspec package, but how to use it, can read the tutorial and worked examples (Sections 3 and 4) independently. © 2016, American Statistical Association. All rights reserved."
"TMB: Automatic differentiation and laplace approximation",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962434200&doi=10.18637%2fjss.v070.i05&partnerID=40&md5=e5561ce560473344e3cd3b77022008de","TMB is an open source R package that enables quick implementation of complex nonlinear random effects (latent variable) models in a manner similar to the established AD Model Builder package (ADMB, http://admb-project.org/; Fournier et al. 2011). In addition, it offers easy access to parallel computations. The user defines the joint likelihood for the data and the random effects as a C++ template function, while all the other operations are done in R; e.g., reading in the data. The package evaluates and maximizes the Laplace approximation of the marginal likelihood where the random effects are automatically integrated out. This approximation, and its derivatives, are obtained using automatic differentiation (up to order three) of the joint likelihood. The computations are designed to be fast for problems with many random effects (≈ 106) and parameters (≈ 103). Computation times using ADMB and TMB are compared on a suite of examples ranging from simple models to large spatial models where the random effects are a Gaussian random field. Speedups ranging from 1.5 to about 100 are obtained with increasing gains for large problems. The package and examples are available at http://tmb-project.org/. © 2016, American Statistical Association. All rights reserved."
"missMDA: A package for handling missing values in multivariate data analysis",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962384848&doi=10.18637%2fjss.v070.i01&partnerID=40&md5=25838d59f76a2ee493c8037416dc98a0","We present the R package missMDA which performs principal component methods on incomplete data sets, aiming to obtain scores, loadings and graphical representations despite missing values. Package methods include principal component analysis for continuous variables, multiple correspondence analysis for categorical variables, factorial analysis on mixed data for both continuous and categorical variables, and multiple factor analysis for multi-table data. Furthermore, missMDA can be used to perform single imputation to complete data involving continuous, categorical and mixed variables. A multiple imputation method is also available. In the principal component analysis framework, variability across different imputations is represented by confidence areas around the row and column positions on the graphical outputs. This allows assessment of the credibility of results obtained from incomplete data sets. © 2016, American Statistical Association. All rights reserved."
"bartMachine: Machine learning with bayesian additive regression trees",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962381511&doi=10.18637%2fjss.v070.i04&partnerID=40&md5=bb4a7bc6c2d8068011f5ebea6212305f","We present a new package in R implementing Bayesian additive regression trees (BART). The package introduces many new features for data analysis using BART such as variable selection, interaction detection, model diagnostic plots, incorporation of missing data and the ability to save trees for future prediction. It is significantly faster than the current R implementation, parallelized, and capable of handling both large sample sizes and high-dimensional data. © 2016, American Statistical Association. All rights reserved."
"R2GUESS: A graphics processing unit-based R package for bayesian variable selection regression of multivariate responses",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956938744&doi=10.18637%2fjss.v069.i02&partnerID=40&md5=536a6a60394e8fcc34705d05c6940ac3","Technological advances in molecular biology over the past decade have given rise tohigh dimensional and complex datasets offering the possibility to investigate biologicalassociations between a range of genomic features and complex phenotypes. The analysisof this novel type of data generated unprecedented computational challenges which ultimatelyled to the definition and implementation of computationally efficient statisticalmodels that were able to scale to genome-wide data, including Bayesian variable selectionapproaches. While extensive methodological work has been carried out in this area, onlyfew methods capable of handling hundreds of thousands of predictors were implementedand distributed. Among these we recently proposed GUESS, a computationally optimize dalgorithm making use of graphics processing unit capabilities, which can accommo date multiple outcomes. In this paper we propose R2GUESS, an R package wrapping theoriginal C++ source code. In addition to providing a user-friendly interface of the originalcode automating its parametrisation, and data handling, R2GUESS also incorporatesmany features to explore the data, to extend statistical inferences from the native algorithm(e.g., effect size estimation, significance assessment), and to visualize outputs fromthe algorithm. We first detail the model and its parametrisation, and describe in detailsits optimised implementation. Based on two examples we finally illustrate its statisticalperformances and flexibility. © 2016, American Statistical Association. All rights reserved."
"Least-squares means: The R package lsmeans",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956925118&doi=10.18637%2fjss.v069.i01&partnerID=40&md5=67206d4797f99d89bd22d1c771a5a5c2","Least-squares means are predictions from a linear model, or averages thereof. They are useful in the analysis of experimental data for summarizing the effects of factors, and for testing linear contrasts among predictions. The lsmeans package (Lenth 2016) provides a simple way of obtaining least-squares means and contrasts thereof. It supports many models fitted by R (R Core Team 2015) core packages (as well as a few key contributed ones) that fit linear or mixed models, and provides a simple way of extending it to cover more model classes. © 2016, American Statistical Association. All rights reserved."
"Label.Switching: An R package for dealing with the label switching problem in MCMC outputs",2016,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956900027&doi=10.18637%2fjss.v069.c01&partnerID=40&md5=29045eecd166c02c49212904f83e8986","Label switching is a well-known and fundamental problem in Bayesian estimation of mixture or hidden Markov models. In case that the prior distribution of the model parameters is the same for all states, then both the likelihood and posterior distribution are invariant to permutations of the parameters. This property makes Markov chain Monte Carlo (MCMC) samples simulated from the posterior distribution non-identifiable. In this paper, the label.switching package is introduced. It contains one probabilistic and seven deterministic relabeling algorithms in order to post-process a given MCMC sample, provided by the user. Each method returns a set of permutations that can be used to reorder the MCMC output. Then, any parametric function of interest can be inferred using the reordered MCMC sample. A set of user-defined permutations is also accepted, allowing the researcher to benchmark new relabeling methods against the available ones. © 2016, American Statistical Association. All rights reserved."
